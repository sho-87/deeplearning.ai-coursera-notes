{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deeplearning.ai - Coursera Course Notes Compiled notes for the deep learning.ai specialization on Coursera. I intend to continuously add to and improve these notes as I complete the 5-course specialization . Resources Notation cheetsheet (I recommend printing this out and sticking it on the wall where you work!) Check links at end of all programming assignments, these are good resources.","title":"About"},{"location":"#deeplearningai-coursera-course-notes","text":"Compiled notes for the deep learning.ai specialization on Coursera. I intend to continuously add to and improve these notes as I complete the 5-course specialization .","title":"Deeplearning.ai - Coursera Course Notes"},{"location":"#resources","text":"Notation cheetsheet (I recommend printing this out and sticking it on the wall where you work!) Check links at end of all programming assignments, these are good resources.","title":"Resources"},{"location":"neural_networks_and_deep_learning/week_1/","text":"Week 1: Introduction What is a neural network? Supervised Learning with Neural Networks In supervised learning, you have some input \\(x\\) and some output \\(y\\) . The goal is to learn a mapping \\(x \\rightarrow y\\) . Possibly, the single most lucrative (but not the most inspiring) application of deep learning today is online advertising. Using information about the ad combined with information about the user as input, neural networks have gotten very good at predicting whether or not you click on an ad. Because the ability to show you ads that you're more likely to click on has a direct impact on the bottom line of some of the very large online advertising companies . Here are some more areas in which deep learning has had a huge impact: Computer vision the recognition and classification of objects in photos and videos. Speech Recognition converting speech in audio files into transcribed text. Machine translation translating one natural language to another. Autonomous driving A lot of the value generation from using neural networks have come from intelligently choosing our \\(x\\) and \\(y\\) and learning a mapping. We tend to use different architectures for different types of data. For example, convolutional neural networks (CNNs) are very common for image data , while recurrent neural networks (RNNs) are very common for sequence data (such as text). Some data, such as radar data from autonomous vehicles, don't neatly fit into any particularly category and so we typical use a complex/hybrid network architecture. Structured vs. Unstructured Data You can think of structured data as essentially meaning databases of data . It is data that is highly structured , typically with multiple, well-defined attributes for each piece of data. For example, in housing price prediction, you might have a database where the columns tells you the size and the number of bedrooms. Or in predicting whether or not a user will click on an ad, you might have information about the user, such as the age, some information about the ad, and then labels why that you're trying to predict. In contrast, unstructured data refers to things like audio, raw audio, or images. Here the features might be the pixel values in an image or the individual words in a piece of text. Historically, it has been much harder for computers to make sense of unstructured data compared to structured data. In contrast, the human race has evolved to be very good at understanding audio cues as well as images. People are really good at interpreting unstructured data . And so one of the most exciting things about the rise of neural networks is that, thanks to deep learning, thanks to neural networks, computers are now much better at interpreting unstructured data as compared to just a few years ago. This creates opportunities for many new exciting applications that use speech recognition, image recognition, and natural language processing on text. Because people have a natural empathy to understanding unstructured data, you might hear about neural network successes on unstructured data more in the media because it's just cool when the neural network recognizes a cat. We all like that, and we all know what that means. But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data, such as much better advertising systems, much better profit recommendations, and just a much better ability to process the giant databases that many companies have to make accurate predictions from them. Why is Deep Learning taking off? If the basic technical details surrounding deep learning have been around for decades, why are they just taking off now? First and foremost, the massive amount of (labeled) data we have been generating for the past couple of decades (in part because of the 'digitization' of our society). It turns out, that large, complex neural networks can take advantage of these huge data stores. Thus, we often say scale has been driving progress with deep learning, where scale means the size of the data, the size/complexity of the neural network, and the growth in computation. The interplay between these 'scales' is apparent when you consider that many of the algorithmic advances of neural networks have come from making them more computational efficient. Algorithmic Advances: ReLu One of the huge breakthroughs in neural networks has been the seemingly simple switch from the sigmoid activation function to the rectified linear (ReLu) activation function. One of the problems with using sigmoid functions is that its gradients approach 0 as input to the sigmoid function approaches and \\(+\\infty\\) and \\(-\\infty\\) . In this case, the updates to the parameters become very small and our learning slows dramatically. With ReLu units, our gradient is equal to \\(1\\) for all positive inputs. This makes learning with gradient descent much faster. See here for more information on ReLu's. Scale Advances With smaller training sets, the relative ordering of the algorithms is actually not very well defined so if you don't have a lot of training data it is often up to your skill at hand engineering features that determines the performance. For small training sets, it's quite possible that if someone training an SVM is more motivated to hand engineer features they will outperform a powerful neural network architecture. However, for very large training sets, we consistently see large neural networks dominating the other approaches .","title":"Week 1"},{"location":"neural_networks_and_deep_learning/week_1/#week-1-introduction","text":"","title":"Week 1: Introduction"},{"location":"neural_networks_and_deep_learning/week_1/#what-is-a-neural-network","text":"","title":"What is a neural network?"},{"location":"neural_networks_and_deep_learning/week_1/#supervised-learning-with-neural-networks","text":"In supervised learning, you have some input \\(x\\) and some output \\(y\\) . The goal is to learn a mapping \\(x \\rightarrow y\\) . Possibly, the single most lucrative (but not the most inspiring) application of deep learning today is online advertising. Using information about the ad combined with information about the user as input, neural networks have gotten very good at predicting whether or not you click on an ad. Because the ability to show you ads that you're more likely to click on has a direct impact on the bottom line of some of the very large online advertising companies . Here are some more areas in which deep learning has had a huge impact: Computer vision the recognition and classification of objects in photos and videos. Speech Recognition converting speech in audio files into transcribed text. Machine translation translating one natural language to another. Autonomous driving A lot of the value generation from using neural networks have come from intelligently choosing our \\(x\\) and \\(y\\) and learning a mapping. We tend to use different architectures for different types of data. For example, convolutional neural networks (CNNs) are very common for image data , while recurrent neural networks (RNNs) are very common for sequence data (such as text). Some data, such as radar data from autonomous vehicles, don't neatly fit into any particularly category and so we typical use a complex/hybrid network architecture.","title":"Supervised Learning with Neural Networks"},{"location":"neural_networks_and_deep_learning/week_1/#structured-vs-unstructured-data","text":"You can think of structured data as essentially meaning databases of data . It is data that is highly structured , typically with multiple, well-defined attributes for each piece of data. For example, in housing price prediction, you might have a database where the columns tells you the size and the number of bedrooms. Or in predicting whether or not a user will click on an ad, you might have information about the user, such as the age, some information about the ad, and then labels why that you're trying to predict. In contrast, unstructured data refers to things like audio, raw audio, or images. Here the features might be the pixel values in an image or the individual words in a piece of text. Historically, it has been much harder for computers to make sense of unstructured data compared to structured data. In contrast, the human race has evolved to be very good at understanding audio cues as well as images. People are really good at interpreting unstructured data . And so one of the most exciting things about the rise of neural networks is that, thanks to deep learning, thanks to neural networks, computers are now much better at interpreting unstructured data as compared to just a few years ago. This creates opportunities for many new exciting applications that use speech recognition, image recognition, and natural language processing on text. Because people have a natural empathy to understanding unstructured data, you might hear about neural network successes on unstructured data more in the media because it's just cool when the neural network recognizes a cat. We all like that, and we all know what that means. But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data, such as much better advertising systems, much better profit recommendations, and just a much better ability to process the giant databases that many companies have to make accurate predictions from them.","title":"Structured vs. Unstructured Data"},{"location":"neural_networks_and_deep_learning/week_1/#why-is-deep-learning-taking-off","text":"If the basic technical details surrounding deep learning have been around for decades, why are they just taking off now? First and foremost, the massive amount of (labeled) data we have been generating for the past couple of decades (in part because of the 'digitization' of our society). It turns out, that large, complex neural networks can take advantage of these huge data stores. Thus, we often say scale has been driving progress with deep learning, where scale means the size of the data, the size/complexity of the neural network, and the growth in computation. The interplay between these 'scales' is apparent when you consider that many of the algorithmic advances of neural networks have come from making them more computational efficient. Algorithmic Advances: ReLu One of the huge breakthroughs in neural networks has been the seemingly simple switch from the sigmoid activation function to the rectified linear (ReLu) activation function. One of the problems with using sigmoid functions is that its gradients approach 0 as input to the sigmoid function approaches and \\(+\\infty\\) and \\(-\\infty\\) . In this case, the updates to the parameters become very small and our learning slows dramatically. With ReLu units, our gradient is equal to \\(1\\) for all positive inputs. This makes learning with gradient descent much faster. See here for more information on ReLu's. Scale Advances With smaller training sets, the relative ordering of the algorithms is actually not very well defined so if you don't have a lot of training data it is often up to your skill at hand engineering features that determines the performance. For small training sets, it's quite possible that if someone training an SVM is more motivated to hand engineer features they will outperform a powerful neural network architecture. However, for very large training sets, we consistently see large neural networks dominating the other approaches .","title":"Why is Deep Learning taking off?"},{"location":"neural_networks_and_deep_learning/week_2/","text":"Week 2: Neural networks basics Binary Classification First, some notation, \\(n\\) is the number of data attributes, or features \\(m\\) is the number of input examples in our dataset (sometimes we write \\(m_{train}, m_{test}\\) to be more explicit). our data is represented as input, output pairs, \\((x^{(1)},y^{(1)}), ..., (x^{(m)},y^{(m)})\\) where \\(x \\in \\mathbb R^n\\) , \\(y \\in {0,1}\\) \\(X\\) is our design matrix, which is simply columns of our input vectors \\(x^{(i)}\\) , thus it has dimensions of \\(n\\) x \\(m\\) . \\(Y = [y^{(1)}, ..., y^{(m)}]\\) , and is thus a \\(1\\) x \\(m\\) matrix. Note, this is different from many other courses which represent the design matrix, \\(X\\) as rows of transposed input vectors and the output vector \\(Y\\) as a \\(m\\) x \\(1\\) column vector. The above convention turns out to be easier to implement. When programming neural networks, implementation details become extremely important ( e.g . vectorization in place of for loops). We are going to introduce many of the key concepts of neural networks using logistic regression , as this will make them easier to understand. Logistic regression is an algorithm for binary classification . In binary classification, we have an input ( e.g . an image) that we want to classifying as belonging to one of two classes. Logistic Regression (Crash course) Given an input feature vector \\(x\\) (perhaps corresponding to an images flattened pixel data), we want \\(\\hat y\\) , the probability of the input examples class, \\(\\hat y = P(y=1 | x)\\) If \\(x\\) is a picture, we want the chance that this is a picture of a cat, \\(\\hat y\\) . The parameters of our model are \\(w \\in \\mathbb R^{n_x}\\) , \\(b \\in \\mathbb R\\) . Our output is \\(\\hat y = \\sigma(w^Tx + b)\\) were \\(\\sigma\\) is the sigmoid function . The formula for the sigmoid function is given by: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) where \\(z = w^Tx + b\\) . We notice a few things: If \\(z\\) is very large, \\(e^{-z}\\) will be close to \\(0\\) , and so \\(\\sigma(z)\\) is very close to \\(1\\) . If \\(z\\) is very small, \\(e^{-z}\\) will grow very large, and so \\(\\sigma(z)\\) is very close to \\(0\\) . It helps to look at the plot \\(y = e^{-x}\\) Thus, logistic regression attempts to learn parameters which will classify images based on their probability of belonging to one class or the other. The classification decision is decided by applying the sigmoid function to \\(w^Tx + b\\) . Note, with neural networks, it is easier to keep the weights \\(w\\) and the biases \\(b\\) separate. Another notation involves adding an extra parameters (\\(w_0\\) which plays the role of the bias. Loss function Our prediction for a given example \\(x^{(i)}\\) is \\(\\hat y^{(i)} = \\sigma(w^Tx^{(i)} + b)\\) . We chose loss function , \\(\\ell(\\hat y, y) = -(y \\; log\\; \\hat y + (1-y) \\;log(1-\\hat y))\\). We note that: If \\(y=1\\) , then the loss function is \\(\\ell(\\hat y, y) = -log\\; \\hat y\\) . Thus, the loss approaches zero as \\(\\hat y\\) approaches 1. If \\(y=0\\) , then the loss function is \\(\\ell(\\hat y, y) = -log\\; (1 -\\hat y)\\) . Thus, the loss approaches zero as \\(\\hat y\\) approaches 0. Note, while \\(\\ell_2\\) loss is taught in many courses and seems like an appropriate choice, it is non-convex and so we cannot use gradient descent to optimize it. An optional video is given further justifying the use of this loss function. Watch it and add notes here! Note that the loss function measures how well we are doing on a single example . We now define a cost function , which captures how well we are doing on the entire dataset: \\(J(w,b) = \\frac{1}{m}\\sum^m_{i=1} \\ell(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m}\\sum^m_{i=1}(y^{(i)} \\; log\\; \\hat y^{(i)} + (1-y^{(i)}) \\;log(1-\\hat y^{(i)}))\\) Note that this notation is somewhat unique, typically the cost/loss functions are just interchangeable terms. However in this course, we will define the loss function as computing the error for a single training example and the cost function as the average of the loss functions of the entire training set. Gradient Descent We want to find \\(w,b\\) which minimize \\(J(w,b)\\) . We can plot the cost function with \\(w\\) and \\(b\\) as our horizontal axes: In practice, \\(w\\) typically has many more dimensions. Thus, the cost function \\(J(w,b)\\) can be thought of as a surface, were the height of the surface above the horizontal axes is its value. We want to find the values of our parameters \\(w, b\\) at the lowest point of this surface, the point at which the average loss is at its minimum. Gradient Descent Algorithm Initialize \\(w,b\\) to some random values because this cost function is convex, it doesn't matter what values we use to initialize, \\(0\\) is usually chosen for logistic regression. Repeat \\(w := w - \\alpha \\frac{dJ(w)}{dw}\\) \\(b := b - \\alpha \\frac{dJ(w)}{db}\\) \\(\\alpha\\) is our learning rate, it controls how big a step we take on each iteration. In some notations, we use \\(\\partial\\) to denote the partial derivative of a function with \\(2\\) or more variables, and \\(d\\) to denote the derivative of a function of only \\(1\\) variable. When implementing gradient descent in code, we will use the variable \\(dw\\) to represent \\(\\frac{dJ(w, b)}{dw}\\) (this size of the step for \\(w\\) and \\(db\\) to represent \\(\\frac{dJ(w, b)}{db}\\) (the size of the step for \\(b\\) . (ASIDE) Calculus Review Intuition about derivatives Linear Function Example Take the function \\(f(a) = 3a\\). Then \\(f(a) = 6\\) when \\(a = 2\\) . If we were to give \\(a\\) a tiny nudge, say to \\(a = 2.001\\) , what happens to \\(f(a)\\) ? Then \\(f(a) = 6.003\\) , but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \\(a\\) and \\(a + 0.001\\) as the \\(\\frac{height}{width} = 3\\) . Thus, the derivative (or slope) of \\(f(a)\\) w.r.t \\(a\\) is \\(3\\) . We say that \\(\\frac{df(a)}{da} = 3\\) or \\(\\frac{d}{da}f(a) = 3\\) Add my calculus notes here! Link to BlueBrown videos. Non-Linear Function Example Take the function \\(f(a) = a^2\\) . Then \\(f(a) = 4\\) when \\(a = 2\\) . If we were to give \\(a\\) a tiny nudge, say to \\(a = 2.001\\), what happens to \\(f(a)\\)? Then \\(f(a) = 4.004\\), but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \\(a\\) and \\(a + 0.001\\) as the \\(\\frac{height}{width} = 4\\) . In a similar way, we can perform this analysis for any point \\(a\\) on the plot, and we will see that slope of \\(f(a)\\) at some point \\(a\\) is equal to \\(2a\\) . Thus, the derivative (or slope) of \\(f(a)\\) w.r.t \\(a\\) is \\(2a\\) . We say that \\(\\frac{df(a)}{da} = 2a\\) or \\(\\frac{d}{da}f(a) = 2a\\) . Computation Graph A computation graph organizes a series of computations into left-to-right and right-to-left passes. Lets build the intuition behind a computation graph. Say we are trying to compute a function \\(J(a,b,c) = 3(a + bc)\\) . This computation of this function actually has three discrete steps: compute \\(u = bc\\) compute \\(v = a + u\\) compute J = \\(3v\\) We can draw this computation in a graph: The computation graph is useful when you have some variable or output variable that you want to optimize (\\(J\\) in this case, in logistic regression it would be our cost function output ). A forward pass through the graph is represented by left-to-right arrows (as drawn above) and a backwards pass is represented by right-to-left arrows. A backwards pass is a natural way to represent the computation of our derivatives. Derivatives with a computation graph Lets take a look at our computation graph, and see how we can use it to compute the partial derivatives of \\(J\\) i.e., lets carry out backpropogation on this computation graph by hand. Informally, you can think of this as asking: \"If we were to change the value of \\(v\\) slightly, how would \\(J\\) change?\" First, we use our informal way of computing derivatives, and note that a small change to \\(v\\) results in a change to \\(J\\) of 3X that small change, and so \\(\\frac{dJ}{dv} = 3\\) . This represents one step in our backward pass, the first step in backpropagation. Now let's look at another example. What is \\(\\frac{dJ}{da}\\)? We compute the \\(\\frac{dJ}{da}\\) from the second node in the computation graph by noting that a small change to \\(a\\) results in a change to \\(J\\) of 3X that small change, and so \\(\\frac{dJ}{da} = 3.\\) This represents our second step in our backpropagation. One way to break this down is to say that by changing \\(a\\), we change \\(v\\), the magnitude of this change is \\(\\frac{dv}{da}\\) . Through this change in \\(v\\), we also change \\(J\\), and the magnitude of the change is \\(\\frac{dJ}{dv}\\) . To capture this more generally, we use the chain rule from calculus, informally: \\[\\text{if } a \\rightarrow v \\rightarrow J \\text{, then } \\frac{dJ}{da} = \\frac{dJ}{dv} \\frac{dv}{da}\\] Here, just take \\(\\rightarrow\\) to mean 'effects'. A formal definition of the chain rule can be found here . The amount \\(J\\) changes when you when you nudge \\(a\\) is the product of the amount \\(J\\) changes when you nudge \\(v\\) multiplied by the amount \\(v\\) changes when you nudge \\(a\\) . Implementation note : When writing code to implement backpropagation, there is typically a single output variable you want to optimize, \\(dvar\\), (the value of the cost function). We will follow to notation of calling this variable \\(dvar\\) . If we continue performing backpropagation steps, we can determine the individual contribution a change to the input variables has on the output variable. For example, \\[\\frac{dJ}{db} = \\frac{dJ}{du} \\frac{du}{db} = (3)(2) = 6\\] The key take away from this video is that when computing derivatives to determine the contribution of input variables to change in an output variable, the most efficient way to do so is through a right to left pass through a computation graph. In particular, we'll first compute the derivative with respect to the output of the left-most node in a backward pass, which becomes useful for computing the derivative with respect to the next node and so forth. The chain rule makes the computation of these derivatives tractable. Logistic Regression Gradient Descent Logistic regression recap: \\[z = w^Tx + b\\] \\[\\hat y = a = \\sigma(z)\\] \\[\\ell(a,y) = -(ylog(a) + (1-y)log(1-a))\\] \\(\\ell\\) is our loss for a single example, and \\(\\hat y\\) are our predictions. For this example, lets assume we have only two features: \\(x_1\\), \\(x_2\\) . Our computation graph is thus: Our goal is to modify the parameters to minimize the loss \\(\\ell\\) . This translates to computing derivatives \\(w.r.t\\) the loss function. Following our generic example above, we can compute all the relevant derivatives using the chain rule. The first two passes are computed by the following derivatives: \\(\\frac{d\\ell(a,y)}{da} = - \\frac{y}{a} + \\frac{1-y}{1-a}\\) \\(\\frac{d\\ell(a,y)}{dz} = \\frac{d\\ell(a,y)}{da} \\cdot \\frac{da}{dz} = a - y\\) Note: You should prove these to yourself. Implementation note , we use \\(dx\\) as a shorthand for \\(\\frac{d\\ell(\\hat y,y)}{dx}\\) for some variable \\(x\\) when implementing this in code. Recall that the final step is to determine the derivatives of the loss function \\(w.r.t\\) to the parameters. \\(\\frac{d\\ell(a,y)}{dw_1} = x_1 \\cdot \\frac{d\\ell(a,y)}{dz}\\) \\(\\frac{d\\ell(a,y)}{dw_2} = x_2 \\cdot \\frac{d\\ell(a,y)}{dz}\\) One step of gradient descent would perform the updates: \\(w_1 := w_1 - \\alpha \\frac{d\\ell(a,y)}{dw_1}\\) \\(w_2 := w_2 - \\alpha \\frac{d\\ell(a,y)}{dw_2}\\) \\(b := b - \\alpha \\frac{d\\ell(a,y)}{db}\\) Extending to \\(m\\) examples Lets first remind ourself of the logistic regression cost function: \\[J(w,b) = \\frac{1}{m}\\sum^m_{i=1} \\ell(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m}\\sum^m_{i=1}(y^{(i)} \\; log\\; \\hat y^{(i)} + (1-y^{(i)}) \\;log(1-\\hat y^{(i)}))\\] Where, \\[\\hat y = a = \\sigma(z) = \\sigma(w^Tx^{(i)} + b)\\] In the example above for a single training example, we showed that to perform a gradient step we first need to compute the derivatives \\(\\frac{d\\ell(a,y)}{dw_1}, \\frac{d\\ell(a,y)}{dw_2}, \\frac{d\\ell(a,y)}{db}\\) . For \\(m\\) examples, these are computed as follows: \\(\\frac{\\partial\\ell(a,y)}{\\partial dw_1} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial w_1} \\ell(\\hat y^{(i)}, y^{(i)})\\) \\(\\frac{\\partial\\ell(a,y)}{\\partial w_2} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial w_2} \\ell(\\hat y^{(i)}, y^{(i)})\\) \\(\\frac{\\partial\\ell(a,y)}{\\partial b} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial b} \\ell(\\hat y^{(i)}, y^{(i)})\\) We have already shown on the previous slide how to compute \\(\\frac{\\partial}{\\partial w_1} \\ell(\\hat y^{(i)}, y^{(i)}), \\frac{\\partial}{\\partial w_2} \\ell(\\hat y^{(i)}, y^{(i)})\\) and \\(\\frac{\\partial}{\\partial b} \\ell(\\hat y^{(i)}, y^{(i)})\\) . Gradient descent for \\(m\\) examples essentially involves computing these derivatives for each input example \\(x^{(i)}\\) and averaging the result before performing the gradient step. Concretely, the pseudo-code for gradient descent on \\(m\\) examples of \\(n=2\\) features follows: ALGO Initialize \\(J=0; dw_1 = 0; dw_2 = 0; db = 0\\) for \\(i=1\\) to \\(m\\): \\(z^{(i)} = w^Tx^{(i)}\\) \\(a^{(i)} = \\sigma(z^{(i)})\\) \\(J \\text{+= } -[y^{(i)}log(a^{(i)}) + (1-y^{(i)})log(1-a^{(i)})]\\) \\(dz^{(i)} = a^{(i)} - y^{(i)}\\) for \\(j = 1\\) to \\(n\\) \\(dw_j \\text{+= } x_j^{(i)}dz^{(i)}\\) \\(dw_j \\text{+= } x_j^{(i)}dz^{(i)}\\) \\(db \\text{+= } dz^{(i)}\\) \\(J \\text{/= } m;\\; dw_1 \\text{/= } m;\\; dw_2 \\text{/= } m;\\;db \\text{/= } m\\) In plain english, for each training example, we use the sigmoid function to compute its activation, accumulate a loss for that example based on the current parameters, compute the derivative of the current cost function \\(w.r.t\\) the activation function, and update our parameters and bias. Finally we take the average of our cost function and our gradients. Finally, we use our derivatives to update our parameters, \\(w_1 := w_1 - \\alpha \\cdot {dw_1}\\) \\(w_2 := w_2 - \\alpha \\cdot {dw_2}\\) \\(b := b - \\alpha \\cdot {db}\\) This constitutes one step of gradient descent. The main problem with this implementation is the nested for loops. For deep learning, which requires very large training sets, explicit for loops will make our implementation very slow. Vectorizing this algorithm will greatly speed up our algorithms running time. Vectorization Vectorization is basically the art of getting rid of explicit for loops. In practice, deep learning requires large datasets (at least to obtain high performance). Explicit for loops lead to computational overhead that significantly slows down the training process. The main reason vectorization makes such a dramatic difference is that it allows us to take advantage of parallelization . The rule of thumb to remember is: whenever possible, avoid explicit for-loops . In a toy example were \\(n_x\\) is \\(10^6\\), and \\(w, x^{(i)}\\) are random values, vectorization leads to an approximately 300X speed up to compute all \\(z^{(i)}\\) Lets take a look at some explicit examples: Multiply a matrix by a vector , e.g., \\(u = Av\\) . So, \\(u_i = \\sum_jA_{ij}v_j\\) . Instead of using for nested loops, use: u = np.dot(A,v) Apply exponential operation on every element of a matrix/vector \\(v\\). Again, use libraries such as numpy to perform this with a single operation, e.g., u = np.exp(v) This example applies to almost all operations, np.log(v) , np.abs(v) , np.max(v) , etc... Example: Vectorization of Logistic Regression Forward pass Lets first review the forward pass of logistic regression for \\(m\\) examples: \\(z^{(1)} = w^Tx^{(1)} + b\\); \\(a^{(1)} = \\sigma(z^{1})\\), \\(...\\) , \\(z^{(m)} = w^Tx^{(m)} + b\\); \\(a^{(m)} = \\sigma(z^{m})\\) In logistic regression, we need to compute \\(z^{(i)} = w^Tx^{(i)}+b\\) for each input example \\(x^{(i)}\\) . Instead of using a for loop over each \\(i\\) in range \\((m)\\) we can use a vectorized implementation to compute z directly. Our vectors are of the dimensions: \\(w \\in \\mathbb R^{n_x}\\), \\(b \\in \\mathbb R^{n_x}\\), \\(x \\in \\mathbb R^{n_x}\\). Our parameter vector, bias vector, and design matrix are, \\(w = \\begin{bmatrix}w_1 \\\\ ... \\\\ w_{n_x}\\end{bmatrix}\\), \\(b = \\begin{bmatrix}b_1 \\\\ ... \\\\ b_{n_x}\\end{bmatrix}\\), \\(X = \\begin{bmatrix}x_1^{(1)} & ... & x_1^{(m)} \\\\ ... \\\\ x^{(1)}_{n_x}\\end{bmatrix}\\) So, \\(w^T \\cdot X + b = w^Tx^{(i)} + b\\) (for all \\(i\\)). Thus we can compute all \\(w^Tx^{(i)}\\) in one operation if we vectorize! In numpy code: Z = np.dot(w.T, X) + b Note, \\(+ b\\) will perform element-wise addition in python, and is an example of broadcasting . Where \\(Z\\) is a row vector \\([z^{(1)}, ..., z^{(m)}]\\) . Backward pass Recall, for the gradient computation, we computed the following derivatives: \\(dz^{(1)} = a^{(1)} - y^{(1)} ... dz^{(m)} = a^{(m)} - y^{(m)}\\) We define a row vector, \\(dZ = [dz^{(1)}, ..., dz^{(m)}]\\) . From which it is trivial to see that, \\(dZ = A - Y\\), where \\(A = [a^{(1)}, ..., a^{(m)}]\\) and \\(Y = [y^{(1)}, ..., y^{(m)}]\\) . This is an element-wise subtraction, \\(a^{(1)} - y^{(1)}, ..., a^{(m)} - y^{(m)}\\) that produces a \\(m\\) length row vector. We can then compute our average derivatives of the cost function \\(w.r.t\\) to the parameters in two lines of codes, db = 1/m * np.sum(dZ) dw = 1/m * np.dot(X, dZ.T) Finally, we compare our non-vectorized approach to linear regression vs our vectorized approaches Non-vectorized Approach Vectorized Approach Two for loops, one over the training examples \\(x^{(i)}\\) and a second over the features \\(x^{(i)}_j\\) . We have omitted the outermost loop that iterates over gradient steps. Note that, we still need a single for loop to iterate over each gradient step (regardless if we are using stochastic or mini-batch gradient descent) even in our vectorized approach. Broadcasting Lets motivate the usefulness of broadcasting with an example. Lets say you wanted to get the percent of total calories from carbs, proteins, and fats for multiple foods. Can we do this without an explicit for loop? Set this matrix to a (3,4) numpy matrix A . import numy as np # some numpy array of shape (3,4) A = np.array([ [...], [...], [...] ]) cal = A.sum(axis=0) # get column-wise sums percentage = 100 * A / cal.reshape(1,4) # get percentage of total calories So, we took a (3,4) matrix A and divided it by a (1,4) matrix cal . This is an example of broadcasting. The general principle of broadcast can be summed up as follows: \\((m,n) \\text{ [+ OR - OR * OR /] } (1, n) \\Rightarrow (m,n) \\text{ [+ OR - OR * OR /] } (m \\text{ copies}, n)\\) \\((m,n) \\text{ [+ OR - OR * OR /] } (m, 1) \\Rightarrow (m,n) \\text{ [+ OR - OR * OR /] } (m, n \\text{ copies})\\) Where \\((m, n), (1, n)\\) are matrices, and the operations are performed element-wise after broadcasting. More broadcasting examples Addition Example 1 : \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{bmatrix} + 100 == \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{bmatrix} + \\begin{bmatrix}100 \\\\ 100 \\\\ 100 \\\\ 100\\end{bmatrix} = \\begin{bmatrix}101 \\\\ 102 \\\\ 103 \\\\ 104\\end{bmatrix}\\) Example 2 : \\(\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 200 & 300\\end{bmatrix} == \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 200 & 300 \\\\ 100 & 200 & 300\\end{bmatrix} = \\begin{bmatrix}101 & 202 & 303 \\\\ 104 & 205 & 306\\end{bmatrix}\\) Example 3 : \\(\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 \\\\ 200\\end{bmatrix} == \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 100 & 100 \\\\ 200 & 200 & 200\\end{bmatrix} = \\begin{bmatrix}101 & 202 & 303 \\\\ 104 & 205 & 206\\end{bmatrix}\\) (AISDE) A note on python/numpy vectors The great flexibility of the python language paired with the numpy library is both a strength and a weakness. It is a strength because of the great expressivity of the pair, but with this comes the opportunity to intro strange, hard-to-catch bugs if you aren't familiar with the intricacies of numpy and in particular broadcasting. Here are a couple of tips and tricks to minimize the number of these bugs: Creating a random array: a = np.random.randn(5) Arrays of shape (x, ) are known as rank 1 array . They have some nonintuitive properties and don't consistently behave like either a column vector or a row vector. Let b be a rank 1 array. b.T == b np.dot(b, b.T) is a real number, not the outer product as you might expect . Thus, in this class at least, using rank 1 tensors with an unspecified dimension length is not generally advised. Always specify both dimensions . If you know the size that your numpy arrays should be in advance, its often useful to throw in a python assertion to help catch strange bugs before they happen: assert(a.shape == (5,1)) Additionally, the reshape function runs in linear time and is thus very cheap to call, use it freely! a = a.reshape((5,1))","title":"Week 2"},{"location":"neural_networks_and_deep_learning/week_2/#week-2-neural-networks-basics","text":"","title":"Week 2: Neural networks basics"},{"location":"neural_networks_and_deep_learning/week_2/#binary-classification","text":"First, some notation, \\(n\\) is the number of data attributes, or features \\(m\\) is the number of input examples in our dataset (sometimes we write \\(m_{train}, m_{test}\\) to be more explicit). our data is represented as input, output pairs, \\((x^{(1)},y^{(1)}), ..., (x^{(m)},y^{(m)})\\) where \\(x \\in \\mathbb R^n\\) , \\(y \\in {0,1}\\) \\(X\\) is our design matrix, which is simply columns of our input vectors \\(x^{(i)}\\) , thus it has dimensions of \\(n\\) x \\(m\\) . \\(Y = [y^{(1)}, ..., y^{(m)}]\\) , and is thus a \\(1\\) x \\(m\\) matrix. Note, this is different from many other courses which represent the design matrix, \\(X\\) as rows of transposed input vectors and the output vector \\(Y\\) as a \\(m\\) x \\(1\\) column vector. The above convention turns out to be easier to implement. When programming neural networks, implementation details become extremely important ( e.g . vectorization in place of for loops). We are going to introduce many of the key concepts of neural networks using logistic regression , as this will make them easier to understand. Logistic regression is an algorithm for binary classification . In binary classification, we have an input ( e.g . an image) that we want to classifying as belonging to one of two classes.","title":"Binary Classification"},{"location":"neural_networks_and_deep_learning/week_2/#logistic-regression-crash-course","text":"Given an input feature vector \\(x\\) (perhaps corresponding to an images flattened pixel data), we want \\(\\hat y\\) , the probability of the input examples class, \\(\\hat y = P(y=1 | x)\\) If \\(x\\) is a picture, we want the chance that this is a picture of a cat, \\(\\hat y\\) . The parameters of our model are \\(w \\in \\mathbb R^{n_x}\\) , \\(b \\in \\mathbb R\\) . Our output is \\(\\hat y = \\sigma(w^Tx + b)\\) were \\(\\sigma\\) is the sigmoid function . The formula for the sigmoid function is given by: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) where \\(z = w^Tx + b\\) . We notice a few things: If \\(z\\) is very large, \\(e^{-z}\\) will be close to \\(0\\) , and so \\(\\sigma(z)\\) is very close to \\(1\\) . If \\(z\\) is very small, \\(e^{-z}\\) will grow very large, and so \\(\\sigma(z)\\) is very close to \\(0\\) . It helps to look at the plot \\(y = e^{-x}\\) Thus, logistic regression attempts to learn parameters which will classify images based on their probability of belonging to one class or the other. The classification decision is decided by applying the sigmoid function to \\(w^Tx + b\\) . Note, with neural networks, it is easier to keep the weights \\(w\\) and the biases \\(b\\) separate. Another notation involves adding an extra parameters (\\(w_0\\) which plays the role of the bias. Loss function Our prediction for a given example \\(x^{(i)}\\) is \\(\\hat y^{(i)} = \\sigma(w^Tx^{(i)} + b)\\) . We chose loss function , \\(\\ell(\\hat y, y) = -(y \\; log\\; \\hat y + (1-y) \\;log(1-\\hat y))\\). We note that: If \\(y=1\\) , then the loss function is \\(\\ell(\\hat y, y) = -log\\; \\hat y\\) . Thus, the loss approaches zero as \\(\\hat y\\) approaches 1. If \\(y=0\\) , then the loss function is \\(\\ell(\\hat y, y) = -log\\; (1 -\\hat y)\\) . Thus, the loss approaches zero as \\(\\hat y\\) approaches 0. Note, while \\(\\ell_2\\) loss is taught in many courses and seems like an appropriate choice, it is non-convex and so we cannot use gradient descent to optimize it. An optional video is given further justifying the use of this loss function. Watch it and add notes here! Note that the loss function measures how well we are doing on a single example . We now define a cost function , which captures how well we are doing on the entire dataset: \\(J(w,b) = \\frac{1}{m}\\sum^m_{i=1} \\ell(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m}\\sum^m_{i=1}(y^{(i)} \\; log\\; \\hat y^{(i)} + (1-y^{(i)}) \\;log(1-\\hat y^{(i)}))\\) Note that this notation is somewhat unique, typically the cost/loss functions are just interchangeable terms. However in this course, we will define the loss function as computing the error for a single training example and the cost function as the average of the loss functions of the entire training set.","title":"Logistic Regression (Crash course)"},{"location":"neural_networks_and_deep_learning/week_2/#gradient-descent","text":"We want to find \\(w,b\\) which minimize \\(J(w,b)\\) . We can plot the cost function with \\(w\\) and \\(b\\) as our horizontal axes: In practice, \\(w\\) typically has many more dimensions. Thus, the cost function \\(J(w,b)\\) can be thought of as a surface, were the height of the surface above the horizontal axes is its value. We want to find the values of our parameters \\(w, b\\) at the lowest point of this surface, the point at which the average loss is at its minimum. Gradient Descent Algorithm Initialize \\(w,b\\) to some random values because this cost function is convex, it doesn't matter what values we use to initialize, \\(0\\) is usually chosen for logistic regression. Repeat \\(w := w - \\alpha \\frac{dJ(w)}{dw}\\) \\(b := b - \\alpha \\frac{dJ(w)}{db}\\) \\(\\alpha\\) is our learning rate, it controls how big a step we take on each iteration. In some notations, we use \\(\\partial\\) to denote the partial derivative of a function with \\(2\\) or more variables, and \\(d\\) to denote the derivative of a function of only \\(1\\) variable. When implementing gradient descent in code, we will use the variable \\(dw\\) to represent \\(\\frac{dJ(w, b)}{dw}\\) (this size of the step for \\(w\\) and \\(db\\) to represent \\(\\frac{dJ(w, b)}{db}\\) (the size of the step for \\(b\\) .","title":"Gradient Descent"},{"location":"neural_networks_and_deep_learning/week_2/#aside-calculus-review","text":"Intuition about derivatives","title":"(ASIDE) Calculus Review"},{"location":"neural_networks_and_deep_learning/week_2/#linear-function-example","text":"Take the function \\(f(a) = 3a\\). Then \\(f(a) = 6\\) when \\(a = 2\\) . If we were to give \\(a\\) a tiny nudge, say to \\(a = 2.001\\) , what happens to \\(f(a)\\) ? Then \\(f(a) = 6.003\\) , but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \\(a\\) and \\(a + 0.001\\) as the \\(\\frac{height}{width} = 3\\) . Thus, the derivative (or slope) of \\(f(a)\\) w.r.t \\(a\\) is \\(3\\) . We say that \\(\\frac{df(a)}{da} = 3\\) or \\(\\frac{d}{da}f(a) = 3\\) Add my calculus notes here! Link to BlueBrown videos.","title":"Linear Function Example"},{"location":"neural_networks_and_deep_learning/week_2/#non-linear-function-example","text":"Take the function \\(f(a) = a^2\\) . Then \\(f(a) = 4\\) when \\(a = 2\\) . If we were to give \\(a\\) a tiny nudge, say to \\(a = 2.001\\), what happens to \\(f(a)\\)? Then \\(f(a) = 4.004\\), but more importantly if we inspect the triangle formed by performing the nudge, we can get the slope of the function between \\(a\\) and \\(a + 0.001\\) as the \\(\\frac{height}{width} = 4\\) . In a similar way, we can perform this analysis for any point \\(a\\) on the plot, and we will see that slope of \\(f(a)\\) at some point \\(a\\) is equal to \\(2a\\) . Thus, the derivative (or slope) of \\(f(a)\\) w.r.t \\(a\\) is \\(2a\\) . We say that \\(\\frac{df(a)}{da} = 2a\\) or \\(\\frac{d}{da}f(a) = 2a\\) .","title":"Non-Linear Function Example"},{"location":"neural_networks_and_deep_learning/week_2/#computation-graph","text":"A computation graph organizes a series of computations into left-to-right and right-to-left passes. Lets build the intuition behind a computation graph. Say we are trying to compute a function \\(J(a,b,c) = 3(a + bc)\\) . This computation of this function actually has three discrete steps: compute \\(u = bc\\) compute \\(v = a + u\\) compute J = \\(3v\\) We can draw this computation in a graph: The computation graph is useful when you have some variable or output variable that you want to optimize (\\(J\\) in this case, in logistic regression it would be our cost function output ). A forward pass through the graph is represented by left-to-right arrows (as drawn above) and a backwards pass is represented by right-to-left arrows. A backwards pass is a natural way to represent the computation of our derivatives. Derivatives with a computation graph Lets take a look at our computation graph, and see how we can use it to compute the partial derivatives of \\(J\\) i.e., lets carry out backpropogation on this computation graph by hand. Informally, you can think of this as asking: \"If we were to change the value of \\(v\\) slightly, how would \\(J\\) change?\" First, we use our informal way of computing derivatives, and note that a small change to \\(v\\) results in a change to \\(J\\) of 3X that small change, and so \\(\\frac{dJ}{dv} = 3\\) . This represents one step in our backward pass, the first step in backpropagation. Now let's look at another example. What is \\(\\frac{dJ}{da}\\)? We compute the \\(\\frac{dJ}{da}\\) from the second node in the computation graph by noting that a small change to \\(a\\) results in a change to \\(J\\) of 3X that small change, and so \\(\\frac{dJ}{da} = 3.\\) This represents our second step in our backpropagation. One way to break this down is to say that by changing \\(a\\), we change \\(v\\), the magnitude of this change is \\(\\frac{dv}{da}\\) . Through this change in \\(v\\), we also change \\(J\\), and the magnitude of the change is \\(\\frac{dJ}{dv}\\) . To capture this more generally, we use the chain rule from calculus, informally: \\[\\text{if } a \\rightarrow v \\rightarrow J \\text{, then } \\frac{dJ}{da} = \\frac{dJ}{dv} \\frac{dv}{da}\\] Here, just take \\(\\rightarrow\\) to mean 'effects'. A formal definition of the chain rule can be found here . The amount \\(J\\) changes when you when you nudge \\(a\\) is the product of the amount \\(J\\) changes when you nudge \\(v\\) multiplied by the amount \\(v\\) changes when you nudge \\(a\\) . Implementation note : When writing code to implement backpropagation, there is typically a single output variable you want to optimize, \\(dvar\\), (the value of the cost function). We will follow to notation of calling this variable \\(dvar\\) . If we continue performing backpropagation steps, we can determine the individual contribution a change to the input variables has on the output variable. For example, \\[\\frac{dJ}{db} = \\frac{dJ}{du} \\frac{du}{db} = (3)(2) = 6\\] The key take away from this video is that when computing derivatives to determine the contribution of input variables to change in an output variable, the most efficient way to do so is through a right to left pass through a computation graph. In particular, we'll first compute the derivative with respect to the output of the left-most node in a backward pass, which becomes useful for computing the derivative with respect to the next node and so forth. The chain rule makes the computation of these derivatives tractable.","title":"Computation Graph"},{"location":"neural_networks_and_deep_learning/week_2/#logistic-regression-gradient-descent","text":"Logistic regression recap: \\[z = w^Tx + b\\] \\[\\hat y = a = \\sigma(z)\\] \\[\\ell(a,y) = -(ylog(a) + (1-y)log(1-a))\\] \\(\\ell\\) is our loss for a single example, and \\(\\hat y\\) are our predictions. For this example, lets assume we have only two features: \\(x_1\\), \\(x_2\\) . Our computation graph is thus: Our goal is to modify the parameters to minimize the loss \\(\\ell\\) . This translates to computing derivatives \\(w.r.t\\) the loss function. Following our generic example above, we can compute all the relevant derivatives using the chain rule. The first two passes are computed by the following derivatives: \\(\\frac{d\\ell(a,y)}{da} = - \\frac{y}{a} + \\frac{1-y}{1-a}\\) \\(\\frac{d\\ell(a,y)}{dz} = \\frac{d\\ell(a,y)}{da} \\cdot \\frac{da}{dz} = a - y\\) Note: You should prove these to yourself. Implementation note , we use \\(dx\\) as a shorthand for \\(\\frac{d\\ell(\\hat y,y)}{dx}\\) for some variable \\(x\\) when implementing this in code. Recall that the final step is to determine the derivatives of the loss function \\(w.r.t\\) to the parameters. \\(\\frac{d\\ell(a,y)}{dw_1} = x_1 \\cdot \\frac{d\\ell(a,y)}{dz}\\) \\(\\frac{d\\ell(a,y)}{dw_2} = x_2 \\cdot \\frac{d\\ell(a,y)}{dz}\\) One step of gradient descent would perform the updates: \\(w_1 := w_1 - \\alpha \\frac{d\\ell(a,y)}{dw_1}\\) \\(w_2 := w_2 - \\alpha \\frac{d\\ell(a,y)}{dw_2}\\) \\(b := b - \\alpha \\frac{d\\ell(a,y)}{db}\\) Extending to \\(m\\) examples Lets first remind ourself of the logistic regression cost function: \\[J(w,b) = \\frac{1}{m}\\sum^m_{i=1} \\ell(\\hat y^{(i)}, y^{(i)}) = - \\frac{1}{m}\\sum^m_{i=1}(y^{(i)} \\; log\\; \\hat y^{(i)} + (1-y^{(i)}) \\;log(1-\\hat y^{(i)}))\\] Where, \\[\\hat y = a = \\sigma(z) = \\sigma(w^Tx^{(i)} + b)\\] In the example above for a single training example, we showed that to perform a gradient step we first need to compute the derivatives \\(\\frac{d\\ell(a,y)}{dw_1}, \\frac{d\\ell(a,y)}{dw_2}, \\frac{d\\ell(a,y)}{db}\\) . For \\(m\\) examples, these are computed as follows: \\(\\frac{\\partial\\ell(a,y)}{\\partial dw_1} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial w_1} \\ell(\\hat y^{(i)}, y^{(i)})\\) \\(\\frac{\\partial\\ell(a,y)}{\\partial w_2} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial w_2} \\ell(\\hat y^{(i)}, y^{(i)})\\) \\(\\frac{\\partial\\ell(a,y)}{\\partial b} = \\frac{1}{m}\\sum^m_{i=1} \\frac{\\partial}{\\partial b} \\ell(\\hat y^{(i)}, y^{(i)})\\) We have already shown on the previous slide how to compute \\(\\frac{\\partial}{\\partial w_1} \\ell(\\hat y^{(i)}, y^{(i)}), \\frac{\\partial}{\\partial w_2} \\ell(\\hat y^{(i)}, y^{(i)})\\) and \\(\\frac{\\partial}{\\partial b} \\ell(\\hat y^{(i)}, y^{(i)})\\) . Gradient descent for \\(m\\) examples essentially involves computing these derivatives for each input example \\(x^{(i)}\\) and averaging the result before performing the gradient step. Concretely, the pseudo-code for gradient descent on \\(m\\) examples of \\(n=2\\) features follows: ALGO Initialize \\(J=0; dw_1 = 0; dw_2 = 0; db = 0\\) for \\(i=1\\) to \\(m\\): \\(z^{(i)} = w^Tx^{(i)}\\) \\(a^{(i)} = \\sigma(z^{(i)})\\) \\(J \\text{+= } -[y^{(i)}log(a^{(i)}) + (1-y^{(i)})log(1-a^{(i)})]\\) \\(dz^{(i)} = a^{(i)} - y^{(i)}\\) for \\(j = 1\\) to \\(n\\) \\(dw_j \\text{+= } x_j^{(i)}dz^{(i)}\\) \\(dw_j \\text{+= } x_j^{(i)}dz^{(i)}\\) \\(db \\text{+= } dz^{(i)}\\) \\(J \\text{/= } m;\\; dw_1 \\text{/= } m;\\; dw_2 \\text{/= } m;\\;db \\text{/= } m\\) In plain english, for each training example, we use the sigmoid function to compute its activation, accumulate a loss for that example based on the current parameters, compute the derivative of the current cost function \\(w.r.t\\) the activation function, and update our parameters and bias. Finally we take the average of our cost function and our gradients. Finally, we use our derivatives to update our parameters, \\(w_1 := w_1 - \\alpha \\cdot {dw_1}\\) \\(w_2 := w_2 - \\alpha \\cdot {dw_2}\\) \\(b := b - \\alpha \\cdot {db}\\) This constitutes one step of gradient descent. The main problem with this implementation is the nested for loops. For deep learning, which requires very large training sets, explicit for loops will make our implementation very slow. Vectorizing this algorithm will greatly speed up our algorithms running time.","title":"Logistic Regression Gradient Descent"},{"location":"neural_networks_and_deep_learning/week_2/#vectorization","text":"Vectorization is basically the art of getting rid of explicit for loops. In practice, deep learning requires large datasets (at least to obtain high performance). Explicit for loops lead to computational overhead that significantly slows down the training process. The main reason vectorization makes such a dramatic difference is that it allows us to take advantage of parallelization . The rule of thumb to remember is: whenever possible, avoid explicit for-loops . In a toy example were \\(n_x\\) is \\(10^6\\), and \\(w, x^{(i)}\\) are random values, vectorization leads to an approximately 300X speed up to compute all \\(z^{(i)}\\) Lets take a look at some explicit examples: Multiply a matrix by a vector , e.g., \\(u = Av\\) . So, \\(u_i = \\sum_jA_{ij}v_j\\) . Instead of using for nested loops, use: u = np.dot(A,v) Apply exponential operation on every element of a matrix/vector \\(v\\). Again, use libraries such as numpy to perform this with a single operation, e.g., u = np.exp(v) This example applies to almost all operations, np.log(v) , np.abs(v) , np.max(v) , etc... Example: Vectorization of Logistic Regression","title":"Vectorization"},{"location":"neural_networks_and_deep_learning/week_2/#forward-pass","text":"Lets first review the forward pass of logistic regression for \\(m\\) examples: \\(z^{(1)} = w^Tx^{(1)} + b\\); \\(a^{(1)} = \\sigma(z^{1})\\), \\(...\\) , \\(z^{(m)} = w^Tx^{(m)} + b\\); \\(a^{(m)} = \\sigma(z^{m})\\) In logistic regression, we need to compute \\(z^{(i)} = w^Tx^{(i)}+b\\) for each input example \\(x^{(i)}\\) . Instead of using a for loop over each \\(i\\) in range \\((m)\\) we can use a vectorized implementation to compute z directly. Our vectors are of the dimensions: \\(w \\in \\mathbb R^{n_x}\\), \\(b \\in \\mathbb R^{n_x}\\), \\(x \\in \\mathbb R^{n_x}\\). Our parameter vector, bias vector, and design matrix are, \\(w = \\begin{bmatrix}w_1 \\\\ ... \\\\ w_{n_x}\\end{bmatrix}\\), \\(b = \\begin{bmatrix}b_1 \\\\ ... \\\\ b_{n_x}\\end{bmatrix}\\), \\(X = \\begin{bmatrix}x_1^{(1)} & ... & x_1^{(m)} \\\\ ... \\\\ x^{(1)}_{n_x}\\end{bmatrix}\\) So, \\(w^T \\cdot X + b = w^Tx^{(i)} + b\\) (for all \\(i\\)). Thus we can compute all \\(w^Tx^{(i)}\\) in one operation if we vectorize! In numpy code: Z = np.dot(w.T, X) + b Note, \\(+ b\\) will perform element-wise addition in python, and is an example of broadcasting . Where \\(Z\\) is a row vector \\([z^{(1)}, ..., z^{(m)}]\\) .","title":"Forward pass"},{"location":"neural_networks_and_deep_learning/week_2/#backward-pass","text":"Recall, for the gradient computation, we computed the following derivatives: \\(dz^{(1)} = a^{(1)} - y^{(1)} ... dz^{(m)} = a^{(m)} - y^{(m)}\\) We define a row vector, \\(dZ = [dz^{(1)}, ..., dz^{(m)}]\\) . From which it is trivial to see that, \\(dZ = A - Y\\), where \\(A = [a^{(1)}, ..., a^{(m)}]\\) and \\(Y = [y^{(1)}, ..., y^{(m)}]\\) . This is an element-wise subtraction, \\(a^{(1)} - y^{(1)}, ..., a^{(m)} - y^{(m)}\\) that produces a \\(m\\) length row vector. We can then compute our average derivatives of the cost function \\(w.r.t\\) to the parameters in two lines of codes, db = 1/m * np.sum(dZ) dw = 1/m * np.dot(X, dZ.T) Finally, we compare our non-vectorized approach to linear regression vs our vectorized approaches Non-vectorized Approach Vectorized Approach Two for loops, one over the training examples \\(x^{(i)}\\) and a second over the features \\(x^{(i)}_j\\) . We have omitted the outermost loop that iterates over gradient steps. Note that, we still need a single for loop to iterate over each gradient step (regardless if we are using stochastic or mini-batch gradient descent) even in our vectorized approach.","title":"Backward pass"},{"location":"neural_networks_and_deep_learning/week_2/#broadcasting","text":"Lets motivate the usefulness of broadcasting with an example. Lets say you wanted to get the percent of total calories from carbs, proteins, and fats for multiple foods. Can we do this without an explicit for loop? Set this matrix to a (3,4) numpy matrix A . import numy as np # some numpy array of shape (3,4) A = np.array([ [...], [...], [...] ]) cal = A.sum(axis=0) # get column-wise sums percentage = 100 * A / cal.reshape(1,4) # get percentage of total calories So, we took a (3,4) matrix A and divided it by a (1,4) matrix cal . This is an example of broadcasting. The general principle of broadcast can be summed up as follows: \\((m,n) \\text{ [+ OR - OR * OR /] } (1, n) \\Rightarrow (m,n) \\text{ [+ OR - OR * OR /] } (m \\text{ copies}, n)\\) \\((m,n) \\text{ [+ OR - OR * OR /] } (m, 1) \\Rightarrow (m,n) \\text{ [+ OR - OR * OR /] } (m, n \\text{ copies})\\) Where \\((m, n), (1, n)\\) are matrices, and the operations are performed element-wise after broadcasting. More broadcasting examples","title":"Broadcasting"},{"location":"neural_networks_and_deep_learning/week_2/#addition","text":"Example 1 : \\(\\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{bmatrix} + 100 == \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{bmatrix} + \\begin{bmatrix}100 \\\\ 100 \\\\ 100 \\\\ 100\\end{bmatrix} = \\begin{bmatrix}101 \\\\ 102 \\\\ 103 \\\\ 104\\end{bmatrix}\\) Example 2 : \\(\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 200 & 300\\end{bmatrix} == \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 200 & 300 \\\\ 100 & 200 & 300\\end{bmatrix} = \\begin{bmatrix}101 & 202 & 303 \\\\ 104 & 205 & 306\\end{bmatrix}\\) Example 3 : \\(\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 \\\\ 200\\end{bmatrix} == \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix} + \\begin{bmatrix}100 & 100 & 100 \\\\ 200 & 200 & 200\\end{bmatrix} = \\begin{bmatrix}101 & 202 & 303 \\\\ 104 & 205 & 206\\end{bmatrix}\\)","title":"Addition"},{"location":"neural_networks_and_deep_learning/week_2/#aisde-a-note-on-pythonnumpy-vectors","text":"The great flexibility of the python language paired with the numpy library is both a strength and a weakness. It is a strength because of the great expressivity of the pair, but with this comes the opportunity to intro strange, hard-to-catch bugs if you aren't familiar with the intricacies of numpy and in particular broadcasting. Here are a couple of tips and tricks to minimize the number of these bugs: Creating a random array: a = np.random.randn(5) Arrays of shape (x, ) are known as rank 1 array . They have some nonintuitive properties and don't consistently behave like either a column vector or a row vector. Let b be a rank 1 array. b.T == b np.dot(b, b.T) is a real number, not the outer product as you might expect . Thus, in this class at least, using rank 1 tensors with an unspecified dimension length is not generally advised. Always specify both dimensions . If you know the size that your numpy arrays should be in advance, its often useful to throw in a python assertion to help catch strange bugs before they happen: assert(a.shape == (5,1)) Additionally, the reshape function runs in linear time and is thus very cheap to call, use it freely! a = a.reshape((5,1))","title":"(AISDE) A note on python/numpy vectors"},{"location":"neural_networks_and_deep_learning/week_3/","text":"Week 3: Shallow neural networks Neural network overview Up until this point, we have used logistic regression as a stand-in for neural networks. The \"network\" we have been describing looked like: Network Computation Graph \\(a\\) and \\(\\hat y\\) are used interchangeably A neural network looks something like this: Network Computation Graph We typically don't distinguish between \\(z\\) and \\(a\\) when talking about neural networks, one neuron = one activation = one \\(a\\) like calculation. We will introduce the notation of superscripting values with \\(^{[l]}\\), where \\(l\\) refers to the layer of the neural network that we are talking about. Not to be confused with \\(^{(i)}\\) which we use to refer to a single input example \\(i\\) . The key intuition is that neural networks stack activations of inputs multiplied by their weights . Similar to the 'backwards' step that we discussed for logistic regression, we will explore the backwards steps that makes learning in a neural network possible. Neural network Representation This is the canonical representation of a neural network On the left, we have the input features stacked vertically. This constitutes our input layer . The final layer, is called the output layer and it is responsible for generating the predicted value \\(\\hat y\\) . Any layer in between these two layers is known as a hidden layer . This name derives from the fact that the true values of these hidden units is not observed in the training set. The hidden layers and output layers have parameters associated with them. These parameters are denoted \\(W^{[l]}\\) and \\(b^{[l]}\\) for layer \\(l\\) . Previously, we were referring to our input examples as \\(x^{(i)}\\) and organizing them in a design matrix \\(X\\) . With neural networks, we will introduce the convention of denoting output values of a layer \\(l\\), as a column vector \\(a^{[l]}\\), where \\(a\\) stands for activation . You can also think of these as the values a layer \\(l\\) passes on to the next layer. Another note: the network shown above is a 2-layer neural network. We typically do not count the input layer. In light of this, we usually denote the input layer as \\(l=0\\). Computing a Neural Networks Output We will use the example of a single hidden layer neural network to demonstrate the forward propagation of inputs through the network leading to the networks output. We can think of each unit in the neural network as performing two steps, the multiplication of inputs by weights and the addition of a bias , and the activation of the resulting value Recall, that we will use a superscript, \\(^{[l]}\\) to denote values belonging to the \\(l-th\\) layer. So, the \\(j^{th}\\) node of the \\(l^{th}\\) layer performs the computation \\[ a_j^{[l]} = \\sigma(w_j^{[l]^T}a^{[l-1]} + b_j^{[l]}) \\] Where \\(a^{[l-1]}\\) is the activation values from the precious layer. for some input \\(x\\). With this notation, we can draw our neural network as follows: In order to easily vectorize the computations we need to perform, we designate a matrix \\(W^{[l]}\\) for each layer \\(l\\), which has dimensions (number of units in current layer X number of units in previous layer) We can vectorize the computation of \\(z^{[l]}\\) as follows: And the computation of \\(a^{[l]}\\) just becomes the element-wise application of the sigmoid function: We can put it all together for our two layer neural network, and outline all the computations using our new notation: Vectorizing across multiple examples In the last video, we saw how to compute the prediction for a neural network with a single input example. In this video, we introduce a vectorized approach to compute predictions for many input examples. We have seen how to take a single input example \\(x\\) and compute \\(a^{[2]} = \\hat y\\) for a 2-layered neural network. If we have \\(m\\) training examples, we can used a vectorized approach to compute all \\(m\\) predictions. First, lets introduce a new notation. The activation values of layer \\(l\\) for input example \\(i\\) is: \\[ a^{[l] (i)} \\] The \\(m\\) predictions our 2-layered are therefore computed in the following way: Recall that \\(X\\) is a \\((n_x, m)\\) design matrix, where each column is a single input example and \\(W^{[l]}\\) is a matrix where each row is the transpose of the parameter column vector for layer \\(l\\). Thus, we can now compute the activation of a layer in the neural network for all training examples: \\[Z^{[l]} = W^{[l]}X + b^{[l]}\\] \\[A^{[l]} = sign(Z^{[l]})\\] As an example, the result of a matrix multiplication of \\(W^{[1]}\\) by \\(X\\) is a matrix with dimensions \\((j, m)\\) where \\(j\\) is the number of units in layer \\(1\\) and \\(m\\) is the number of input examples \\(A^{[l]}\\) is therefore a matrix of dimensions (size of layer \\(l\\) X \\(m\\)). The top-leftmost value is the activation for the first unit in the layer \\(l\\) for the first input example \\(i\\), and the bottom-rightmost value is the activation for the last unit in the layer \\(l\\) for the last input example \\(m\\) . Activation Functions So far, we have been using the sigmoid activation function \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] It turns out there are much better options. Tanh The hyperbolic tangent function is a non-linear activation function that almost always works better than the sigmoid function. \\[tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\] The tanh function is really just a shift of the sigmoid function so that it crosses through the origin. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. The single exception of sigmoid outperforming tanh is when its used in the ouput layer. In this case, it can be more desirable to scale our outputs from \\(0\\) to \\(1\\) (particularly in classification, when we want to output the probability that something belongs to a certain class). Indeed, we often mix activation functions in neural networks, and denote them: \\[g^{[p]}(z)\\] Where \\(p\\) is the \\(p^{th}\\) activation function. If \\(z\\) is either very large, or very small, the derivative of both the tanh and sigmoid functions becomes very small, and this can slow down learning. ReLu The rectified linear unit activation function solves the disappearing gradient problem faced by tanh and sigmoid activation functions. In practice, it also leads to faster learning. \\[ReLu(z) = max(0, z)\\] Note: the derivative at exactly 0 is not well-defined. In practice, we can simply set it to 0 or 1 (it matters little, due to the unlikeliness of a floating point number to ever be \\(0.0000...\\) exactly). One disadvantage of ReLu is that the derivative is equal to \\(0\\) when \\(z\\) is negative. Leaky ReLu 's aim to solve this problem with a slight negative slope for values of \\(z<0\\) . \\[ReLu(z) = max(0.01 * z, z)\\] Image sourced from here . Sometimes, the \\(0.01\\) value is treated as an adaptive parameter of the learning algorithm. Leaky ReLu's solve a more general problem of \" dead neurons \". However, it is not used as much in practice. Rules of thumb for choosing activations functions If your output is a 0/1 value , i.e., you are performing binary classification, the sigmoid activation is a natural choice for the output layer. For all other units , ReLu's is increasingly the default choice of activation function. Why do you need non-linear activation functions? We could imagine using some linear activation function, \\(g(z) = z\\) in place of the non-linear activation functions we have been using so far. Why is this a bad idea? Lets illustrate out explanation using our simple neural networks For this linear activation function, the activations of our simple network become: \\[z^{[1]} = W^{[1]}x + b^{[1]}\\] \\[a^{[1]} = z^{[1]}\\] \\[z^{[2]} = W^{[2]}x + b^{[2]}\\] \\[a^{[2]} = z^{[2]}\\] From which we can show that, \\[a^{[2]} = (W^{[2]}W^{[1]})x + (W^{[2]}b^{[1]} + b^{[2]})\\] \\[a^{[2]} = W'x + b' \\text{, where } W' = W^{[2]}W^{[1]} \\text{ and } b' = W^{[2]}b^{[1]} + b^{[2]}\\] Therefore, in the case of a linear activation function , the neural network is outputting a linear function of the inputs , no matter how many hidden layers! Exceptions There are (maybe) two cases in which you may actually want to use a linear activation function. The output layer of a network used to perform regression, where we want \\(\\hat y\\) to be a real-valued number, \\(\\hat y \\in \\mathbb R\\) Extremely specific cases pertaining to compression. Derivatives of activation functions When performing back-propogation on a network, we need to compute the derivatives of the activation functions. Lets take a look at our activation functions and their derivatives Sigmoid The deriviative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = \\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}})= g(z)(1-g(z)) = a(1-a)\\] We can sanity check this by inputting very large, or very small values of \\(z\\) into our derivative formula and inspecting the size of the outputs. Notice that if we have already computed the value of \\(a\\), we can very cheaply compute the value of \\(g(z)'\\) . Tanh The deriviative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = 1 - (tanh(z))^z\\] Again, we can sanity check this inspecting that the outputs for different values of \\(z\\) match our intuition about the activation function. ReLu The derivative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = 0 \\text{ if } z < 0 ; 1 \\text{ if } z > 0; \\text{ undefined if } z = 0\\] If \\(z = 0\\), we typically default to setting \\(g(z)\\) to either \\(0\\) or \\(1\\) . In practice this matters little. Gradient descent for Neural Networks Lets implement gradient descent for our simple 2-layer neural network. Recall, our parameters are: \\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\) . We have number of features, \\(n_x = n^{[0]}\\), number of hidden units \\(n^{[1]}\\), and \\(n^{[2]}\\) output units. Thus our dimensions: \\(W^{[1]}\\) : (\\(n^{[1]}, n^{[0]}\\)) \\(b^{[1]}\\) : (\\(n^{[1]}, 1\\)) \\(W^{[2]}\\) : (\\(n^{[2]}, n^{[1]}\\)) \\(b^{[2]}\\) : (\\(n^{[2]}, 1\\)) Our cost function is: \\(J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m}\\sum_{i=1}^m \\ell(\\hat y, y)\\) We are assuming binary classification. Gradient Descent sketch Initialize parameters randomly Repeat: compute predictions \\(\\hat y^{(i)}\\) for \\(i = 1 ,..., m\\) \\(dW^{[1]} = \\frac{\\partial J}{\\partial W^{[1]}}, db^{[1]} = \\frac{\\partial J}{\\partial b^{[1]}}, ...\\) \\(W^{[1]} = W^{[1]} - \\alpha dW^{[1]}, ...\\) \\(b^{[1]} = b^{[1]} - \\alpha db^{[1]}, ...\\) The key to gradient descent is to computation of the derivatives, \\(\\frac{\\partial J}{\\partial W^{[l]}}\\) and \\(\\frac{\\partial J}{\\partial b^{[l]}}\\) for all layers \\(l\\) . Formulas for computing derivatives We are going to simply present the formulas you need, and defer their explanation to the next video. Recall the computation graph for our 2-layered neural network: | And the vectorized implementation of our computations in our forward propagation 1.\\[Z^{[1]} = W^{[1]}X + b^{[1]}\\] 2.\\[A^{[1]} = g^{[1]}(Z^{[1]})\\] 3.\\[Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}\\] 4.\\[A^{[2]} = g^{[2]}(Z^{[2]}) = \\sigma(Z^{[2]})\\] Where \\(g^{[2]}\\) would likely be the sigmoid function if we are doing binary classification. Now we list the computations for our backward propagation 1.\\[ dZ^{[2]} = A^{[2]} - Y \\] 2.\\[ dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T} \\] Transpose of A accounts for the fact that W is composed of transposed column vectors of parameters. 3.\\[db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims=True)\\] Where \\(Y = [y^{(1)}, ..., y^{[m]}]\\) . The keepdims arguments prevents numpy from returning a rank 1 array, \\((n,)\\) 4.\\[dZ^{[1]} = W^{[2]T}dZ^{[2]} \\odot g(Z)' (Z^{[1]})\\] Where \\(\\odot\\) is the element-wise product. Note: this is a collapse of \\(dZ\\) and \\(dA\\) computations. 5.\\[dW{[1]} = \\frac{1}{m} = dZ^{[1]}X^T\\] 6.\\[db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\\] Random Initialization When you train your neural network, it is important to initialize your parameters randomly . With logistic regression, we were able to initialize our weights to zero because the cost function was convex. We will see that this will not work with neural networks. Lets take the following network as example: Lets say we initialize our parameters as follows: \\(W^{[1]} = \\begin{bmatrix}0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\), \\(b^{[1]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\), \\(W^{[2]} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\), \\(b^{[2]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) It turns out that initializing the bias \\(b\\) with zeros is OK. The problem with this initialization is that for any input examples \\(i, j\\), \\[a^{[1]}_i == a^{[1]}_j\\] Similarly, \\[dz^{[1]}_i == dz^{[1]}_j\\] Thus, \\(dW^{[1]}\\) will be some matrix \\(\\begin{bmatrix}u & v \\\\ u & v\\end{bmatrix}\\) and all updates to the parameters \\(W^{[1]}\\) will be identical. Note we are referring to our single hidden layer \\(^{[1]}\\) but this would apply to any hidden layer of any fully-connected network, no matter how large. Using a proof by induction , it is actually possible to prove that after any number of rounds of training the two hidden units are still computing identical functions . This is often called the symmetry breaking problem . The solution to this problem, is to initialize parameters randomly . Heres an example on how to do that with numpy: \\(W^{[1]}\\) = np.random.rand(2,2) * 0.01 \\(W^{[2]}\\) = np.random.rand(1,2) * 0.01 ... This will generate small, gaussian random values. \\(b^{[1]}\\) = np.zeros((2,1)) \\(b^{[2]}\\) = 0 ... In next weeks material, we will talk about how and when you might choose a different factor than \\(0.01\\) for initialization. It turns out the \\(b\\) does not have this symmetry breaking problem, because as long as the hidden units are computing different functions, the network will converge on different values of \\(b\\), and so it is fine to initialize it to zeros. Why do we initialize to small values? For a sigmoid-like activation function, large parameter weights (positive or negative) will make it more likely that \\(z\\) is very large (positive or negative) and thus \\(dz\\) will approach \\(0\\), slowing down learning dramatically . Note this is a less of an issue when using ReLu's, however many classification problems use sigmoid activations in their output layer.","title":"Week 3"},{"location":"neural_networks_and_deep_learning/week_3/#week-3-shallow-neural-networks","text":"","title":"Week 3: Shallow neural networks"},{"location":"neural_networks_and_deep_learning/week_3/#neural-network-overview","text":"Up until this point, we have used logistic regression as a stand-in for neural networks. The \"network\" we have been describing looked like: Network Computation Graph \\(a\\) and \\(\\hat y\\) are used interchangeably A neural network looks something like this: Network Computation Graph We typically don't distinguish between \\(z\\) and \\(a\\) when talking about neural networks, one neuron = one activation = one \\(a\\) like calculation. We will introduce the notation of superscripting values with \\(^{[l]}\\), where \\(l\\) refers to the layer of the neural network that we are talking about. Not to be confused with \\(^{(i)}\\) which we use to refer to a single input example \\(i\\) . The key intuition is that neural networks stack activations of inputs multiplied by their weights . Similar to the 'backwards' step that we discussed for logistic regression, we will explore the backwards steps that makes learning in a neural network possible.","title":"Neural network overview"},{"location":"neural_networks_and_deep_learning/week_3/#neural-network-representation","text":"This is the canonical representation of a neural network On the left, we have the input features stacked vertically. This constitutes our input layer . The final layer, is called the output layer and it is responsible for generating the predicted value \\(\\hat y\\) . Any layer in between these two layers is known as a hidden layer . This name derives from the fact that the true values of these hidden units is not observed in the training set. The hidden layers and output layers have parameters associated with them. These parameters are denoted \\(W^{[l]}\\) and \\(b^{[l]}\\) for layer \\(l\\) . Previously, we were referring to our input examples as \\(x^{(i)}\\) and organizing them in a design matrix \\(X\\) . With neural networks, we will introduce the convention of denoting output values of a layer \\(l\\), as a column vector \\(a^{[l]}\\), where \\(a\\) stands for activation . You can also think of these as the values a layer \\(l\\) passes on to the next layer. Another note: the network shown above is a 2-layer neural network. We typically do not count the input layer. In light of this, we usually denote the input layer as \\(l=0\\).","title":"Neural network Representation"},{"location":"neural_networks_and_deep_learning/week_3/#computing-a-neural-networks-output","text":"We will use the example of a single hidden layer neural network to demonstrate the forward propagation of inputs through the network leading to the networks output. We can think of each unit in the neural network as performing two steps, the multiplication of inputs by weights and the addition of a bias , and the activation of the resulting value Recall, that we will use a superscript, \\(^{[l]}\\) to denote values belonging to the \\(l-th\\) layer. So, the \\(j^{th}\\) node of the \\(l^{th}\\) layer performs the computation \\[ a_j^{[l]} = \\sigma(w_j^{[l]^T}a^{[l-1]} + b_j^{[l]}) \\] Where \\(a^{[l-1]}\\) is the activation values from the precious layer. for some input \\(x\\). With this notation, we can draw our neural network as follows: In order to easily vectorize the computations we need to perform, we designate a matrix \\(W^{[l]}\\) for each layer \\(l\\), which has dimensions (number of units in current layer X number of units in previous layer) We can vectorize the computation of \\(z^{[l]}\\) as follows: And the computation of \\(a^{[l]}\\) just becomes the element-wise application of the sigmoid function: We can put it all together for our two layer neural network, and outline all the computations using our new notation:","title":"Computing a Neural Networks Output"},{"location":"neural_networks_and_deep_learning/week_3/#vectorizing-across-multiple-examples","text":"In the last video, we saw how to compute the prediction for a neural network with a single input example. In this video, we introduce a vectorized approach to compute predictions for many input examples. We have seen how to take a single input example \\(x\\) and compute \\(a^{[2]} = \\hat y\\) for a 2-layered neural network. If we have \\(m\\) training examples, we can used a vectorized approach to compute all \\(m\\) predictions. First, lets introduce a new notation. The activation values of layer \\(l\\) for input example \\(i\\) is: \\[ a^{[l] (i)} \\] The \\(m\\) predictions our 2-layered are therefore computed in the following way: Recall that \\(X\\) is a \\((n_x, m)\\) design matrix, where each column is a single input example and \\(W^{[l]}\\) is a matrix where each row is the transpose of the parameter column vector for layer \\(l\\). Thus, we can now compute the activation of a layer in the neural network for all training examples: \\[Z^{[l]} = W^{[l]}X + b^{[l]}\\] \\[A^{[l]} = sign(Z^{[l]})\\] As an example, the result of a matrix multiplication of \\(W^{[1]}\\) by \\(X\\) is a matrix with dimensions \\((j, m)\\) where \\(j\\) is the number of units in layer \\(1\\) and \\(m\\) is the number of input examples \\(A^{[l]}\\) is therefore a matrix of dimensions (size of layer \\(l\\) X \\(m\\)). The top-leftmost value is the activation for the first unit in the layer \\(l\\) for the first input example \\(i\\), and the bottom-rightmost value is the activation for the last unit in the layer \\(l\\) for the last input example \\(m\\) .","title":"Vectorizing across multiple examples"},{"location":"neural_networks_and_deep_learning/week_3/#activation-functions","text":"So far, we have been using the sigmoid activation function \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] It turns out there are much better options.","title":"Activation Functions"},{"location":"neural_networks_and_deep_learning/week_3/#tanh","text":"The hyperbolic tangent function is a non-linear activation function that almost always works better than the sigmoid function. \\[tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\] The tanh function is really just a shift of the sigmoid function so that it crosses through the origin. The tanh activation usually works better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data better for the next layer. The single exception of sigmoid outperforming tanh is when its used in the ouput layer. In this case, it can be more desirable to scale our outputs from \\(0\\) to \\(1\\) (particularly in classification, when we want to output the probability that something belongs to a certain class). Indeed, we often mix activation functions in neural networks, and denote them: \\[g^{[p]}(z)\\] Where \\(p\\) is the \\(p^{th}\\) activation function. If \\(z\\) is either very large, or very small, the derivative of both the tanh and sigmoid functions becomes very small, and this can slow down learning.","title":"Tanh"},{"location":"neural_networks_and_deep_learning/week_3/#relu","text":"The rectified linear unit activation function solves the disappearing gradient problem faced by tanh and sigmoid activation functions. In practice, it also leads to faster learning. \\[ReLu(z) = max(0, z)\\] Note: the derivative at exactly 0 is not well-defined. In practice, we can simply set it to 0 or 1 (it matters little, due to the unlikeliness of a floating point number to ever be \\(0.0000...\\) exactly). One disadvantage of ReLu is that the derivative is equal to \\(0\\) when \\(z\\) is negative. Leaky ReLu 's aim to solve this problem with a slight negative slope for values of \\(z<0\\) . \\[ReLu(z) = max(0.01 * z, z)\\] Image sourced from here . Sometimes, the \\(0.01\\) value is treated as an adaptive parameter of the learning algorithm. Leaky ReLu's solve a more general problem of \" dead neurons \". However, it is not used as much in practice. Rules of thumb for choosing activations functions If your output is a 0/1 value , i.e., you are performing binary classification, the sigmoid activation is a natural choice for the output layer. For all other units , ReLu's is increasingly the default choice of activation function. Why do you need non-linear activation functions? We could imagine using some linear activation function, \\(g(z) = z\\) in place of the non-linear activation functions we have been using so far. Why is this a bad idea? Lets illustrate out explanation using our simple neural networks For this linear activation function, the activations of our simple network become: \\[z^{[1]} = W^{[1]}x + b^{[1]}\\] \\[a^{[1]} = z^{[1]}\\] \\[z^{[2]} = W^{[2]}x + b^{[2]}\\] \\[a^{[2]} = z^{[2]}\\] From which we can show that, \\[a^{[2]} = (W^{[2]}W^{[1]})x + (W^{[2]}b^{[1]} + b^{[2]})\\] \\[a^{[2]} = W'x + b' \\text{, where } W' = W^{[2]}W^{[1]} \\text{ and } b' = W^{[2]}b^{[1]} + b^{[2]}\\] Therefore, in the case of a linear activation function , the neural network is outputting a linear function of the inputs , no matter how many hidden layers!","title":"ReLu"},{"location":"neural_networks_and_deep_learning/week_3/#exceptions","text":"There are (maybe) two cases in which you may actually want to use a linear activation function. The output layer of a network used to perform regression, where we want \\(\\hat y\\) to be a real-valued number, \\(\\hat y \\in \\mathbb R\\) Extremely specific cases pertaining to compression.","title":"Exceptions"},{"location":"neural_networks_and_deep_learning/week_3/#derivatives-of-activation-functions","text":"When performing back-propogation on a network, we need to compute the derivatives of the activation functions. Lets take a look at our activation functions and their derivatives Sigmoid The deriviative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = \\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}})= g(z)(1-g(z)) = a(1-a)\\] We can sanity check this by inputting very large, or very small values of \\(z\\) into our derivative formula and inspecting the size of the outputs. Notice that if we have already computed the value of \\(a\\), we can very cheaply compute the value of \\(g(z)'\\) . Tanh The deriviative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = 1 - (tanh(z))^z\\] Again, we can sanity check this inspecting that the outputs for different values of \\(z\\) match our intuition about the activation function. ReLu The derivative of \\(g(z)\\), \\(g(z)'\\) is: \\[\\frac{d}{dz}g(z) = 0 \\text{ if } z < 0 ; 1 \\text{ if } z > 0; \\text{ undefined if } z = 0\\] If \\(z = 0\\), we typically default to setting \\(g(z)\\) to either \\(0\\) or \\(1\\) . In practice this matters little.","title":"Derivatives of activation functions"},{"location":"neural_networks_and_deep_learning/week_3/#gradient-descent-for-neural-networks","text":"Lets implement gradient descent for our simple 2-layer neural network. Recall, our parameters are: \\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\) . We have number of features, \\(n_x = n^{[0]}\\), number of hidden units \\(n^{[1]}\\), and \\(n^{[2]}\\) output units. Thus our dimensions: \\(W^{[1]}\\) : (\\(n^{[1]}, n^{[0]}\\)) \\(b^{[1]}\\) : (\\(n^{[1]}, 1\\)) \\(W^{[2]}\\) : (\\(n^{[2]}, n^{[1]}\\)) \\(b^{[2]}\\) : (\\(n^{[2]}, 1\\)) Our cost function is: \\(J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m}\\sum_{i=1}^m \\ell(\\hat y, y)\\) We are assuming binary classification. Gradient Descent sketch Initialize parameters randomly Repeat: compute predictions \\(\\hat y^{(i)}\\) for \\(i = 1 ,..., m\\) \\(dW^{[1]} = \\frac{\\partial J}{\\partial W^{[1]}}, db^{[1]} = \\frac{\\partial J}{\\partial b^{[1]}}, ...\\) \\(W^{[1]} = W^{[1]} - \\alpha dW^{[1]}, ...\\) \\(b^{[1]} = b^{[1]} - \\alpha db^{[1]}, ...\\) The key to gradient descent is to computation of the derivatives, \\(\\frac{\\partial J}{\\partial W^{[l]}}\\) and \\(\\frac{\\partial J}{\\partial b^{[l]}}\\) for all layers \\(l\\) .","title":"Gradient descent for Neural Networks"},{"location":"neural_networks_and_deep_learning/week_3/#formulas-for-computing-derivatives","text":"We are going to simply present the formulas you need, and defer their explanation to the next video. Recall the computation graph for our 2-layered neural network: | And the vectorized implementation of our computations in our forward propagation 1.\\[Z^{[1]} = W^{[1]}X + b^{[1]}\\] 2.\\[A^{[1]} = g^{[1]}(Z^{[1]})\\] 3.\\[Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}\\] 4.\\[A^{[2]} = g^{[2]}(Z^{[2]}) = \\sigma(Z^{[2]})\\] Where \\(g^{[2]}\\) would likely be the sigmoid function if we are doing binary classification. Now we list the computations for our backward propagation 1.\\[ dZ^{[2]} = A^{[2]} - Y \\] 2.\\[ dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T} \\] Transpose of A accounts for the fact that W is composed of transposed column vectors of parameters. 3.\\[db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims=True)\\] Where \\(Y = [y^{(1)}, ..., y^{[m]}]\\) . The keepdims arguments prevents numpy from returning a rank 1 array, \\((n,)\\) 4.\\[dZ^{[1]} = W^{[2]T}dZ^{[2]} \\odot g(Z)' (Z^{[1]})\\] Where \\(\\odot\\) is the element-wise product. Note: this is a collapse of \\(dZ\\) and \\(dA\\) computations. 5.\\[dW{[1]} = \\frac{1}{m} = dZ^{[1]}X^T\\] 6.\\[db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)\\]","title":"Formulas for computing derivatives"},{"location":"neural_networks_and_deep_learning/week_3/#random-initialization","text":"When you train your neural network, it is important to initialize your parameters randomly . With logistic regression, we were able to initialize our weights to zero because the cost function was convex. We will see that this will not work with neural networks. Lets take the following network as example: Lets say we initialize our parameters as follows: \\(W^{[1]} = \\begin{bmatrix}0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\), \\(b^{[1]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\), \\(W^{[2]} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\), \\(b^{[2]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) It turns out that initializing the bias \\(b\\) with zeros is OK. The problem with this initialization is that for any input examples \\(i, j\\), \\[a^{[1]}_i == a^{[1]}_j\\] Similarly, \\[dz^{[1]}_i == dz^{[1]}_j\\] Thus, \\(dW^{[1]}\\) will be some matrix \\(\\begin{bmatrix}u & v \\\\ u & v\\end{bmatrix}\\) and all updates to the parameters \\(W^{[1]}\\) will be identical. Note we are referring to our single hidden layer \\(^{[1]}\\) but this would apply to any hidden layer of any fully-connected network, no matter how large. Using a proof by induction , it is actually possible to prove that after any number of rounds of training the two hidden units are still computing identical functions . This is often called the symmetry breaking problem . The solution to this problem, is to initialize parameters randomly . Heres an example on how to do that with numpy: \\(W^{[1]}\\) = np.random.rand(2,2) * 0.01 \\(W^{[2]}\\) = np.random.rand(1,2) * 0.01 ... This will generate small, gaussian random values. \\(b^{[1]}\\) = np.zeros((2,1)) \\(b^{[2]}\\) = 0 ... In next weeks material, we will talk about how and when you might choose a different factor than \\(0.01\\) for initialization. It turns out the \\(b\\) does not have this symmetry breaking problem, because as long as the hidden units are computing different functions, the network will converge on different values of \\(b\\), and so it is fine to initialize it to zeros. Why do we initialize to small values? For a sigmoid-like activation function, large parameter weights (positive or negative) will make it more likely that \\(z\\) is very large (positive or negative) and thus \\(dz\\) will approach \\(0\\), slowing down learning dramatically . Note this is a less of an issue when using ReLu's, however many classification problems use sigmoid activations in their output layer.","title":"Random Initialization"},{"location":"neural_networks_and_deep_learning/week_4/","text":"Week 4: Deep Neural Networks What is a deep neural network? A deep neural network is simply a network with more than 1 hidden layer. Compared to logistic regression or a simple neural network with one hidden layer (which are considered shallow models) we say that a neural network with many hidden layers is a deep model, hence the terms deep learning / deep neural networks . Shallow Vs. deep is a matter of degree, the more hidden layers, the deeper the model. Over the years, the machine learning and AI community has realized that deep networks are excellent function approximators, and are able to learn incredibly complex functions to map inputs to outputs. Note that is difficult to know in advance how deep a neural network needs to be to learn an effective mapping. Notation Lets go over the notation we will need using an example network We will use \\(L\\) to denote the number of layers in the network (in this network \\(L=4\\)) \\(n^{[l]}\\) denotes the number of units in layers \\(l\\) (for example, in this network \\(n^{[1]} = 5\\)) \\(a^{[l]}\\) denotes the activations in layer \\(l\\) \\(W^{[l]}\\), \\(b^{[l]}\\) denotes the weights and biases for \\(z^{[l]}\\) \\(x = a^{[0]}\\) and \\(\\hat y = a^{[L]}\\) Forward Propagation in a Deep Network Forward propogation in a deep network just extends what we have already seen for forward propogation in a neural network by some number of layers. More specifically, for each layer \\(l\\) we perform the computations: \\[ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\] \\[ A^{[l]} = g^{[l]}(Z^{[l]}) \\] Note that the above implementation is vectorized across all training examples. Matrices \\(A^{[l]}\\) and \\(Z^{[l]}\\) stacked column vectors pertaining to a single input example for layer \\(l\\). Finally, our predictions (the results of our output layer) are: \\[\\hat Y = g(Z^{[L]}) = A^{[L]}\\] Note that this solution is not completely vectorized, we still need an explicit for loop over our layers \\(l = 0, 1, ..., L\\) Getting your matrix dimensions right When implementing a neural network, it is extremely important that we ensure our matrix dimensions \"line up\". A simple debugging tool for neural networks then, is pen and paper! For a \\(l\\)-layered neural network, our dimensions are as follows: \\(W^{[l]}: (n^{[l]}, n^{[l-1]})\\) \\(b^{[l]}: (n^{[l]}, 1)\\) \\(Z^{[l]}, A^{[l]}: (n^{[l]}, m)\\) \\(A^{[0]} = X: (n^{[0]}, m)\\) Where \\(n^{[l]}\\) is the number of units in layer \\(l\\). See this video for a derivation of these dimensions. When implementing backpropagation, the dimensions are the same, i.e., the dimensions of \\(W\\), \\(b\\), \\(A\\) and \\(Z\\) are the same as \\(dW\\), \\(db\\), ... Why deep representations? Lets train to gain some intuition behind the success of deep representation for certain problem domains. What is a deep network computing? Lets take the example of image recognition. Perhaps you input a picture of a face, then you can think of the first layer of the neural network as an \"edge detector\". The next layer can use the outputs from the previous layer, which can roughly be thought of as detected edges, and \"group\" them in order to detect parts of faces. Each neuron may become tuned to detect different parts of faces. Finally, the output layer uses the output of the previous layer, detected features of a face, and compose them together to recognize a whole face. The main intuition is that earlier layers detect \"simpler\" structures, and pass this information onto the next layer which can use it to detect increasingly complex structures. These general idea applies to other examples than just computer vision tasks (e.g., audio). Moreover, there is an analogy between deep representations in neural networks and how the brain works, however it can be dangerous to push these analogies too far. Circuit theory and deep learning Circuit theory also provides us with a possible explanation as to why deep networks work so well for some tasks. Informally, there are function you can compute with a \"small\" L-layer deep neural network that shallower networks require exponentially more hidden units to compute. Check out this video starting at 5:36 for a deeper explanation of this. Building blocks of deep neural networks Lets take a more holistic approach and talk about all the building blocks of deep neural networks. Here is a deep neural network with a few hidden layers Lets pick one layer, \\(l\\) and look at the computations involved. For this layer \\(l\\), we have parameters \\(W^{[l]}\\) and \\(b^{[l]}\\). Our two major computation steps through this layer are: Forward Propagation Input: \\(a^{[l-1]}\\) Output: \\(a^{[l]}\\) Linear function: \\(z^{[l]} = W^{[l]}a^{[l-1] + b^{[l]}}\\) Activation function: \\(a^{[l]} = g^{[l]}(z^{[l]})\\) Because \\(z^{[l]}, W^{[l]}, b^{[l]}\\) are used in then backpropagation steps, it helps to cache these values during forward propagation. Backwards Propagation Input: \\(da^{[l]}, cache(z^{[l]})\\) Output: \\(da^{[l-1]}, dW^{[l]}, db^{[l]}\\) The key insight, is that for every computation in forward propagation there is a corresponding computation in backwards propagation So one iteration of training with a neural network involves feeding our inputs into the network (\\(a^{[0]})\\), performing forward propagation computing \\(\\hat y\\), and using it to compute the loss and perform backpropagation through the network. This will produce all the derivatives of the parameters w.r.t the loss that we need to update the parameters for gradient descent. Parameters vs hyperparameters The parameters of your model are the adaptive values, \\(W\\) and \\(b\\) which are learned during training via gradient descent. In contrast, hyperparameters are set before training and can be viewed as the \"settings\" of the learning algorithms. They have a direct effect on the eventual value of the parameters. Examples include: number of iterations learning rate number of hidden layers \\(L\\) number of hidden units \\(n^{[1]}, n^{[2]}, ...\\) choice of activation function the learning rate is sometimes called a parameter. We will follow the convention of calling it a hyperparameter. It can be difficult to know the optimal hyperparameters in advance. Often, we start by simply trying out many values to see what works best, this allows us to build our intuition about the best hyperparameters to use. We will defer a deep discussion on how to choose hyperparameters to the next course. What does this all have to do with the brain? At the risk of giving away the punch line, not a whole lot . The most important mathematical components of a neural networks: forward propagation and backwards propagation are rather complex, and it has been difficult to convey the intuition behind these methods. As a result, the phrase, \"it's like the brain\" has become an easy, but dramatically oversimplified explanation. It also helps that this explanation has caught the publics imagination. There is a loose analogy to be drawn from a biological neuron and the neurons in our artificial neural networks. Both take inputs (derived from other neurons) process the information and propagate a signal forward. However, even today neuroscientists don't fully understand what a neuron is doing when it receives and propagates a signal. Indeed, we have no idea on whether the biological brain is performing some algorithmic processes similar to those performed by an ANN. Deep learning is an excellent method for complex function approximation, i.e., learning mappings from inputs \\(x\\) to outputs \\(y\\). However we should be very wary about pushing the, \"its like a brain!\" analogy too far.","title":"Week 4"},{"location":"neural_networks_and_deep_learning/week_4/#week-4-deep-neural-networks","text":"","title":"Week 4: Deep Neural Networks"},{"location":"neural_networks_and_deep_learning/week_4/#what-is-a-deep-neural-network","text":"A deep neural network is simply a network with more than 1 hidden layer. Compared to logistic regression or a simple neural network with one hidden layer (which are considered shallow models) we say that a neural network with many hidden layers is a deep model, hence the terms deep learning / deep neural networks . Shallow Vs. deep is a matter of degree, the more hidden layers, the deeper the model. Over the years, the machine learning and AI community has realized that deep networks are excellent function approximators, and are able to learn incredibly complex functions to map inputs to outputs. Note that is difficult to know in advance how deep a neural network needs to be to learn an effective mapping.","title":"What is a deep neural network?"},{"location":"neural_networks_and_deep_learning/week_4/#notation","text":"Lets go over the notation we will need using an example network We will use \\(L\\) to denote the number of layers in the network (in this network \\(L=4\\)) \\(n^{[l]}\\) denotes the number of units in layers \\(l\\) (for example, in this network \\(n^{[1]} = 5\\)) \\(a^{[l]}\\) denotes the activations in layer \\(l\\) \\(W^{[l]}\\), \\(b^{[l]}\\) denotes the weights and biases for \\(z^{[l]}\\) \\(x = a^{[0]}\\) and \\(\\hat y = a^{[L]}\\)","title":"Notation"},{"location":"neural_networks_and_deep_learning/week_4/#forward-propagation-in-a-deep-network","text":"Forward propogation in a deep network just extends what we have already seen for forward propogation in a neural network by some number of layers. More specifically, for each layer \\(l\\) we perform the computations: \\[ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \\] \\[ A^{[l]} = g^{[l]}(Z^{[l]}) \\] Note that the above implementation is vectorized across all training examples. Matrices \\(A^{[l]}\\) and \\(Z^{[l]}\\) stacked column vectors pertaining to a single input example for layer \\(l\\). Finally, our predictions (the results of our output layer) are: \\[\\hat Y = g(Z^{[L]}) = A^{[L]}\\] Note that this solution is not completely vectorized, we still need an explicit for loop over our layers \\(l = 0, 1, ..., L\\)","title":"Forward Propagation in a Deep Network"},{"location":"neural_networks_and_deep_learning/week_4/#getting-your-matrix-dimensions-right","text":"When implementing a neural network, it is extremely important that we ensure our matrix dimensions \"line up\". A simple debugging tool for neural networks then, is pen and paper! For a \\(l\\)-layered neural network, our dimensions are as follows: \\(W^{[l]}: (n^{[l]}, n^{[l-1]})\\) \\(b^{[l]}: (n^{[l]}, 1)\\) \\(Z^{[l]}, A^{[l]}: (n^{[l]}, m)\\) \\(A^{[0]} = X: (n^{[0]}, m)\\) Where \\(n^{[l]}\\) is the number of units in layer \\(l\\). See this video for a derivation of these dimensions. When implementing backpropagation, the dimensions are the same, i.e., the dimensions of \\(W\\), \\(b\\), \\(A\\) and \\(Z\\) are the same as \\(dW\\), \\(db\\), ...","title":"Getting your matrix dimensions right"},{"location":"neural_networks_and_deep_learning/week_4/#why-deep-representations","text":"Lets train to gain some intuition behind the success of deep representation for certain problem domains.","title":"Why deep representations?"},{"location":"neural_networks_and_deep_learning/week_4/#what-is-a-deep-network-computing","text":"Lets take the example of image recognition. Perhaps you input a picture of a face, then you can think of the first layer of the neural network as an \"edge detector\". The next layer can use the outputs from the previous layer, which can roughly be thought of as detected edges, and \"group\" them in order to detect parts of faces. Each neuron may become tuned to detect different parts of faces. Finally, the output layer uses the output of the previous layer, detected features of a face, and compose them together to recognize a whole face. The main intuition is that earlier layers detect \"simpler\" structures, and pass this information onto the next layer which can use it to detect increasingly complex structures. These general idea applies to other examples than just computer vision tasks (e.g., audio). Moreover, there is an analogy between deep representations in neural networks and how the brain works, however it can be dangerous to push these analogies too far.","title":"What is a deep network computing?"},{"location":"neural_networks_and_deep_learning/week_4/#circuit-theory-and-deep-learning","text":"Circuit theory also provides us with a possible explanation as to why deep networks work so well for some tasks. Informally, there are function you can compute with a \"small\" L-layer deep neural network that shallower networks require exponentially more hidden units to compute. Check out this video starting at 5:36 for a deeper explanation of this.","title":"Circuit theory and deep learning"},{"location":"neural_networks_and_deep_learning/week_4/#building-blocks-of-deep-neural-networks","text":"Lets take a more holistic approach and talk about all the building blocks of deep neural networks. Here is a deep neural network with a few hidden layers Lets pick one layer, \\(l\\) and look at the computations involved. For this layer \\(l\\), we have parameters \\(W^{[l]}\\) and \\(b^{[l]}\\). Our two major computation steps through this layer are: Forward Propagation Input: \\(a^{[l-1]}\\) Output: \\(a^{[l]}\\) Linear function: \\(z^{[l]} = W^{[l]}a^{[l-1] + b^{[l]}}\\) Activation function: \\(a^{[l]} = g^{[l]}(z^{[l]})\\) Because \\(z^{[l]}, W^{[l]}, b^{[l]}\\) are used in then backpropagation steps, it helps to cache these values during forward propagation. Backwards Propagation Input: \\(da^{[l]}, cache(z^{[l]})\\) Output: \\(da^{[l-1]}, dW^{[l]}, db^{[l]}\\) The key insight, is that for every computation in forward propagation there is a corresponding computation in backwards propagation So one iteration of training with a neural network involves feeding our inputs into the network (\\(a^{[0]})\\), performing forward propagation computing \\(\\hat y\\), and using it to compute the loss and perform backpropagation through the network. This will produce all the derivatives of the parameters w.r.t the loss that we need to update the parameters for gradient descent.","title":"Building blocks of deep neural networks"},{"location":"neural_networks_and_deep_learning/week_4/#parameters-vs-hyperparameters","text":"The parameters of your model are the adaptive values, \\(W\\) and \\(b\\) which are learned during training via gradient descent. In contrast, hyperparameters are set before training and can be viewed as the \"settings\" of the learning algorithms. They have a direct effect on the eventual value of the parameters. Examples include: number of iterations learning rate number of hidden layers \\(L\\) number of hidden units \\(n^{[1]}, n^{[2]}, ...\\) choice of activation function the learning rate is sometimes called a parameter. We will follow the convention of calling it a hyperparameter. It can be difficult to know the optimal hyperparameters in advance. Often, we start by simply trying out many values to see what works best, this allows us to build our intuition about the best hyperparameters to use. We will defer a deep discussion on how to choose hyperparameters to the next course.","title":"Parameters vs hyperparameters"},{"location":"neural_networks_and_deep_learning/week_4/#what-does-this-all-have-to-do-with-the-brain","text":"At the risk of giving away the punch line, not a whole lot . The most important mathematical components of a neural networks: forward propagation and backwards propagation are rather complex, and it has been difficult to convey the intuition behind these methods. As a result, the phrase, \"it's like the brain\" has become an easy, but dramatically oversimplified explanation. It also helps that this explanation has caught the publics imagination. There is a loose analogy to be drawn from a biological neuron and the neurons in our artificial neural networks. Both take inputs (derived from other neurons) process the information and propagate a signal forward. However, even today neuroscientists don't fully understand what a neuron is doing when it receives and propagates a signal. Indeed, we have no idea on whether the biological brain is performing some algorithmic processes similar to those performed by an ANN. Deep learning is an excellent method for complex function approximation, i.e., learning mappings from inputs \\(x\\) to outputs \\(y\\). However we should be very wary about pushing the, \"its like a brain!\" analogy too far.","title":"What does this all have to do with the brain?"},{"location":"sequence_models/week_1/","text":"Week 1: Recurrent Neural Networks Recurrent neural networks have been proven to perform extremely well on temporal data. This model has several variants including LSTMs , GRUs and Bidirectional RNNs , which you are going to learn about in this section. Why sequence models? Recurrent neural networks (RNNs) have proven to be incredibly powerful networks for sequence modelling tasks (where the inputs x x , outputs y y or both are sequences) including: speech recognition music generation sentiment classification DNA sequence analysis machine translation video activity recognition named entity recognition Notation As a motivating example, we will \"build\" a model that performs named entity recognition ( NER ). Example input : x: \\text{Harry Potter and Hermione Granger invented a new spell.} x: \\text{Harry Potter and Hermione Granger invented a new spell.} We want our model to output a target vector with the same number elements as our input sequence x x , representing the named entities in x x . We will refer to each element in our input ( x) x) and output ( y y ) sequences with angled brackets, so for example, x^{<1>} x^{<1>} would refer to \"Harry\". Because we have multiple input sequences, we denote the i-th i-th sequence x^{(i)} x^{(i)} (and its corresponding output sequence y^{(i)} y^{(i)} ). The t-th t-th element of the i-th i-th input sequence is therefore x^{(i)<t>} x^{(i)<t>} . Let T_x T_x be the length of the input sequence and T_y T_y the length of the output sequence. Note In our example, T_x T_x == T_y T_y Representing words For NLP applications, we have to decide on some way to represent words. Typically, we start by generating a vocabulary (a dictionary of all the words that appear in our corpus). Note In modern applications, a vocabulary of 30-50K is common and massive vocabularies (>1 million word types) are often used in commercial applications, especially by big tech. A common way to represent each word is to use a one-hot encoding. In this way, we represent each token by a vector of dimension ||V|| ||V|| (our vocabulary size). Example : x^{<1>} = \\begin{pmatrix} 0 \\\\\\ ..\\\\\\ 1 \\\\ ... \\\\\\ 0 \\end{pmatrix} x^{<1>} = \\begin{pmatrix} 0 \\\\\\ ..\\\\\\ 1 \\\\ ... \\\\\\ 0 \\end{pmatrix} x^{<1>} x^{<1>} of our sequence (i.e. the token Harry ) is represented as a vector which contains all zeros except for a single value of one at row j j , where j j is its position in V V . Note \"One-hot\" refers to the fact that each vector contains only a single 1. The goal is to learn a mapping from each x^{<t>} x^{<t>} to some tag (i.e. PERSON). Note To deal with out-of-vocabulary (OOV) tokens, we typically assign a special value <UNK> and a corresponding vector. Recurrent Neural Network Model Why not a standard network? In our previous example, we had 9 input words. You could imagine taking these 9 input words (represented as one-hot encoded vectors) as inputs to a \"standard\" neural network This turns out not to work well. There are two main problems: Inputs and outputs can be different lengths in different examples (its not as if every T_x, T_y T_x, T_y pair is of the same length). A \"standard\" network doesn't share features learned across different positions of text. This is a problem for multiple reasons, but a big one is that this network architecture doesn't capture dependencies between elements in the sequence (e.g., the information that is a word in its context is not captured). Recurrent Neural Networks Unlike a \"standard\" neural network, recurrent neural networks ( RNN ) accept input from the previous timestep in a sequence. For our example x x above, the unrolled RNN diagram might look like the following: Note Timestep 0 is usually initialized with a fake vector of 0's Note that the diagram is sometimes drawn like this: Where the little black box represented a delay of 1 timestep . A RNN learns on a sequence from left to right , sharing the parameters from each timestep . the parameters governing the connection from x^{<t>} x^{<t>} to the hidden layer will be some set of parameters we're going to write as W_{ax} W_{ax} . the activations, the horizontal connections, will be governed by some set of parameters W_{aa} W_{aa} W_{ya} W_{ya} , governs the output predictions Note We take the notation W_{ya} W_{ya} , to mean (for example) that the parameters for variable y y are obtained by multiplying by some quantity a a . Notice this parameter sharing means that when we make the prediction for, y^{<3>} y^{<3>} say, the RNN gets the information not only from x^{<3>} x^{<3>} but also from the all the previous timesteps. Note a potential weakness here. We don't incorporate information from future timesteps in our predictions. This problem is solved by using bidirectional RNNs (BRNNs) which we discuss in a future video. Example : Given the sentences: x^{(1)} x^{(1)} : He said, \"Teddy Roosevelt was a great President\" x^{(2)} x^{(2)} : He said, \"Teddy bears are on sale!\" And the task of named entity recognition ( NER ), it would be really useful to know that the word \" President \" follows the name \" Teddy Roosevelt \" because as the second example suggest, using only previous information in the sequence might not be enough to make a classification decision about an entity. RNN Computation Lets dig deeper into how a RNN works. First, lets start with a cleaned up depiction of our network Forward Propagation Typically, we start off with the input a^{<0>} = \\vec 0 a^{<0>} = \\vec 0 . Then we perform our forward pass Compute our activation for timestep 1: a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a) a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a) Compute our prediction for timestep 1: \\hat y^{<1>} = g(W_{ya}a^{<1>} + b_y) \\hat y^{<1>} = g(W_{ya}a^{<1>} + b_y) More generally: a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) Note Where b b is our bias vector. The activation function used for the units of a RNN is most commonly tanh , although ReLU is sometimes used. For the output units, it depends on our problem. Often sigmoid / softmax are used for binary and multi-class classification problems respectively. Simplified RNN Notation Lets take the general equations for forward propagation we developed above: a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) We define our simplified hidden activation formulation: a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) Where W_a = \\begin{pmatrix} W_{aa} & | & W_{ax} \\end{pmatrix} W_a = \\begin{pmatrix} W_{aa} & | & W_{ax} \\end{pmatrix} [a^{<t-1>}, x^{<t>}] = \\begin{pmatrix} a^{<t-1>} \\\\\\ x^{<t>} \\end{pmatrix} [a^{<t-1>}, x^{<t>}] = \\begin{pmatrix} a^{<t-1>} \\\\\\ x^{<t>} \\end{pmatrix} Note W_a[a^{<t-1>}, x^{<t>}] = W_{aa}a^{<t-1>} + W_{ax}x^{<t>} W_a[a^{<t-1>}, x^{<t>}] = W_{aa}a^{<t-1>} + W_{ax}x^{<t>} The advantages of this notation is that we can compress two parameter matrices into one. And our simplified output activation formulation: \\hat y^{<t>} = g(W_{y}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{y}a^{<t-1>} + b_y) Backpropagation through time Forward propagation We have seen at a high-level how forward propagation works for an RNN. Essentially, we forward propagate the input, multiplying it by our weight matrices and applying our activation for each timestep until we have outputted a prediction for each timestep in the input sequence. More explicitly, we can represent the process of foward propogation as a series of matrix multiplications in diagram form: Backward propogation through time (BPTT) In order to perform backward propagation through time ( BPTT ), we first have to specify a loss function. We will choose cross-entropy loss (we also saw this when discussing logisitc regression): For a single prediction (timestep) \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) = -y^{<t>} \\log \\hat y^{<t>} - (1 - y^{<t>}) \\log(1- \\hat y^{<t>}) \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) = -y^{<t>} \\log \\hat y^{<t>} - (1 - y^{<t>}) \\log(1- \\hat y^{<t>}) For all predictions \\ell = \\sum_{t=1}^{T_y}\\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) \\ell = \\sum_{t=1}^{T_y}\\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) While not covered in detail here, BPTT simply involves applying our loss function to each prediction at each timestep, and then using this information along with the chain rule to compute the gradients we will need to update our parameters and assign blame proportionally . The entire process might look something like the following: Different types of RNNs So far, we have only seen an RNN where the input and output are both sequences of lengths \\gt 1 \\gt 1 . Particularly, our input and output sequences were of the same length ( T_x == T_y T_x == T_y ), For many applications, T_x \\not = T_y T_x \\not = T_y . Take sentiment classification for example, where the input is typically a sequence (of text) and the output a integer scale (a 1-5 star review, for example). Example : x x : \"There is nothing to like in this movie\" We want our network to output a single prediction from 1-5. This is an example of a many-to-one architecture. Another type of RNN architecture is one-to-many . An example of this architecture is music generation , where we might input an integer (indicating a genre) or the 0-vector (no input) and generate musical notes as our output. In this case, we input a single value to the network at timestep 1, and then propagate that input through the network (the remaining timesteps), with the caveat that in this architecture, we often take the ouput from the previous timestep and feed it to the next timestep: The final example is a many-to-many architecture. Unlike our previous example where T_x == T_y T_x == T_y , in machine translation T_x \\not = T_y T_x \\not = T_y , as the number of words in the input sentence (say, in english ) is not necessarily the same as the output sentence (say, in french ). These problems are typicaly solved with sequence to sequence models , that are composed of distinct encoder and decoder RNNs. Summary of RNN types One-to-one : a standard, generic neural network. Strictly speaking, you wouldn't model this problem with an RNN. One-to-many : Where our input is a single value (or in some cases, a null input represented by the 0-vector) which propogates through the network and our output is a sequence. Often, we use the prediction from the previous timestep when computing the hidden activations. An example is music generation or sequence generation more generally. Many-to-one : Where our input is a sequence and our output is a single value. Typically we take the prediction from the last timestep of the RNN. An example is sentiment classification Many-to-many : Where both our input and outputs are sequences. These sequence are not necessarily the same length ( T_x \\not = T_y T_x \\not = T_y ). When T_x == T_y T_x == T_y our architecture looks like a standard RNN: and when T_x \\not = T_y T_x \\not = T_y are architecture is a sequence to sequence model which looks like: Language model and sequence generation Language modeling is one of the most basic and important tasks in natural language processing. It's also one that RNNs handle very well. What is a language modeling? Let's say you are building a speech recognition system and you hear the sentence: \\text{\"The apple and pear/pair salad\"} \\text{\"The apple and pear/pair salad\"} How does a neural network determine whether the speaker said pear or pair (never mind that the correct answer is obvious to us). The answer is that the network encodes a language model . This language model is able to determine the probability of a given sentence (think of this as a measure of \"correctness\" or \"goodness\"). For example, our language model might output: P(\\text{The apple and pair salad}) = 3.2 \\times 10^{-13} P(\\text{The apple and pair salad}) = 3.2 \\times 10^{-13} P(\\text{The apple and pear salad}) = 5.7 \\times 10^{-10} P(\\text{The apple and pear salad}) = 5.7 \\times 10^{-10} This system would then pick the much more likely second option. Language modeling with an RNN We start with a large corpus of english text. The first step is to tokenize the text in order to form a vocabulary \\text{\"Cats average 15 hours of sleep a day\"} \\rightarrow \\text{[\"Cats\", \"average\", \"15\", \"hours\", \"of\", \"sleep\", \"a\", \"day\", \".\"]} \\text{\"Cats average 15 hours of sleep a day\"} \\rightarrow \\text{[\"Cats\", \"average\", \"15\", \"hours\", \"of\", \"sleep\", \"a\", \"day\", \".\"]} These tokens are then one-hot encoded or mapped to indices . Sometimes, a special end-of-sentence token is appended to each sequence ( <EOS> ). Note What if some of the words we encounter are not in our vocabulary? Typically we add a special token, <UNK> to deal with this problem. Finally, we build an RNN to model the likelihood of any given sentence, learned from the training corpus. RNN model At time 0, we compute some activation a^{<1>} a^{<1>} as a function of some inputs x^{<1>} x^{<1>} . In this case, x^{<1>} x^{<1>} will just be set to the zero vector. Similarly, a^{<0>} a^{<0>} , by convention, is also set to the zero vector. a^{<1>} a^{<1>} will make a softmax prediction over the entire vocabulary to determine \\hat y^{<1>} \\hat y^{<1>} (the probability of observing any of the tokens in your vocabulary as the first word in a sentence). At the second timestep, we will actually feed the first token in the sequence as the input ( x^{<2>} = y^{<1>} x^{<2>} = y^{<1>} ). This occurs, so forth and so on, such that the input to each timestep are the tokens for all previous timesteps. Our outputs \\hat y^{<t>} \\hat y^{<t>} are therefore P(x^{<t>}|x^{<t-1>}, x^{<t-2>}, ..., x^{<t-n>}) P(x^{<t>}|x^{<t-1>}, x^{<t-2>}, ..., x^{<t-n>}) where n n is the length of the sequence. Note Just a note here, we are choosing x^{<t>} = y^{<t-1>} x^{<t>} = y^{<t-1>} NOT x^{<t>} = \\hat y^{<t-1>} x^{<t>} = \\hat y^{<t-1>} The full model looks something like: There are two important steps in this process: Estimate \\hat y^{<t>} = P(y^{<t>} | y^{<1>}, y^{<2>}, ..., y^{<t-1>}) \\hat y^{<t>} = P(y^{<t>} | y^{<1>}, y^{<2>}, ..., y^{<t-1>}) Then pass the ground-truth word from the training set to the next time-step. The loss function is simply the cross-entropy lost function that we saw earlier: For single examples: \\ell(\\hat y^{<t>}, y^{<t>}) = - \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>} \\ell(\\hat y^{<t>}, y^{<t>}) = - \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>} For the entire training set: \\ell = \\sum_i \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) \\ell = \\sum_i \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) Once trained, the RNN will be able to predict the probability of any given sentence (we simply multiply the probabilities output by the RNN at each timestep). Sampling novel sequences After you train a sequence model, one of the ways you can get an informal sense of what is learned is to sample novel sequences (also known as an intrinsic evaluation ). Let's take a look at how you could do that. Word-level models Remember that a sequence model models the probability of any given sequence of words. What we would like to to is to sample from this distribution to generate novel sequence of words. Note At this point, Andrew makes a distinction between the architecture used for training a language modeling and the architecture used for sampling from a language model. The distinction is completely lost on me. We start by computing the activation a^{<1>} a^{<1>} as a function of some inputs x^{<1>} x^{<1>} and a^{<0>} a^{<0>} (again, these are set to the zero vector by convention). The softmax function is used to generate a probability distribution over all words in the vocabulary, representing the likelihood of seeing each at the first position of a word sequence. We then randomly sample from this distribution, choosing a single token ( \\hat y^{<1>} \\hat y^{<1>} ), and pass it as input for the next timestep. Note For example, if we sampled \"the\" in the first timestep, we would set \\hat y^{<1>} = the = x^{<2>} \\hat y^{<1>} = the = x^{<2>} . This means that at the second timestep, we are computing a probability distribution P(v | the) P(v | the) over all tokens v v in our vocabulary V V . The entire procedure looks something like: How do we know when the sequence ends ? If we included the token in our training procedure (and this included it in our vocabulary) the sequence ends when and <EOS> token is generated. Otherwise, stop when a pre-determined number of tokens has been reached. What if we generate an <UNK> token ? We can simply re-sample until we generate a non-<UNK> token. Character-level models We could also build a character-level language model . The only major difference is that we train on a sequence of characters as opposed to tokens , and therefore our vocabulary consists of individual characters (which typically include digits, punctuation, etc.) Character -level language models are more computational expensive, and because a sequence of characters is typically much longer than a sequence of words (obviously) it is more difficult to capture the long range dependencies (as they are longer, of course). However, using a character -level language models has the benefit of avoiding the problem of out-of-vocabulary tokens, as we can build a non-zero vector representation of any token using the learned character representations. Note You can also combine word-level and character-level language models! Vanishing gradients with RNNs One of the problems with the basic RNN algorithm is the vanishing gradient problem . The RNN architecture as we have described it so far: Take the following two input examples: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(2)} = \\text{The cats, which already ate ..., were full} x^{(2)} = \\text{The cats, which already ate ..., were full} Note Take the \"...\" to be an sequence of english words of arbitrary length. Cleary, there is a long range-dependency here between the grammatical number of the noun \"cat\" and the grammatical tense of the verb \"was\". Note It is important to note that while this is a contrived example, language very often contains long-range dependencies. It turns out that the basic RNNs that we have described thus far is not good at capturing such long-range dependencies. To explain why, think back to our earlier discussions about the vanishing gradient problems in very deep neural networks. The basic idea is that in a network with many layers, the gradient becomes increasingly smaller as it is backpropagated through a very deep network, effectively \"vanishing\". RNNs face the same problem, leading to errors in the outputs of later timesteps having little effect on the gradients of earlier timesteps. This leads to a failure to capture long-range dependencies. Note Because of this problem, a basic RNN captures mainly local influences. Recall that exploding gradients are a similar yet opposite problem. It turns out that vanishing gradients are a bigger problems for RNNs, but exploding gradients do occur. However, exploding gradients are typically easier to catch as we simply need to look for gradients that become very very large (also, they usually lead to computational overflow, and generate NaNs). The solution to exploding gradient problems is fairly straightforward however, as we can use a technique like gradient clipping to scale our gradients according to some maximum values. Gated Recurrent Unit (GRU) You've seen how a basic RNN works. In this section, you learn about the Gated Recurrent Unit ( GRU ) which is a modification to the RNN hidden layer that makes it much better at capturing long range connections and helps a lot with the vanishing gradient problems. Recall the activation function for an RNN at timestep t t : a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) As a picture: Note Two papers were important for the development of GRUs: Cho et al., 2014 and Chung et al., 2014 . Lets define a new variable, c c for the memory cell . The job of the memory cell is to remember information earlier in a sequence. So at time <t> <t> the memory cell will have some value c^{<t>} c^{<t>} . In GRUs, it turns out that c^{<t>} == a^{<t>} c^{<t>} == a^{<t>} . Note It will be useful to use the distinct variables however, as in LSTM networks c^{<t>} \\not = a^{<t>} c^{<t>} \\not = a^{<t>} At every timestep t t , we are going to consider overwriting the value of the memory cell c^{<t>} c^{<t>} with a new value, computed with an activation function: \\text{Candidate memory: }\\tilde c^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c) \\text{Candidate memory: }\\tilde c^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c) The most important idea in the GRU is that of an update gate , \\Gamma_u \\Gamma_u , which always has a value between 0 and 1: \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) Note Subscript u u stands for update. To build our intuition, think about the example we introduced earlier: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} We noted that here, the fact that the word \"cat\" was singular was a huge hint that the verb \"was\" would also be singular in number. We can imagine c^{<t>} c^{<t>} as memorizing the case of the noun \" cat \" until it reached the verb \" was \". The job of the gate would be to remember this information between \" ... cat ... were ... \" and forget it afterwords. To compute c^{<t>} c^{<t>} : c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t>} There is a very intuitive understanding of this computation. When \\Gamma_u \\Gamma_u is 1, we simply forget the old value of c^{<t>} c^{<t>} by overwriting it with \\tilde c^{<t>} \\tilde c^{<t>} . When \\Gamma_u \\Gamma_u is 0, we do the opposite (completely diregard the new candidate memory \\tilde c^{<t>} \\tilde c^{<t>} in favour of the old memory cell value c^{<t>} c^{<t>} ). Note Remember that \\Gamma_u \\Gamma_u can take on any value between 0 and 1. The larger the value, the more weight that the candidate memory cell value takes over the old memory cell value. For our example sentence above, we might hope that the GRU would set \\Gamma_u = 1 \\Gamma_u = 1 once it reached \" cats \", and then \\Gamma_u = 0 \\Gamma_u = 0 for every other timestep until it reached \" was \", where it might set \\Gamma_u = 1 \\Gamma_u = 1 again. Think of this as the network memorizing the grammatical number of the subject of the sentence in order to determine the number of its verb, a concept known as agreement . As a picture: Note The purple box just represents our calculation of c^{<t>} c^{<t>} GRUs are remarkably good at determining when to update the memory cell in order to memorize or forget information in the sequence. Vanishing gradient problem The way a GRU solves the vanishing gradient problem is straightforward: the memory cell c^{<t>} c^{<t>} is able to retain information over many timesteps. Even if \\Gamma_u \\Gamma_u becomes very very small, c^{<t>} c^{<t>} will essentially retain its value across many many timesteps. Implementation details c^{<t>}, \\tilde c^{<t>} \\text{ and } \\Gamma_u c^{<t>}, \\tilde c^{<t>} \\text{ and } \\Gamma_u are all vectors of the same dimension. This means that in the computation of: c^{<t>} = \\Gamma_u \\ast \\tilde c^{<t>} + (1 - \\Gamma_u) \\ast c^{<t>} c^{<t>} = \\Gamma_u \\ast \\tilde c^{<t>} + (1 - \\Gamma_u) \\ast c^{<t>} \\ast \\ast are element-wise multiplications. Thus, if \\Gamma_u \\Gamma_u is a 100-dimensional vector, it is really a 100-dimensional vector of bits which tells us of the 100-dimensional memory cell c^{<t>} c^{<t>} , which are the bits we want to update . Note Of course, in practice \\Gamma_u \\Gamma_u will take on values that are not exactly 0 or 1, but its helpful to image it as a bit vector to build our intuition. Invoking our earlier example one more time: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} we could imagine representing the grammatical number of the noun \" cat \" as a single bit in the memory cell. Full GRU unit The description of a GRU unit provided above is actually somewhat simplified. Below is the computations for the full GRU unit: \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} We introduce another gate, \\Gamma_r \\Gamma_r . Where we can think of this gate as capturing how relevant c^{<t-1>} c^{<t-1>} is for computing the next candidate c^{<t>} c^{<t>} . Note You can think of r r as standing for relevance. Note that Andrew tried to establish a consistent notation to use for explaining both GRUs and LSTMs. In the academic literature, you might often see: \\tilde c^{<t>} : \\tilde h \\tilde c^{<t>} : \\tilde h \\Gamma_u : u \\Gamma_u : u \\Gamma_r : r \\Gamma_r : r c^{<t>} : h c^{<t>} : h Note (our notation : common academic notation) Long Short Term Memory (LSTM) In the last video, you learned about the GRU , and how that can allow you to learn very long range dependencies in a sequence. The other type of unit that allows you to do this very well is the LSTM or the long short term memory units, and is even more powerful than the GRU. Recall the full set of equations defining a GRU above: \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} a^{<t>} = c^{<t>} a^{<t>} = c^{<t>} The LSTM unit is a more powerful and slightly more general version of the GRU (in truth, the LSTM was defined before the GRU). Its computations are defined as follows: \\tilde c^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u) \\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) \\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + \\Gamma_f * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + \\Gamma_f * c^{<t-1>} a^{<t>} = \\Gamma_o * tanh(c^{<t>}) a^{<t>} = \\Gamma_o * tanh(c^{<t>}) Note Original LSTM paper . Notice that with LSTMs, a^{<t>} \\not = c^{<t>} a^{<t>} \\not = c^{<t>} . One new property of the LSTM is that instead of one update gate, \\Gamma_u \\Gamma_u , we have two update gates, \\Gamma_u \\Gamma_u and \\Gamma_f \\Gamma_f (for update and forget respectively). This gives the memory cell the option of keeping the old memory cell information c^{<t-1>} c^{<t-1>} and just adding to it some new information \\tilde c^{<t>} \\tilde c^{<t>} . We can represent the LSTM unit in diagram form as follows: Note See here for an more detailed explanation of an LSTM unit. One thing you may notice is that if we draw out multiple units in temporal succession, it becomes clear how the LSTM is able to achieve something akin to \"memory\" over a sequence: Modifications to LSTMs There are many modifications to the LSTM described above. One involves including c^{<t-1>} c^{<t-1>} along with a^{<t-1>}, x^{<t>} a^{<t-1>}, x^{<t>} in the gate computations, known as a peephole connection . This allows for the gate values to depend not just on the input and the previous timesteps activation, but also on the previous timesteps value of the memory cell. Bidirectional RNNs (BRNNs) By now, you've seen most of the building blocks of RNNs. There are two more ideas that let you build much more powerful models. One is bidirectional RNNs ( BRNNs ), which lets you at a point in time to take information from both earlier and later in the sequence. And second, is deep RNNs, which you'll see in the next video. To motivate bidirectional RNNs, we will look at an example we saw previously: x^{(1)} x^{(1)} : He said, \"Teddy Roosevelt was a great President\" x^{(2)} x^{(2)} : He said, \"Teddy bears are on sale!\" Recall, that for the task of NER we established that correctly predicting the token Teddy as a person entity without seeing the words that follow it would be difficult. Note Note: this problem is independent of whether these are standard RNN, GRU, or LSTM units. A solution to this problem is to introduce another RNN in the opposite direction, going backwards in time. During forward propogation, we compute activations as we have seen previously, with key difference being that we learn two series of activations: one from left-to-right \\overrightarrow a^{<t>} \\overrightarrow a^{<t>} and one from right-to-left \\overleftarrow a^{<t>} \\overleftarrow a^{<t>} . What this allows us to do is learn the representation of each element in the sequence within its context . Explicitly, this is done by using the output of both the forward and backward units at each time step in order to make a prediction \\hat y^{<t>} \\hat y^{<t>} : \\hat y^{<t>} = g(W_y[\\overrightarrow a^{<t>}, \\overleftarrow a^{<t>}] + b_y) \\hat y^{<t>} = g(W_y[\\overrightarrow a^{<t>}, \\overleftarrow a^{<t>}] + b_y) For the example given above, this means that our prediction for the token Teddy , y^{<3>} y^{<3>} , is able to makes use of information seen previously in the sequence ( t = 3, 2, ... t = 3, 2, ... ) and future information in the sequence ( t = 4, 5, ... t = 4, 5, ... ) Note Note again that we can build bidirectional networks with standard RNN, GRU and LSTM units. Bidirectional LSTMs are extremely common. The disadvantage of BRNNs is that we need to see the entire sequence before we can make any predictions. This can be a problem in applications such as real-time speech recognition. Note the BRNN will let you take into account the entire speech utterance but if you use our straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it and make a speech recognition prediction For applications like these, there exists somewhat more complex modules that allow predictions to be made before the full sequence has been seen. bidirectional RNN as you've seen here. For many NLP applications where you can get the entire sentence all the same time, our standard BRNN algorithm is actually very effective. Deep RNNs The different versions of RNNs you've seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models. Recall, that for a standard neural network we have some input x x which is fed to a hidden layer with activations a^{[l]} a^{[l]} which are in turn fed to the next layer to produce activations a^{[l+1]} a^{[l+1]} . In this was, we can stack as many layers as we like. The same is true of RNNs. Lets use the notation a^{[l]<t>} a^{[l]<t>} to denote the activations of layer l l for timestep t t . A stacked RNN would thus look something like the following: The computation of, for example, a^{[2]<3>} a^{[2]<3>} would be: a^{[2]<3>} = g(W_a^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_a^{[2]}) a^{[2]<3>} = g(W_a^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_a^{[2]}) Notice that the second layer has parameters W_a^{[2]} W_a^{[2]} and b_a^{[2]} b_a^{[2]} which are shared across all timesteps, but not across the layers (which have their own corresponding set of parameters). Unlike standard neural networks, we rarely stack RNNs very deep. Part of the reason is that RNNs are already quite large due to their temporal dimension. Note A common depth would be 2 stacked RNNs. Something that has become more common is to apply deep neural networks to the output of each timestep. In this approach, the same deep neural network is typically applied to each output of the final RNN layer.","title":"Week 1"},{"location":"sequence_models/week_1/#week-1-recurrent-neural-networks","text":"Recurrent neural networks have been proven to perform extremely well on temporal data. This model has several variants including LSTMs , GRUs and Bidirectional RNNs , which you are going to learn about in this section.","title":"Week 1: Recurrent Neural Networks"},{"location":"sequence_models/week_1/#why-sequence-models","text":"Recurrent neural networks (RNNs) have proven to be incredibly powerful networks for sequence modelling tasks (where the inputs x x , outputs y y or both are sequences) including: speech recognition music generation sentiment classification DNA sequence analysis machine translation video activity recognition named entity recognition","title":"Why sequence models?"},{"location":"sequence_models/week_1/#notation","text":"As a motivating example, we will \"build\" a model that performs named entity recognition ( NER ). Example input : x: \\text{Harry Potter and Hermione Granger invented a new spell.} x: \\text{Harry Potter and Hermione Granger invented a new spell.} We want our model to output a target vector with the same number elements as our input sequence x x , representing the named entities in x x . We will refer to each element in our input ( x) x) and output ( y y ) sequences with angled brackets, so for example, x^{<1>} x^{<1>} would refer to \"Harry\". Because we have multiple input sequences, we denote the i-th i-th sequence x^{(i)} x^{(i)} (and its corresponding output sequence y^{(i)} y^{(i)} ). The t-th t-th element of the i-th i-th input sequence is therefore x^{(i)<t>} x^{(i)<t>} . Let T_x T_x be the length of the input sequence and T_y T_y the length of the output sequence. Note In our example, T_x T_x == T_y T_y","title":"Notation"},{"location":"sequence_models/week_1/#representing-words","text":"For NLP applications, we have to decide on some way to represent words. Typically, we start by generating a vocabulary (a dictionary of all the words that appear in our corpus). Note In modern applications, a vocabulary of 30-50K is common and massive vocabularies (>1 million word types) are often used in commercial applications, especially by big tech. A common way to represent each word is to use a one-hot encoding. In this way, we represent each token by a vector of dimension ||V|| ||V|| (our vocabulary size). Example : x^{<1>} = \\begin{pmatrix} 0 \\\\\\ ..\\\\\\ 1 \\\\ ... \\\\\\ 0 \\end{pmatrix} x^{<1>} = \\begin{pmatrix} 0 \\\\\\ ..\\\\\\ 1 \\\\ ... \\\\\\ 0 \\end{pmatrix} x^{<1>} x^{<1>} of our sequence (i.e. the token Harry ) is represented as a vector which contains all zeros except for a single value of one at row j j , where j j is its position in V V . Note \"One-hot\" refers to the fact that each vector contains only a single 1. The goal is to learn a mapping from each x^{<t>} x^{<t>} to some tag (i.e. PERSON). Note To deal with out-of-vocabulary (OOV) tokens, we typically assign a special value <UNK> and a corresponding vector.","title":"Representing words"},{"location":"sequence_models/week_1/#recurrent-neural-network-model","text":"","title":"Recurrent Neural Network Model"},{"location":"sequence_models/week_1/#why-not-a-standard-network","text":"In our previous example, we had 9 input words. You could imagine taking these 9 input words (represented as one-hot encoded vectors) as inputs to a \"standard\" neural network This turns out not to work well. There are two main problems: Inputs and outputs can be different lengths in different examples (its not as if every T_x, T_y T_x, T_y pair is of the same length). A \"standard\" network doesn't share features learned across different positions of text. This is a problem for multiple reasons, but a big one is that this network architecture doesn't capture dependencies between elements in the sequence (e.g., the information that is a word in its context is not captured).","title":"Why not a standard network?"},{"location":"sequence_models/week_1/#recurrent-neural-networks","text":"Unlike a \"standard\" neural network, recurrent neural networks ( RNN ) accept input from the previous timestep in a sequence. For our example x x above, the unrolled RNN diagram might look like the following: Note Timestep 0 is usually initialized with a fake vector of 0's Note that the diagram is sometimes drawn like this: Where the little black box represented a delay of 1 timestep . A RNN learns on a sequence from left to right , sharing the parameters from each timestep . the parameters governing the connection from x^{<t>} x^{<t>} to the hidden layer will be some set of parameters we're going to write as W_{ax} W_{ax} . the activations, the horizontal connections, will be governed by some set of parameters W_{aa} W_{aa} W_{ya} W_{ya} , governs the output predictions Note We take the notation W_{ya} W_{ya} , to mean (for example) that the parameters for variable y y are obtained by multiplying by some quantity a a . Notice this parameter sharing means that when we make the prediction for, y^{<3>} y^{<3>} say, the RNN gets the information not only from x^{<3>} x^{<3>} but also from the all the previous timesteps. Note a potential weakness here. We don't incorporate information from future timesteps in our predictions. This problem is solved by using bidirectional RNNs (BRNNs) which we discuss in a future video. Example : Given the sentences: x^{(1)} x^{(1)} : He said, \"Teddy Roosevelt was a great President\" x^{(2)} x^{(2)} : He said, \"Teddy bears are on sale!\" And the task of named entity recognition ( NER ), it would be really useful to know that the word \" President \" follows the name \" Teddy Roosevelt \" because as the second example suggest, using only previous information in the sequence might not be enough to make a classification decision about an entity.","title":"Recurrent Neural Networks"},{"location":"sequence_models/week_1/#rnn-computation","text":"Lets dig deeper into how a RNN works. First, lets start with a cleaned up depiction of our network","title":"RNN Computation"},{"location":"sequence_models/week_1/#forward-propagation","text":"Typically, we start off with the input a^{<0>} = \\vec 0 a^{<0>} = \\vec 0 . Then we perform our forward pass Compute our activation for timestep 1: a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a) a^{<1>} = g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a) Compute our prediction for timestep 1: \\hat y^{<1>} = g(W_{ya}a^{<1>} + b_y) \\hat y^{<1>} = g(W_{ya}a^{<1>} + b_y) More generally: a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) Note Where b b is our bias vector. The activation function used for the units of a RNN is most commonly tanh , although ReLU is sometimes used. For the output units, it depends on our problem. Often sigmoid / softmax are used for binary and multi-class classification problems respectively.","title":"Forward Propagation"},{"location":"sequence_models/week_1/#simplified-rnn-notation","text":"Lets take the general equations for forward propagation we developed above: a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{ya}a^{<t-1>} + b_y) We define our simplified hidden activation formulation: a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) Where W_a = \\begin{pmatrix} W_{aa} & | & W_{ax} \\end{pmatrix} W_a = \\begin{pmatrix} W_{aa} & | & W_{ax} \\end{pmatrix} [a^{<t-1>}, x^{<t>}] = \\begin{pmatrix} a^{<t-1>} \\\\\\ x^{<t>} \\end{pmatrix} [a^{<t-1>}, x^{<t>}] = \\begin{pmatrix} a^{<t-1>} \\\\\\ x^{<t>} \\end{pmatrix} Note W_a[a^{<t-1>}, x^{<t>}] = W_{aa}a^{<t-1>} + W_{ax}x^{<t>} W_a[a^{<t-1>}, x^{<t>}] = W_{aa}a^{<t-1>} + W_{ax}x^{<t>} The advantages of this notation is that we can compress two parameter matrices into one. And our simplified output activation formulation: \\hat y^{<t>} = g(W_{y}a^{<t-1>} + b_y) \\hat y^{<t>} = g(W_{y}a^{<t-1>} + b_y)","title":"Simplified RNN Notation"},{"location":"sequence_models/week_1/#backpropagation-through-time","text":"","title":"Backpropagation through time"},{"location":"sequence_models/week_1/#forward-propagation_1","text":"We have seen at a high-level how forward propagation works for an RNN. Essentially, we forward propagate the input, multiplying it by our weight matrices and applying our activation for each timestep until we have outputted a prediction for each timestep in the input sequence. More explicitly, we can represent the process of foward propogation as a series of matrix multiplications in diagram form:","title":"Forward propagation"},{"location":"sequence_models/week_1/#backward-propogation-through-time-bptt","text":"In order to perform backward propagation through time ( BPTT ), we first have to specify a loss function. We will choose cross-entropy loss (we also saw this when discussing logisitc regression): For a single prediction (timestep) \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) = -y^{<t>} \\log \\hat y^{<t>} - (1 - y^{<t>}) \\log(1- \\hat y^{<t>}) \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) = -y^{<t>} \\log \\hat y^{<t>} - (1 - y^{<t>}) \\log(1- \\hat y^{<t>}) For all predictions \\ell = \\sum_{t=1}^{T_y}\\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) \\ell = \\sum_{t=1}^{T_y}\\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) While not covered in detail here, BPTT simply involves applying our loss function to each prediction at each timestep, and then using this information along with the chain rule to compute the gradients we will need to update our parameters and assign blame proportionally . The entire process might look something like the following:","title":"Backward propogation through time (BPTT)"},{"location":"sequence_models/week_1/#different-types-of-rnns","text":"So far, we have only seen an RNN where the input and output are both sequences of lengths \\gt 1 \\gt 1 . Particularly, our input and output sequences were of the same length ( T_x == T_y T_x == T_y ), For many applications, T_x \\not = T_y T_x \\not = T_y . Take sentiment classification for example, where the input is typically a sequence (of text) and the output a integer scale (a 1-5 star review, for example). Example : x x : \"There is nothing to like in this movie\" We want our network to output a single prediction from 1-5. This is an example of a many-to-one architecture. Another type of RNN architecture is one-to-many . An example of this architecture is music generation , where we might input an integer (indicating a genre) or the 0-vector (no input) and generate musical notes as our output. In this case, we input a single value to the network at timestep 1, and then propagate that input through the network (the remaining timesteps), with the caveat that in this architecture, we often take the ouput from the previous timestep and feed it to the next timestep: The final example is a many-to-many architecture. Unlike our previous example where T_x == T_y T_x == T_y , in machine translation T_x \\not = T_y T_x \\not = T_y , as the number of words in the input sentence (say, in english ) is not necessarily the same as the output sentence (say, in french ). These problems are typicaly solved with sequence to sequence models , that are composed of distinct encoder and decoder RNNs.","title":"Different types of RNNs"},{"location":"sequence_models/week_1/#summary-of-rnn-types","text":"One-to-one : a standard, generic neural network. Strictly speaking, you wouldn't model this problem with an RNN. One-to-many : Where our input is a single value (or in some cases, a null input represented by the 0-vector) which propogates through the network and our output is a sequence. Often, we use the prediction from the previous timestep when computing the hidden activations. An example is music generation or sequence generation more generally. Many-to-one : Where our input is a sequence and our output is a single value. Typically we take the prediction from the last timestep of the RNN. An example is sentiment classification Many-to-many : Where both our input and outputs are sequences. These sequence are not necessarily the same length ( T_x \\not = T_y T_x \\not = T_y ). When T_x == T_y T_x == T_y our architecture looks like a standard RNN: and when T_x \\not = T_y T_x \\not = T_y are architecture is a sequence to sequence model which looks like:","title":"Summary of RNN types"},{"location":"sequence_models/week_1/#language-model-and-sequence-generation","text":"Language modeling is one of the most basic and important tasks in natural language processing. It's also one that RNNs handle very well.","title":"Language model and sequence generation"},{"location":"sequence_models/week_1/#what-is-a-language-modeling","text":"Let's say you are building a speech recognition system and you hear the sentence: \\text{\"The apple and pear/pair salad\"} \\text{\"The apple and pear/pair salad\"} How does a neural network determine whether the speaker said pear or pair (never mind that the correct answer is obvious to us). The answer is that the network encodes a language model . This language model is able to determine the probability of a given sentence (think of this as a measure of \"correctness\" or \"goodness\"). For example, our language model might output: P(\\text{The apple and pair salad}) = 3.2 \\times 10^{-13} P(\\text{The apple and pair salad}) = 3.2 \\times 10^{-13} P(\\text{The apple and pear salad}) = 5.7 \\times 10^{-10} P(\\text{The apple and pear salad}) = 5.7 \\times 10^{-10} This system would then pick the much more likely second option.","title":"What is a language modeling?"},{"location":"sequence_models/week_1/#language-modeling-with-an-rnn","text":"We start with a large corpus of english text. The first step is to tokenize the text in order to form a vocabulary \\text{\"Cats average 15 hours of sleep a day\"} \\rightarrow \\text{[\"Cats\", \"average\", \"15\", \"hours\", \"of\", \"sleep\", \"a\", \"day\", \".\"]} \\text{\"Cats average 15 hours of sleep a day\"} \\rightarrow \\text{[\"Cats\", \"average\", \"15\", \"hours\", \"of\", \"sleep\", \"a\", \"day\", \".\"]} These tokens are then one-hot encoded or mapped to indices . Sometimes, a special end-of-sentence token is appended to each sequence ( <EOS> ). Note What if some of the words we encounter are not in our vocabulary? Typically we add a special token, <UNK> to deal with this problem. Finally, we build an RNN to model the likelihood of any given sentence, learned from the training corpus.","title":"Language modeling with an RNN"},{"location":"sequence_models/week_1/#rnn-model","text":"At time 0, we compute some activation a^{<1>} a^{<1>} as a function of some inputs x^{<1>} x^{<1>} . In this case, x^{<1>} x^{<1>} will just be set to the zero vector. Similarly, a^{<0>} a^{<0>} , by convention, is also set to the zero vector. a^{<1>} a^{<1>} will make a softmax prediction over the entire vocabulary to determine \\hat y^{<1>} \\hat y^{<1>} (the probability of observing any of the tokens in your vocabulary as the first word in a sentence). At the second timestep, we will actually feed the first token in the sequence as the input ( x^{<2>} = y^{<1>} x^{<2>} = y^{<1>} ). This occurs, so forth and so on, such that the input to each timestep are the tokens for all previous timesteps. Our outputs \\hat y^{<t>} \\hat y^{<t>} are therefore P(x^{<t>}|x^{<t-1>}, x^{<t-2>}, ..., x^{<t-n>}) P(x^{<t>}|x^{<t-1>}, x^{<t-2>}, ..., x^{<t-n>}) where n n is the length of the sequence. Note Just a note here, we are choosing x^{<t>} = y^{<t-1>} x^{<t>} = y^{<t-1>} NOT x^{<t>} = \\hat y^{<t-1>} x^{<t>} = \\hat y^{<t-1>} The full model looks something like: There are two important steps in this process: Estimate \\hat y^{<t>} = P(y^{<t>} | y^{<1>}, y^{<2>}, ..., y^{<t-1>}) \\hat y^{<t>} = P(y^{<t>} | y^{<1>}, y^{<2>}, ..., y^{<t-1>}) Then pass the ground-truth word from the training set to the next time-step. The loss function is simply the cross-entropy lost function that we saw earlier: For single examples: \\ell(\\hat y^{<t>}, y^{<t>}) = - \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>} \\ell(\\hat y^{<t>}, y^{<t>}) = - \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>} For the entire training set: \\ell = \\sum_i \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) \\ell = \\sum_i \\ell^{<t>}(\\hat y^{<t>}, y^{<t>}) Once trained, the RNN will be able to predict the probability of any given sentence (we simply multiply the probabilities output by the RNN at each timestep).","title":"RNN model"},{"location":"sequence_models/week_1/#sampling-novel-sequences","text":"After you train a sequence model, one of the ways you can get an informal sense of what is learned is to sample novel sequences (also known as an intrinsic evaluation ). Let's take a look at how you could do that.","title":"Sampling novel sequences"},{"location":"sequence_models/week_1/#word-level-models","text":"Remember that a sequence model models the probability of any given sequence of words. What we would like to to is to sample from this distribution to generate novel sequence of words. Note At this point, Andrew makes a distinction between the architecture used for training a language modeling and the architecture used for sampling from a language model. The distinction is completely lost on me. We start by computing the activation a^{<1>} a^{<1>} as a function of some inputs x^{<1>} x^{<1>} and a^{<0>} a^{<0>} (again, these are set to the zero vector by convention). The softmax function is used to generate a probability distribution over all words in the vocabulary, representing the likelihood of seeing each at the first position of a word sequence. We then randomly sample from this distribution, choosing a single token ( \\hat y^{<1>} \\hat y^{<1>} ), and pass it as input for the next timestep. Note For example, if we sampled \"the\" in the first timestep, we would set \\hat y^{<1>} = the = x^{<2>} \\hat y^{<1>} = the = x^{<2>} . This means that at the second timestep, we are computing a probability distribution P(v | the) P(v | the) over all tokens v v in our vocabulary V V . The entire procedure looks something like: How do we know when the sequence ends ? If we included the token in our training procedure (and this included it in our vocabulary) the sequence ends when and <EOS> token is generated. Otherwise, stop when a pre-determined number of tokens has been reached. What if we generate an <UNK> token ? We can simply re-sample until we generate a non-<UNK> token.","title":"Word-level models"},{"location":"sequence_models/week_1/#character-level-models","text":"We could also build a character-level language model . The only major difference is that we train on a sequence of characters as opposed to tokens , and therefore our vocabulary consists of individual characters (which typically include digits, punctuation, etc.) Character -level language models are more computational expensive, and because a sequence of characters is typically much longer than a sequence of words (obviously) it is more difficult to capture the long range dependencies (as they are longer, of course). However, using a character -level language models has the benefit of avoiding the problem of out-of-vocabulary tokens, as we can build a non-zero vector representation of any token using the learned character representations. Note You can also combine word-level and character-level language models!","title":"Character-level models"},{"location":"sequence_models/week_1/#vanishing-gradients-with-rnns","text":"One of the problems with the basic RNN algorithm is the vanishing gradient problem . The RNN architecture as we have described it so far: Take the following two input examples: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(2)} = \\text{The cats, which already ate ..., were full} x^{(2)} = \\text{The cats, which already ate ..., were full} Note Take the \"...\" to be an sequence of english words of arbitrary length. Cleary, there is a long range-dependency here between the grammatical number of the noun \"cat\" and the grammatical tense of the verb \"was\". Note It is important to note that while this is a contrived example, language very often contains long-range dependencies. It turns out that the basic RNNs that we have described thus far is not good at capturing such long-range dependencies. To explain why, think back to our earlier discussions about the vanishing gradient problems in very deep neural networks. The basic idea is that in a network with many layers, the gradient becomes increasingly smaller as it is backpropagated through a very deep network, effectively \"vanishing\". RNNs face the same problem, leading to errors in the outputs of later timesteps having little effect on the gradients of earlier timesteps. This leads to a failure to capture long-range dependencies. Note Because of this problem, a basic RNN captures mainly local influences. Recall that exploding gradients are a similar yet opposite problem. It turns out that vanishing gradients are a bigger problems for RNNs, but exploding gradients do occur. However, exploding gradients are typically easier to catch as we simply need to look for gradients that become very very large (also, they usually lead to computational overflow, and generate NaNs). The solution to exploding gradient problems is fairly straightforward however, as we can use a technique like gradient clipping to scale our gradients according to some maximum values.","title":"Vanishing gradients with RNNs"},{"location":"sequence_models/week_1/#gated-recurrent-unit-gru","text":"You've seen how a basic RNN works. In this section, you learn about the Gated Recurrent Unit ( GRU ) which is a modification to the RNN hidden layer that makes it much better at capturing long range connections and helps a lot with the vanishing gradient problems. Recall the activation function for an RNN at timestep t t : a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a) As a picture: Note Two papers were important for the development of GRUs: Cho et al., 2014 and Chung et al., 2014 . Lets define a new variable, c c for the memory cell . The job of the memory cell is to remember information earlier in a sequence. So at time <t> <t> the memory cell will have some value c^{<t>} c^{<t>} . In GRUs, it turns out that c^{<t>} == a^{<t>} c^{<t>} == a^{<t>} . Note It will be useful to use the distinct variables however, as in LSTM networks c^{<t>} \\not = a^{<t>} c^{<t>} \\not = a^{<t>} At every timestep t t , we are going to consider overwriting the value of the memory cell c^{<t>} c^{<t>} with a new value, computed with an activation function: \\text{Candidate memory: }\\tilde c^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c) \\text{Candidate memory: }\\tilde c^{<t>} = tanh(W_c[c^{<t-1>}, x^{<t>}] + b_c) The most important idea in the GRU is that of an update gate , \\Gamma_u \\Gamma_u , which always has a value between 0 and 1: \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) Note Subscript u u stands for update. To build our intuition, think about the example we introduced earlier: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} We noted that here, the fact that the word \"cat\" was singular was a huge hint that the verb \"was\" would also be singular in number. We can imagine c^{<t>} c^{<t>} as memorizing the case of the noun \" cat \" until it reached the verb \" was \". The job of the gate would be to remember this information between \" ... cat ... were ... \" and forget it afterwords. To compute c^{<t>} c^{<t>} : c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t>} There is a very intuitive understanding of this computation. When \\Gamma_u \\Gamma_u is 1, we simply forget the old value of c^{<t>} c^{<t>} by overwriting it with \\tilde c^{<t>} \\tilde c^{<t>} . When \\Gamma_u \\Gamma_u is 0, we do the opposite (completely diregard the new candidate memory \\tilde c^{<t>} \\tilde c^{<t>} in favour of the old memory cell value c^{<t>} c^{<t>} ). Note Remember that \\Gamma_u \\Gamma_u can take on any value between 0 and 1. The larger the value, the more weight that the candidate memory cell value takes over the old memory cell value. For our example sentence above, we might hope that the GRU would set \\Gamma_u = 1 \\Gamma_u = 1 once it reached \" cats \", and then \\Gamma_u = 0 \\Gamma_u = 0 for every other timestep until it reached \" was \", where it might set \\Gamma_u = 1 \\Gamma_u = 1 again. Think of this as the network memorizing the grammatical number of the subject of the sentence in order to determine the number of its verb, a concept known as agreement . As a picture: Note The purple box just represents our calculation of c^{<t>} c^{<t>} GRUs are remarkably good at determining when to update the memory cell in order to memorize or forget information in the sequence.","title":"Gated Recurrent Unit (GRU)"},{"location":"sequence_models/week_1/#vanishing-gradient-problem","text":"The way a GRU solves the vanishing gradient problem is straightforward: the memory cell c^{<t>} c^{<t>} is able to retain information over many timesteps. Even if \\Gamma_u \\Gamma_u becomes very very small, c^{<t>} c^{<t>} will essentially retain its value across many many timesteps.","title":"Vanishing gradient problem"},{"location":"sequence_models/week_1/#implementation-details","text":"c^{<t>}, \\tilde c^{<t>} \\text{ and } \\Gamma_u c^{<t>}, \\tilde c^{<t>} \\text{ and } \\Gamma_u are all vectors of the same dimension. This means that in the computation of: c^{<t>} = \\Gamma_u \\ast \\tilde c^{<t>} + (1 - \\Gamma_u) \\ast c^{<t>} c^{<t>} = \\Gamma_u \\ast \\tilde c^{<t>} + (1 - \\Gamma_u) \\ast c^{<t>} \\ast \\ast are element-wise multiplications. Thus, if \\Gamma_u \\Gamma_u is a 100-dimensional vector, it is really a 100-dimensional vector of bits which tells us of the 100-dimensional memory cell c^{<t>} c^{<t>} , which are the bits we want to update . Note Of course, in practice \\Gamma_u \\Gamma_u will take on values that are not exactly 0 or 1, but its helpful to image it as a bit vector to build our intuition. Invoking our earlier example one more time: x^{(1)} = \\text{The cat, which already ate ..., was full} x^{(1)} = \\text{The cat, which already ate ..., was full} we could imagine representing the grammatical number of the noun \" cat \" as a single bit in the memory cell.","title":"Implementation details"},{"location":"sequence_models/week_1/#full-gru-unit","text":"The description of a GRU unit provided above is actually somewhat simplified. Below is the computations for the full GRU unit: \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} We introduce another gate, \\Gamma_r \\Gamma_r . Where we can think of this gate as capturing how relevant c^{<t-1>} c^{<t-1>} is for computing the next candidate c^{<t>} c^{<t>} . Note You can think of r r as standing for relevance. Note that Andrew tried to establish a consistent notation to use for explaining both GRUs and LSTMs. In the academic literature, you might often see: \\tilde c^{<t>} : \\tilde h \\tilde c^{<t>} : \\tilde h \\Gamma_u : u \\Gamma_u : u \\Gamma_r : r \\Gamma_r : r c^{<t>} : h c^{<t>} : h Note (our notation : common academic notation)","title":"Full GRU unit"},{"location":"sequence_models/week_1/#long-short-term-memory-lstm","text":"In the last video, you learned about the GRU , and how that can allow you to learn very long range dependencies in a sequence. The other type of unit that allows you to do this very well is the LSTM or the long short term memory units, and is even more powerful than the GRU. Recall the full set of equations defining a GRU above: \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\Gamma_r = \\sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[\\Gamma_r \\ast c^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>} a^{<t>} = c^{<t>} a^{<t>} = c^{<t>} The LSTM unit is a more powerful and slightly more general version of the GRU (in truth, the LSTM was defined before the GRU). Its computations are defined as follows: \\tilde c^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c) \\tilde c^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c) \\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u) \\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u) \\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) \\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + \\Gamma_f * c^{<t-1>} c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + \\Gamma_f * c^{<t-1>} a^{<t>} = \\Gamma_o * tanh(c^{<t>}) a^{<t>} = \\Gamma_o * tanh(c^{<t>}) Note Original LSTM paper . Notice that with LSTMs, a^{<t>} \\not = c^{<t>} a^{<t>} \\not = c^{<t>} . One new property of the LSTM is that instead of one update gate, \\Gamma_u \\Gamma_u , we have two update gates, \\Gamma_u \\Gamma_u and \\Gamma_f \\Gamma_f (for update and forget respectively). This gives the memory cell the option of keeping the old memory cell information c^{<t-1>} c^{<t-1>} and just adding to it some new information \\tilde c^{<t>} \\tilde c^{<t>} . We can represent the LSTM unit in diagram form as follows: Note See here for an more detailed explanation of an LSTM unit. One thing you may notice is that if we draw out multiple units in temporal succession, it becomes clear how the LSTM is able to achieve something akin to \"memory\" over a sequence:","title":"Long Short Term Memory (LSTM)"},{"location":"sequence_models/week_1/#modifications-to-lstms","text":"There are many modifications to the LSTM described above. One involves including c^{<t-1>} c^{<t-1>} along with a^{<t-1>}, x^{<t>} a^{<t-1>}, x^{<t>} in the gate computations, known as a peephole connection . This allows for the gate values to depend not just on the input and the previous timesteps activation, but also on the previous timesteps value of the memory cell.","title":"Modifications to LSTMs"},{"location":"sequence_models/week_1/#bidirectional-rnns-brnns","text":"By now, you've seen most of the building blocks of RNNs. There are two more ideas that let you build much more powerful models. One is bidirectional RNNs ( BRNNs ), which lets you at a point in time to take information from both earlier and later in the sequence. And second, is deep RNNs, which you'll see in the next video. To motivate bidirectional RNNs, we will look at an example we saw previously: x^{(1)} x^{(1)} : He said, \"Teddy Roosevelt was a great President\" x^{(2)} x^{(2)} : He said, \"Teddy bears are on sale!\" Recall, that for the task of NER we established that correctly predicting the token Teddy as a person entity without seeing the words that follow it would be difficult. Note Note: this problem is independent of whether these are standard RNN, GRU, or LSTM units. A solution to this problem is to introduce another RNN in the opposite direction, going backwards in time. During forward propogation, we compute activations as we have seen previously, with key difference being that we learn two series of activations: one from left-to-right \\overrightarrow a^{<t>} \\overrightarrow a^{<t>} and one from right-to-left \\overleftarrow a^{<t>} \\overleftarrow a^{<t>} . What this allows us to do is learn the representation of each element in the sequence within its context . Explicitly, this is done by using the output of both the forward and backward units at each time step in order to make a prediction \\hat y^{<t>} \\hat y^{<t>} : \\hat y^{<t>} = g(W_y[\\overrightarrow a^{<t>}, \\overleftarrow a^{<t>}] + b_y) \\hat y^{<t>} = g(W_y[\\overrightarrow a^{<t>}, \\overleftarrow a^{<t>}] + b_y) For the example given above, this means that our prediction for the token Teddy , y^{<3>} y^{<3>} , is able to makes use of information seen previously in the sequence ( t = 3, 2, ... t = 3, 2, ... ) and future information in the sequence ( t = 4, 5, ... t = 4, 5, ... ) Note Note again that we can build bidirectional networks with standard RNN, GRU and LSTM units. Bidirectional LSTMs are extremely common. The disadvantage of BRNNs is that we need to see the entire sequence before we can make any predictions. This can be a problem in applications such as real-time speech recognition. Note the BRNN will let you take into account the entire speech utterance but if you use our straightforward implementation, you need to wait for the person to stop talking to get the entire utterance before you can actually process it and make a speech recognition prediction For applications like these, there exists somewhat more complex modules that allow predictions to be made before the full sequence has been seen. bidirectional RNN as you've seen here. For many NLP applications where you can get the entire sentence all the same time, our standard BRNN algorithm is actually very effective.","title":"Bidirectional RNNs (BRNNs)"},{"location":"sequence_models/week_1/#deep-rnns","text":"The different versions of RNNs you've seen so far will already work quite well by themselves. But for learning very complex functions sometimes is useful to stack multiple layers of RNNs together to build even deeper versions of these models. Recall, that for a standard neural network we have some input x x which is fed to a hidden layer with activations a^{[l]} a^{[l]} which are in turn fed to the next layer to produce activations a^{[l+1]} a^{[l+1]} . In this was, we can stack as many layers as we like. The same is true of RNNs. Lets use the notation a^{[l]<t>} a^{[l]<t>} to denote the activations of layer l l for timestep t t . A stacked RNN would thus look something like the following: The computation of, for example, a^{[2]<3>} a^{[2]<3>} would be: a^{[2]<3>} = g(W_a^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_a^{[2]}) a^{[2]<3>} = g(W_a^{[2]}[a^{[2]<2>}, a^{[1]<3>}] + b_a^{[2]}) Notice that the second layer has parameters W_a^{[2]} W_a^{[2]} and b_a^{[2]} b_a^{[2]} which are shared across all timesteps, but not across the layers (which have their own corresponding set of parameters). Unlike standard neural networks, we rarely stack RNNs very deep. Part of the reason is that RNNs are already quite large due to their temporal dimension. Note A common depth would be 2 stacked RNNs. Something that has become more common is to apply deep neural networks to the output of each timestep. In this approach, the same deep neural network is typically applied to each output of the final RNN layer.","title":"Deep RNNs"},{"location":"sequence_models/week_2/","text":"Week 2: Natural Language Processing & Word Embeddings Natural language processing and deep learning is an important combination . Using word vector representations and embedding layers, you can train recurrent neural networks with outstanding performances in a wide variety of industries. Examples of applications are sentiment analysis , named entity recognition ( NER ) and machine translation . Introduction to word embeddings: Word Representation Last week, we learned about RNNs, GRUs, and LSTMs. In this week, you see how many of these ideas can be applied to Natural Language Processing ( NLP ), which is one of the areas of AI being revolutionized by deep learning. One of the key ideas you learn about is word embeddings , which is a way of representing words. So far, we have been representing words with a vocabulary, \\(V\\), of one-hot-encoded vectors. Lets quickly introduce a new notation. If the token \" Man \" is in position 5391 in our vocabulary \\(V\\) then we denote the corresponding one-hot-encoded vector as \\(O_{5391}\\). One of the weaknesses of this representation is that it treats each word as a \"thing\" onto itself, and doesn't allow a language model to generalize between words. Take the following examples: \\[x_1: \\text{\"I want a glass of orange juice\"}\\] \\[x_2: \\text{\"I want a glass of apple juice\"}\\] Cleary, the example sentences are extremely semantically similar. However, in a one-hot encoding scheme, a model which has learned that \\(x_1\\) is a likely sentence is unable to fully generalize to example \\(x_2\\), as the relationship between \" apple \" and \" orange \" is not any closer than the relationship between \" orange \" and any other word in the vocabulary. Notice, in fact, that the inner product between any two one-hot encoded vectors: \\[O_i \\times O_j = \\vec 0 \\text{ for } \\forall i,j\\] And similarly, the euclidean distance between any two one-hot encoded vectors is identical: \\[||O_i - O_j|| = \\sqrt{|V|}\\text{ for } \\forall i,j\\] To build our intuition of word embeddings, image a contrived example where we represent each word with some feature representation : We could imagine many features (with values -1 to 1, say) that can be used to build up a featue representation, an \\(f_n\\)-dimensional vector, of each word. Similarly to our one-hot representations, lets introduce a new notation \\(e_i\\) to represent the embedding of token \\(i\\) in our vocabulary \\(V\\). Where \\(f_n\\) is the number of features. Thinking back to our previous example, notice that our representations for the tokens \" apple \" and \" orange \" become quite similar. This is the critical point, and what allows our language model to generalize between word tokens and even entire sentences. In the later videos, we will see how to learn these embeddings. Note that the learned representations do not have an easy interpretation like the dummy embeddings we presented above. Visualizing word embeddings Once these feature vectors or embeddings are learned, a popular thing to do is to use dimensionality reduction to embed them into a 2D geometric space for easy visualization. An example of this using our word representations presented above: We notice that semantically similar words tend to cluster together, and that each cluster seems to roughly represent some idea or concept (i.e., numbers typically cluster together). This demonstrates our ability to learn similar feature vectors for similar tokens and will allow our models to generalize between words and even sentences. A common algorithm for doing this is the t-SNE algorithm. The reason this feature representations are called embeddings is because we imagine that we are embedding each word into a geometric space (say, of 300 dimensions). If you imagine a cube, we can think of giving each word a single unit of space within this cube. Introduction to word embeddings: Using word embeddings In the last lecture, you saw what it might mean to learn a featurized representations of different words. In this lecture, you see how we can take these representations and plug them into NLP applications. Named entity recognition example Take again the example of named entity recognition, and image we have the following example: Let's assume we correctly identify \" Sally Johnson \" as a PERSON entity. Now imagine we see the following sequence: \\[x: \\text{\"Robert Lin is a durian cultivator\"}\\] Note that durian is a type of fruit. In all likelihood, a model using word embeddings as input should be able to generalize between the two input examples, a take advantage of the fact that it previously labeled the first two tokens of a similar training example (\" Sally Johnson \") as a PERSON entity. But how does the model generalize between \" orange farmer \" and \" durian cultivator \"? Because word embeddings are typically trained on massive unlabeled text corpora, on the scale of 1 - 100 billion words. Thus, it is likely that the word embeddings would have seen and learned the similarity between word pairs (\" orange \", \" durian \") and (\" farmer \", \" cultivator \"). In truth, this method of transfer learning is typically how we use word embeddings in NLP tasks. Transfer learning and word embeddings How exactly do we utilize transfer learning of word embeddings for NLP tasks? Learn word embeddings from large text corpus (1-100B words), OR, download pre-trained embeddings online. Transfer the embedding to a new task with a (much) smaller training set (say, 100K words). (Optional): Continue to fine-tune the word embeddings with new data. In practice, this is only advisable if your training dataset is quite large. This method of transfer learning with word embeddings has found use in NER, text summarization, co-reference resolution, and parsing. However, it has been less useful for language modeling and machine translation (especially when a lot of data for these tasks is available). One major advantage to using word embeddings to represent tokens is that it reduces the dimensionality of our inputs, compared to the one-hot encoding scheme. For example, a typical vocabulary may be 10,000 or more word types, while a typical word embedding may be around 300 dimensions. Introduction to word embeddings: Properties of word embeddings By now, you should have a sense of how word embeddings can help you build NLP applications. One of the most fascinating properties of word embeddings is that they can also help with analogy reasoning . And while reasoning by analogy may not be, by itself, the most important NLP application, it helps to convey a sense of what information these word embeddings are capturing. Analogies Let us return to our previous example: Say we post the question: \" Man is to women as king is to what ? \" Many of us would agree that the answer to this question is \"Queen\" (in part because of humans remarkable ability to reason by analogy ). But can we have a computer arrive at the same answer using embeddings? First, lets simplify our earlier notation and allow \\(e_{man}\\) to denote the learned embedding for the token \" man \". Now, if we take the difference \\(e_{man} - e_{woman}\\), the resulting vector is closest to \\(e_{king} - e_{queen}\\). Note you can confirm this using our made up embeddings in the table. Explicitly, an algorithm to answer the question \" Man is to women as king is to what ? \" would involve computing \\(e_{man} - e_{woman}\\), and then finding the token \\(w\\) that produces \\(e_{man} - e_{woman} \\approx e_{king} - e_{w}\\). This ability to mimic analogical reasoning and other interesting properties of word embeddings were introduced in this paper . Lets try to visualize why this makes sense. Imagine our word embedding plotted as vectors in a 300D space (represented here in 2 dimensions for visualization). We would expect our vectors to line up in a parallelogram: Note that in reality, if you use a dimensionality reduction algorithm such as t-SNE, you will find that this expected relationship between words in an analogy does not hold: We want to find \\(e_w \\approx e_{king} - e_{man} + e_{woman}\\). Our algorithm is thus: \\[argmax_w \\; sim(e_w, e_{king} - e_{man} + e_{woman})\\] The most commonly used similarity function, \\(sim\\) is the cosine similarity : \\[sim(u, v) = \\frac{u^Tv}{||u||_2||v||_2}\\] Which represents the cosine of the angle between the two vectors \\(u, v\\). Note that we can also use euclidian distance , although this is technically a measure of dissimilarity, so we need to take its negative. See here for an intuitive explanation of the cosine similarity measure. Introduction to word embeddings: Embedding matrix Let's start to formalize the problem of learning a good word embedding. When we implement an algorithm to learn word embeddings, what we actually end up learning is an embedding matrix . Say we are using a vocabulary \\(V\\) where \\(||V|| = 10,000\\). We want to learn an embedding matrix \\(E\\) of shape \\((300, 10000)\\) (i.e., the dimension of our word embeddings by the number of words in our vocabulary). \\[E = \\begin{bmatrix}e_{1, 1} & ... & e_{10000, 1}\\\\ ... & ... \\\\ e_{1, 300} & & ...\\end{bmatrix}\\] Where \\(e_{i, j}\\) is the \\(j-th\\) feature in the \\(i-th\\) token. Recall that we used the notation \\(o_i\\) to represent the one-hot encoded representation of the \\(i-th\\) word in our vocabulary. \\[o_i = \\begin{bmatrix}0 \\\\ ... \\\\ 1 \\\\ ... \\\\ 0\\end{bmatrix}\\] If we take \\(E \\cdot o_i\\) then we are retrieving the embedding for the \\(i-th\\) word in \\(V\\), \\(e_i \\in \\mathbb R^{300 \\times 1}\\). Summary The import thing to remember is that our goal will be to learn an embedding matrix \\(E\\). To do this, we initialize \\(E\\) randomly and learn all the parameters of this, say, 300 by 10,000 dimensional matrix. Finally, \\(E\\) multiplied by our one-hot vector \\(o_i\\) gives you the embedding vector for token \\(i\\), \\(e_i\\). Note that while this method of retrieving embeddings from the embedding matrix is intuitive, the matrix-vector multiplication is not efficient. In practice, we use a specialized function to lookup a column \\(i\\) of the matrix \\(E\\), an embedding \\(e_i\\). Learning word embeddings: Learning word embeddings Lets begin to explore some concrete algorithms for learning word embeddings. In the history of deep learning as applied to learning word embeddings, people actually started off with relatively complex algorithms. And then over time, researchers discovered they can use simpler and simpler algorithms and still get very good results, especially for a large dataset. Some of the algorithms that are most popular today are so simple that they almost seem little bit magical. For this reason, it's actually easier to develop our intuition by introducing some of the more complex algorithms first. Note that a lot of the ideas from this lecture came from Bengio et. al., 2003 . Algorithm 1 We will introduce an early algorithm for learning word embeddings, which was very successful, with an example. Lets say you are building a neural language model . We want to be able to predict the next word for any given sequence of words. For example: Note that, another common strategy is to pick a fixed window of words before the word we need to predict. The window size becomes a hyperparameter of the algorithm. \\[x: \\text{\"I want a glass of orange ____\"}\\] One way to approach this problem is to lookup the embeddings for each word in the given sequence, and feed this to a densely connected layer which itself feeds to a single output unit with softmax . Imagine our embeddings are 300 dimensions. Then our input layer is \\(\\mathbb R^{6 \\times 300}\\). Our dense layer and output softmax layer have their own parameters, \\(W^{[1]}, b^{[1]}\\) and \\(W^{[2]}, b^{[2]}\\). We can then use back-propagation to learn these parameters along with the embedding matrix. The reason this works is because the algorithm is incentivized to learn good word embeddings in order to generalize and perform well when predicting the next word in a sequence. Generalizing Imagine we wanted to learn the word \" juice \" in the following sentence: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] Typically, we would provide a neural language model with some context and have it predict this missing word from that context. There are many choices here: \\(n\\) words on the left & right of the word to predict last \\(n\\) word before the word to predict a single, nearby word What researchers have noticed is that if your goal is to build a robust language model, choosing some \\(n\\) number of words before the target word as the context works best. However, if you goal is simply to learn word embeddings, then choosing other, simpler contexts (like a single, nearby word) work quite well. To summarize, by posing the language modeling problem in which some context (such as the last four words) is used to predict some target word, we can effectively learn the input word embeddings via backprogopogation. Learning Word Embeddings: Word2vec In the last lecture, we used a neural language model in order to learn good word embeddings. Let's take a look at the the Word2Vec algorithm, which is a simpler and more computational efficient way to learn word embeddings. Most of the ideas in this lecture come from this paper: Mikolov et al., 2013 . We are going to discuss the word2Vec skip-gram model for learning word embeddings. Skip-gram Let say we have the following example: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] In the skip-gram model, we choose ( context , target ) pairs in order to create the data needed for our supervised setting. To do this, for each context word we randomly choose a target word within some window (say, +/- 5 words). Our learning problem: \\[x \\rightarrow y\\] \\[\\text{Context }, c\\;(\\text{\"orange\"}) \\rightarrow \\text{Target}, t\\; (\\text{\"juice\"})\\] The learning problem then, is to choose the correct target word within a window around the context word. Clearly, this is a very challenging task. However, remember that the goal is not to perform well on this prediction task, but to use the task along with backprogpogation to force the model to learn good word embeddings. Model details Lets take \\(||V|| = 10000\\). Our neural network involves an embedding layer, \\(E\\) followed by a softmax layer, similar to the one we saw in the previous lecture: \\[E \\cdot o_c \\rightarrow e_c \\rightarrow softmax \\rightarrow \\hat y\\] Our softmax layer computes: \\[p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] where \\(\\theta_t\\) is the parameters associated with output \\(t\\) and the bias term has been omitted. Which is a \\(|V|\\) dimensional vector containing the probability distribution of the target word being any word in the vocabulary for a given context word. Our loss is the familiar negative log-likelihood: \\[\\ell (\\hat y, y) = -\\sum^{10000}_{i=1} y_i \\log \\hat y_i\\] To summarize , our model looks up an embeddings in the embedding matrix which contains our word embeddings and is updated by backpropagation during learning. These embeddings are used by a softmax layer to predict a target word for a given context. Problems with softmax classification It turns out, there are a couple problems with the algorithm as we have described it, primarily due to the expensive computation of the softmax layer. Recall our softmax calculation: \\[p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] Every time we wish to perform this softmax classification (that is, every step during training or testing), we need to perform a sum over \\(|V|\\) elements. This quickly becomes a problem when our vocabulary reaches sizes in the milllions or even billions. One solution is to use a hierarchial softmax classifier. The basic idea is to build a Huffman tree based on word frequencies. In this scheme, the number of computations to perform in the softmax layers scales as \\(\\log |V|\\) instead of \\(V\\). I don't really understand this. How to sample the context c? Sampling our target words, \\(t\\) is straightforward once we have sampled their context, \\(c\\), but how do we choose \\(c\\) itself? Once solution is to sample uniform randomly. However, this leads to us choosing extremely common words (such as the , a , of , and , also known as stop words) much too often. This is a problem, as many updates would be made to \\(e_c\\) for these common words and many less updates would be made for less common words. In practice, we use different heuristics to balance the sampling between very common and less common words. Summary In the original word2vec paper, you will find two versions of the word2vec model: the skip-gram one introduced here and another called CBow , the continuous bag-of-words model. This model takes the surrounding contexts from a middle word, and uses them to try to predict the middle word. Each model has its advantages and disadvantages. The key problem with the skip-gram model as presented so far is that the softmax step is very expensive to calculate because it sums over the entire vocabulary size. Learning Word Embeddings: Negative Sampling In the last lecture, we saw how the skip-gram model allows you to construct a supervised learning task by mapping from contexts to targets, and how this in turn allows us to learn a useful word embeddings. The major the downside of this approach was that was the softmax objective was very slow to compute . Lets take a look at a modified learning problem called negative sampling , which allows us to do something similar to the skip-gram model but with a much more efficient learning algorithm. Again, most of the ideas in this lecture come from this paper: Mikolov et al., 2013 . Similar to the skip-gram model, we are going to create a supervised learning setting from unlabeled data. Explicitly, the problem is to predict whether or not a given pair of words is a context , target pair. First, we need to generate training examples: Positive examples are generated exactly how we saw with the skip-gram model, i.e., by sampling a context word and choosing a target word within some window around the context. To generate the negative examples , we take a sampled context word and then for some \\(k\\) number of times, we choose a target word randomly from our vocabulary (under the assumption that this random word won't be associated with our sampled context word). As an example, take the following sentence: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] Then we might construct the following (context, target) training examples: \\((orange, juice, 1)\\) \\((orange, king, 0)\\) \\((orange, book, 0)\\) \\((orange, the, 0)\\) \\((orange, of, 0)\\) Where 1 denotes a positive example and 0 a negative example. Note that this leads to an obvious problem: some of our randomly chosen target words in our generated negative examples will in fact be within the context of the sampled context word. It turns out this is OK, as much more often than not our generated negative examples are truly negative examples. Next, we define a supervised learning problem, where our inputs \\(x\\) are these generated positive and negative examples, and our targets \\(y\\) are whether or not the input represents a true ( context , target ) pair (1) or not (0): \\[x \\rightarrow y\\] \\[(context, target) \\rightarrow 1 \\text{ or } 0\\] Explicitly, we are asking the model to predict whether or not the two words came from a distribution generated by sampling from within a context (defined3 as some window around the words) or a distribution generated by sampling words from the vocabulary at random. How do you choose \\(k\\)? Mikolov et. al suggest \\(k=5-20\\) for small datasets, and \\(k=2-5\\) for large datasets. You can think of \\(k\\) as a 1:\\(k\\) ratio of positive to negative examples. Model details Recall the softmax classifier from the skip-gram model: \\[\\text{Softmax: } p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] For our model which uses negative sampling, first define each input, output pair as \\(c, t\\) and \\(y\\) respectively. Then, we define a logistic regression classifier: \\[p(y = 1 | c, t) = \\sigma(\\theta_t^Te_c)\\] Where \\(\\theta_t\\) represents the parameter vector for a possible target word \\(t\\), and \\(e_c\\) the embedding for each possible context word. NOTE: totally lost around the 7 min mark. Review this. This technique is called negative sampling because we generate our training data for the supervised learning setting by first creating a positive example and then sampling \\(k\\) negative examples. Selecting negative examples The final import point is how we actually sample negative examples in practice . One option is to sample the target word based on the empirical frequency of words in your training corpus. The problem of this solution is that we end up sampling many highly frequent stop words, such as \"and\", \"of\", \"or\", \"but\", etc. Another extreme is to sample the negative examples uniformly random. However, this also leads to a very non-representive sampling of target words. What the authors found to work best is something in between: \\[P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum^{|V|}_{j=1}f(w_j)^{\\frac{3}{4}}}\\] Here, we sample proportional to the frequency of a word to the power of \\(\\frac{3}{4}\\). This is somewhere between the two extremes of sampling words by their frequency and sampling words at uniform random. Summary To summarize, we've seen how to learn word vectors with a softmax classier , but it's very computationally expensive. we've seen that by changing from a softmax classification to a bunch of binary classification problems, we can very efficiently learn words vectors. as is the case in other areas of deep learning, there are open source implementations of the discussed algorithms you can use to learn these embeddings. There are also pre-trained word vectors that others have trained and released online under permissive licenses. Learning Word Embeddings: GloVe word vectors The final algorithm we will look at for learning word embeddings is global vectors for word representation ( GloVe ). While not used as much as word2vec models, it has its enthusiasts -- in part because of its simplicity. This algorithm was original presented here . Previously, we were sampling pairs of words ( context , target ) by picking two words that appear in close proximity to one another in our text corpus. In the GloVe algorithm, we define: \\(X_{ij}\\): the number of times word \\(i\\) appears in the context of word \\(j\\). \\(X_{ij}\\) == \\(X_{ji}\\) Note that \\(X_{ij}\\) == \\(X_{ji}\\) is not necessarily true in other algorithms (e.g., if we were to define the context as being the immediate next word). Notice that \\(i\\) and \\(j\\) play the role of \\(c\\) and \\(t\\). Model The models objective is as follows: Minimize \\sum^{|V|}_{i=1} \\sum^{|V|}_{j=1} f(X_{ij}) (\\theta_i^Te_j + b_i + b_j' - \\log X_{ij})^2 \\sum^{|V|}_{i=1} \\sum^{|V|}_{j=1} f(X_{ij}) (\\theta_i^Te_j + b_i + b_j' - \\log X_{ij})^2 Think of \\(\\theta_i^Te_j\\) as a measure of how similar two words are, based on how often the occur together: \\(\\log X_{ij}\\). More specifically, we are trying to minimize this difference using gradient descent by searching for the pair of words \\(i, j\\) whose inner product \\(\\theta_i^Te_j\\) is a good predictor of how often they are going to appear together, \\(\\log X_{ij}\\). If \\(X_{ij} = 0\\), \\(\\log X_{ij} = \\log 0 = - \\infty\\) which is undefined. We use \\(f(X_{ij})\\) as weighting term, which is 0 when \\(X_{ij}\\) = 0, so we don't sum over pairs of words \\(i, j\\) when \\(X_{ij} = 0\\). \\(f(X_{ij})\\) is also used to weight words, such that extremely common words don't \"drown\" out uncommon words. There are various heuristics for choosing \\(f(X_{ij})\\). You can look at the original paper for details for how to choose this heuristic. Note, we use the convention \\(0 \\log 0 = 0\\) Something to note about this algorithm is that the roles of \\(theta\\) and \\(e\\) are now completely symmetric . So, \\(\\theta_i\\) and \\(e_j\\) are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective. In fact, one way to train the algorithm is to initialize \\(\\theta\\) and \\(e\\) both uniformly and use gradient descent to minimize its objective, and then when you're done for every word, to then take the average: \\[e_w^{final} = \\frac{e_w + \\theta_w}{2}\\] because \\(theta\\) and \\(e\\) in this particular formulation play symmetric roles unlike the earlier models we saw in the previous videos, where \\(theta\\) and \\(e\\) actually play different roles and couldn't just be averaged like that. A note of the featurization view of word embeddings Recall that when we first introduced the idea of word embeddings, we used a sort of featurization view to motivate the reason why we learn word embeddings in the first place. We said, \"Well, maybe the first dimension of the vector captures gender, the second, age...\", so forth and so on. However in practice, we cannot guarantee that the individual components of the embeddings are interpretable. Why? Lets say that there is some \"space\" where the first axis of the embedding vector is gender, and the second age. There is no way to guarantee that the actual dimension for each \"feature\" that the algorithm arrives at will be easily interpretable by humans. Indeed, if we consider the learned representation of each context, target pair, we note that: \\[\\theta_i^Te_j = (A\\theta_i)^T(A^{-T}e_j) = \\theta_i^TA^TA^{-T}e_j\\] Where \\(A\\) is some arbitrary invertible matrix. The key take away is that the dimensions learned by the algorithm are not human interpretable, and each dimension typically encodes some part of what we might think of a feature, as opposed to encoding an entire feature itself. Applications using Word Embeddings: Sentiment Classification Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they're talking about. It is one of the most important building blocks in NLP and is used in many applications. One of the challenges of sentiment classification is a lack of labeled data. However, with word embeddings, you're able to build good sentiment classifiers even with only modest-size label training sets. Lets look at an example: \\[x: \\text{\"The dessert is excellent.\", } y: 4/5 \\text{ stars}\\] \\[x: \\text{\"Service was quite slow.\", } y: 2/5 \\text{ stars}\\] \\[x: \\text{\"Good for a quick meal, but nothing special.\", } y: 3/5 \\text{ stars}\\] \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 \\text{ stars}\\] While we are using restaurant reviews as an example, sentiment analysis is often applied to voice of the customer materials such as reviews and social media. Common training set sizes for sentiment classification would be around 10,000 to 100,000 words. Given these small training set sizes, word embeddings can be extremely useful. Lets use the above examples to introduce a couple of different algorithms Simple sentiment classification model Take, \\[x: \\text{\"The dessert is excellent.\", } y: 4/5 \\text{ stars}\\] As usual, we map the tokens in the input examples to one-hot vectors, multiply this by a pre-trained embedding matrix and obtain our embeddings, \\(e_w\\). Using a pre-trained matrix is essentially a form of transfer learning, as we are able to encode information learned on a much larger corpus (say, 100B tokens) and use it for learning on a much smaller corpus (say, 10,000 tokens). We could then average or sum these embeddings, and pass the result to a softmax classifier which outputs \\(\\hat y\\), the probability of the review being rated as 1, 2, 3, 4 or 5 stars. This algorithm will work OK, but fails to capture negation of positive words (as it does not take into account word order). For example: \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 stars\\] might incorrectly be predicted to correspond with a high star rating, because of the appearance of \"good\" three times. RNN sentiment classification model A more sophisticated model involves using the embeddings as inputs to an RNN, which uses a softmax layer at the last timestep to predict a star rating: Recall that we actually saw this example when discussing many-to-one RNN architectures. Unlike the previous, simpler model, this model takes into account word order and performs much better on examples such as: \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 \\text{ stars}\\] which contain many negated, positive words. When paired with pre-trained word embeddings, this model works quite while. Summary Pre-trained word embeddings are especially useful for NLP tasks where we don't have a lot of training data. In this lecture, we motivated that idea by showing how pre-trained word embeddings can be used as inputs to very simple models to perform sentiment classification. Applications using Word Embeddings: Debiasing word embeddings Machine learning and AI algorithms are increasingly trusted to help with, or to make, extremely important decisions. As such, we would like to make sure that, as much as possible, they're free of undesirable forms of bias, such as gender bias, ethnicity bias and so on. Lets take a look at reducing bias in word embeddings. Most of the ideas in this lecture came from this paper . When we first introduced the idea of word embeddings, we leaned heavily on the idea of analogical reasoning to build our intuition. For example, we were able to ask \"Man is to woman as king is to ____?\" and using word emebddings, arrive at the example queen. However, we can also ask other questions that reveal a bias in embeddings. Take the following analogies encoding in some learned word embeddings: \\[\\text{\"Man is to computer programmer as woman is to homemaker\"}\\] \\[\\text{\"Father is to doctor as mother is to nurse\"}\\] Clearly, these embeddings are encoding unfortunate gender stereotypes. Note that these are only examples, biases against ethnicity, age, sexual orientation, etc. can also become encoded by the learned word embeddings. In order for these biases to be learned by the model, they must first exist in the data used to train it. Addressing bias in word embeddings Lets say we have already learned 300D embeddings. We are going to stick to gender bias for simplicities sake. The process for debiasing these embeddings is as follows: 1 Identify bias direction : Take a few examples where the only difference (or only major difference) between word embeddings is gender, and subtract them: e_{he} - e_{she} e_{he} - e_{she} e_{male} - e_{female} e_{male} - e_{female} ... Average the differences. The resulting vector encodes a 1D subspace that may be the bias axis. The remaining 299 axes are the non-bias direction Note in the original paper, averaging is replaced by SVD, and the bias axis is not necessarily 1D. 2 Neutralize : For every word that is not definitional, project them onto non-bias direction or axis to get rid of bias. These do not include words that have a legitimate gender component, such as \"grandmother\" but do include words for which we want to eliminate a learned bias, such as \"doctor\" or \"babysitter\" (in this case a gender bias, but it could also be a sexual orientation bias, for example). Choosing which words to neutralize is challenging. For example, \"beard\" is characteristically male, so its likely not a good idea to neutralize it with respect to gender. The authors of the original paper actually trained a classifier to determine which words were definitional with respect to the bias (in our case gender). It turns out that english does not contain many words that are definitional with respect to gender. 3 Equalize pairs : Take pairs of definitional words (such as \"grandmother\" and \"grandfather\" and equalize their difference to the non-bias direction or axis. This ensures that these words are equidistant to all other words for which we have \"neturalized\" and encoded bias. This process is a little complicated, but the end results is that these pairs of words, (e.g. \"grandmother\" and \"grandfather\" ) are moved to a pair of points that are equidistant from the non-bias direction or axis. It turns out, the number of these pairs is very small. It is quite feasible to pick this out by hand. Summary Reducing or eliminating bias of our learning algorithms is a very important problem because these algorithms are being asked to help with or to make more and more important decisions in society. In this lecture we saw just one set of ideas for how to go about trying to address this problem, but this is still a very much an ongoing area of active research by many researchers.","title":"Week 2"},{"location":"sequence_models/week_2/#week-2-natural-language-processing-word-embeddings","text":"Natural language processing and deep learning is an important combination . Using word vector representations and embedding layers, you can train recurrent neural networks with outstanding performances in a wide variety of industries. Examples of applications are sentiment analysis , named entity recognition ( NER ) and machine translation .","title":"Week 2: Natural Language Processing &amp; Word Embeddings"},{"location":"sequence_models/week_2/#introduction-to-word-embeddings-word-representation","text":"Last week, we learned about RNNs, GRUs, and LSTMs. In this week, you see how many of these ideas can be applied to Natural Language Processing ( NLP ), which is one of the areas of AI being revolutionized by deep learning. One of the key ideas you learn about is word embeddings , which is a way of representing words. So far, we have been representing words with a vocabulary, \\(V\\), of one-hot-encoded vectors. Lets quickly introduce a new notation. If the token \" Man \" is in position 5391 in our vocabulary \\(V\\) then we denote the corresponding one-hot-encoded vector as \\(O_{5391}\\). One of the weaknesses of this representation is that it treats each word as a \"thing\" onto itself, and doesn't allow a language model to generalize between words. Take the following examples: \\[x_1: \\text{\"I want a glass of orange juice\"}\\] \\[x_2: \\text{\"I want a glass of apple juice\"}\\] Cleary, the example sentences are extremely semantically similar. However, in a one-hot encoding scheme, a model which has learned that \\(x_1\\) is a likely sentence is unable to fully generalize to example \\(x_2\\), as the relationship between \" apple \" and \" orange \" is not any closer than the relationship between \" orange \" and any other word in the vocabulary. Notice, in fact, that the inner product between any two one-hot encoded vectors: \\[O_i \\times O_j = \\vec 0 \\text{ for } \\forall i,j\\] And similarly, the euclidean distance between any two one-hot encoded vectors is identical: \\[||O_i - O_j|| = \\sqrt{|V|}\\text{ for } \\forall i,j\\] To build our intuition of word embeddings, image a contrived example where we represent each word with some feature representation : We could imagine many features (with values -1 to 1, say) that can be used to build up a featue representation, an \\(f_n\\)-dimensional vector, of each word. Similarly to our one-hot representations, lets introduce a new notation \\(e_i\\) to represent the embedding of token \\(i\\) in our vocabulary \\(V\\). Where \\(f_n\\) is the number of features. Thinking back to our previous example, notice that our representations for the tokens \" apple \" and \" orange \" become quite similar. This is the critical point, and what allows our language model to generalize between word tokens and even entire sentences. In the later videos, we will see how to learn these embeddings. Note that the learned representations do not have an easy interpretation like the dummy embeddings we presented above.","title":"Introduction to word embeddings: Word Representation"},{"location":"sequence_models/week_2/#visualizing-word-embeddings","text":"Once these feature vectors or embeddings are learned, a popular thing to do is to use dimensionality reduction to embed them into a 2D geometric space for easy visualization. An example of this using our word representations presented above: We notice that semantically similar words tend to cluster together, and that each cluster seems to roughly represent some idea or concept (i.e., numbers typically cluster together). This demonstrates our ability to learn similar feature vectors for similar tokens and will allow our models to generalize between words and even sentences. A common algorithm for doing this is the t-SNE algorithm. The reason this feature representations are called embeddings is because we imagine that we are embedding each word into a geometric space (say, of 300 dimensions). If you imagine a cube, we can think of giving each word a single unit of space within this cube.","title":"Visualizing word embeddings"},{"location":"sequence_models/week_2/#introduction-to-word-embeddings-using-word-embeddings","text":"In the last lecture, you saw what it might mean to learn a featurized representations of different words. In this lecture, you see how we can take these representations and plug them into NLP applications.","title":"Introduction to word embeddings: Using word embeddings"},{"location":"sequence_models/week_2/#named-entity-recognition-example","text":"Take again the example of named entity recognition, and image we have the following example: Let's assume we correctly identify \" Sally Johnson \" as a PERSON entity. Now imagine we see the following sequence: \\[x: \\text{\"Robert Lin is a durian cultivator\"}\\] Note that durian is a type of fruit. In all likelihood, a model using word embeddings as input should be able to generalize between the two input examples, a take advantage of the fact that it previously labeled the first two tokens of a similar training example (\" Sally Johnson \") as a PERSON entity. But how does the model generalize between \" orange farmer \" and \" durian cultivator \"? Because word embeddings are typically trained on massive unlabeled text corpora, on the scale of 1 - 100 billion words. Thus, it is likely that the word embeddings would have seen and learned the similarity between word pairs (\" orange \", \" durian \") and (\" farmer \", \" cultivator \"). In truth, this method of transfer learning is typically how we use word embeddings in NLP tasks.","title":"Named entity recognition example"},{"location":"sequence_models/week_2/#transfer-learning-and-word-embeddings","text":"How exactly do we utilize transfer learning of word embeddings for NLP tasks? Learn word embeddings from large text corpus (1-100B words), OR, download pre-trained embeddings online. Transfer the embedding to a new task with a (much) smaller training set (say, 100K words). (Optional): Continue to fine-tune the word embeddings with new data. In practice, this is only advisable if your training dataset is quite large. This method of transfer learning with word embeddings has found use in NER, text summarization, co-reference resolution, and parsing. However, it has been less useful for language modeling and machine translation (especially when a lot of data for these tasks is available). One major advantage to using word embeddings to represent tokens is that it reduces the dimensionality of our inputs, compared to the one-hot encoding scheme. For example, a typical vocabulary may be 10,000 or more word types, while a typical word embedding may be around 300 dimensions.","title":"Transfer learning and word embeddings"},{"location":"sequence_models/week_2/#introduction-to-word-embeddings-properties-of-word-embeddings","text":"By now, you should have a sense of how word embeddings can help you build NLP applications. One of the most fascinating properties of word embeddings is that they can also help with analogy reasoning . And while reasoning by analogy may not be, by itself, the most important NLP application, it helps to convey a sense of what information these word embeddings are capturing.","title":"Introduction to word embeddings: Properties of word embeddings"},{"location":"sequence_models/week_2/#analogies","text":"Let us return to our previous example: Say we post the question: \" Man is to women as king is to what ? \" Many of us would agree that the answer to this question is \"Queen\" (in part because of humans remarkable ability to reason by analogy ). But can we have a computer arrive at the same answer using embeddings? First, lets simplify our earlier notation and allow \\(e_{man}\\) to denote the learned embedding for the token \" man \". Now, if we take the difference \\(e_{man} - e_{woman}\\), the resulting vector is closest to \\(e_{king} - e_{queen}\\). Note you can confirm this using our made up embeddings in the table. Explicitly, an algorithm to answer the question \" Man is to women as king is to what ? \" would involve computing \\(e_{man} - e_{woman}\\), and then finding the token \\(w\\) that produces \\(e_{man} - e_{woman} \\approx e_{king} - e_{w}\\). This ability to mimic analogical reasoning and other interesting properties of word embeddings were introduced in this paper . Lets try to visualize why this makes sense. Imagine our word embedding plotted as vectors in a 300D space (represented here in 2 dimensions for visualization). We would expect our vectors to line up in a parallelogram: Note that in reality, if you use a dimensionality reduction algorithm such as t-SNE, you will find that this expected relationship between words in an analogy does not hold: We want to find \\(e_w \\approx e_{king} - e_{man} + e_{woman}\\). Our algorithm is thus: \\[argmax_w \\; sim(e_w, e_{king} - e_{man} + e_{woman})\\] The most commonly used similarity function, \\(sim\\) is the cosine similarity : \\[sim(u, v) = \\frac{u^Tv}{||u||_2||v||_2}\\] Which represents the cosine of the angle between the two vectors \\(u, v\\). Note that we can also use euclidian distance , although this is technically a measure of dissimilarity, so we need to take its negative. See here for an intuitive explanation of the cosine similarity measure.","title":"Analogies"},{"location":"sequence_models/week_2/#introduction-to-word-embeddings-embedding-matrix","text":"Let's start to formalize the problem of learning a good word embedding. When we implement an algorithm to learn word embeddings, what we actually end up learning is an embedding matrix . Say we are using a vocabulary \\(V\\) where \\(||V|| = 10,000\\). We want to learn an embedding matrix \\(E\\) of shape \\((300, 10000)\\) (i.e., the dimension of our word embeddings by the number of words in our vocabulary). \\[E = \\begin{bmatrix}e_{1, 1} & ... & e_{10000, 1}\\\\ ... & ... \\\\ e_{1, 300} & & ...\\end{bmatrix}\\] Where \\(e_{i, j}\\) is the \\(j-th\\) feature in the \\(i-th\\) token. Recall that we used the notation \\(o_i\\) to represent the one-hot encoded representation of the \\(i-th\\) word in our vocabulary. \\[o_i = \\begin{bmatrix}0 \\\\ ... \\\\ 1 \\\\ ... \\\\ 0\\end{bmatrix}\\] If we take \\(E \\cdot o_i\\) then we are retrieving the embedding for the \\(i-th\\) word in \\(V\\), \\(e_i \\in \\mathbb R^{300 \\times 1}\\).","title":"Introduction to word embeddings: Embedding matrix"},{"location":"sequence_models/week_2/#summary","text":"The import thing to remember is that our goal will be to learn an embedding matrix \\(E\\). To do this, we initialize \\(E\\) randomly and learn all the parameters of this, say, 300 by 10,000 dimensional matrix. Finally, \\(E\\) multiplied by our one-hot vector \\(o_i\\) gives you the embedding vector for token \\(i\\), \\(e_i\\). Note that while this method of retrieving embeddings from the embedding matrix is intuitive, the matrix-vector multiplication is not efficient. In practice, we use a specialized function to lookup a column \\(i\\) of the matrix \\(E\\), an embedding \\(e_i\\).","title":"Summary"},{"location":"sequence_models/week_2/#learning-word-embeddings-learning-word-embeddings","text":"Lets begin to explore some concrete algorithms for learning word embeddings. In the history of deep learning as applied to learning word embeddings, people actually started off with relatively complex algorithms. And then over time, researchers discovered they can use simpler and simpler algorithms and still get very good results, especially for a large dataset. Some of the algorithms that are most popular today are so simple that they almost seem little bit magical. For this reason, it's actually easier to develop our intuition by introducing some of the more complex algorithms first. Note that a lot of the ideas from this lecture came from Bengio et. al., 2003 .","title":"Learning word embeddings: Learning word embeddings"},{"location":"sequence_models/week_2/#algorithm-1","text":"We will introduce an early algorithm for learning word embeddings, which was very successful, with an example. Lets say you are building a neural language model . We want to be able to predict the next word for any given sequence of words. For example: Note that, another common strategy is to pick a fixed window of words before the word we need to predict. The window size becomes a hyperparameter of the algorithm. \\[x: \\text{\"I want a glass of orange ____\"}\\] One way to approach this problem is to lookup the embeddings for each word in the given sequence, and feed this to a densely connected layer which itself feeds to a single output unit with softmax . Imagine our embeddings are 300 dimensions. Then our input layer is \\(\\mathbb R^{6 \\times 300}\\). Our dense layer and output softmax layer have their own parameters, \\(W^{[1]}, b^{[1]}\\) and \\(W^{[2]}, b^{[2]}\\). We can then use back-propagation to learn these parameters along with the embedding matrix. The reason this works is because the algorithm is incentivized to learn good word embeddings in order to generalize and perform well when predicting the next word in a sequence.","title":"Algorithm 1"},{"location":"sequence_models/week_2/#generalizing","text":"Imagine we wanted to learn the word \" juice \" in the following sentence: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] Typically, we would provide a neural language model with some context and have it predict this missing word from that context. There are many choices here: \\(n\\) words on the left & right of the word to predict last \\(n\\) word before the word to predict a single, nearby word What researchers have noticed is that if your goal is to build a robust language model, choosing some \\(n\\) number of words before the target word as the context works best. However, if you goal is simply to learn word embeddings, then choosing other, simpler contexts (like a single, nearby word) work quite well. To summarize, by posing the language modeling problem in which some context (such as the last four words) is used to predict some target word, we can effectively learn the input word embeddings via backprogopogation.","title":"Generalizing"},{"location":"sequence_models/week_2/#learning-word-embeddings-word2vec","text":"In the last lecture, we used a neural language model in order to learn good word embeddings. Let's take a look at the the Word2Vec algorithm, which is a simpler and more computational efficient way to learn word embeddings. Most of the ideas in this lecture come from this paper: Mikolov et al., 2013 . We are going to discuss the word2Vec skip-gram model for learning word embeddings.","title":"Learning Word Embeddings: Word2vec"},{"location":"sequence_models/week_2/#skip-gram","text":"Let say we have the following example: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] In the skip-gram model, we choose ( context , target ) pairs in order to create the data needed for our supervised setting. To do this, for each context word we randomly choose a target word within some window (say, +/- 5 words). Our learning problem: \\[x \\rightarrow y\\] \\[\\text{Context }, c\\;(\\text{\"orange\"}) \\rightarrow \\text{Target}, t\\; (\\text{\"juice\"})\\] The learning problem then, is to choose the correct target word within a window around the context word. Clearly, this is a very challenging task. However, remember that the goal is not to perform well on this prediction task, but to use the task along with backprogpogation to force the model to learn good word embeddings.","title":"Skip-gram"},{"location":"sequence_models/week_2/#model-details","text":"Lets take \\(||V|| = 10000\\). Our neural network involves an embedding layer, \\(E\\) followed by a softmax layer, similar to the one we saw in the previous lecture: \\[E \\cdot o_c \\rightarrow e_c \\rightarrow softmax \\rightarrow \\hat y\\] Our softmax layer computes: \\[p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] where \\(\\theta_t\\) is the parameters associated with output \\(t\\) and the bias term has been omitted. Which is a \\(|V|\\) dimensional vector containing the probability distribution of the target word being any word in the vocabulary for a given context word. Our loss is the familiar negative log-likelihood: \\[\\ell (\\hat y, y) = -\\sum^{10000}_{i=1} y_i \\log \\hat y_i\\] To summarize , our model looks up an embeddings in the embedding matrix which contains our word embeddings and is updated by backpropagation during learning. These embeddings are used by a softmax layer to predict a target word for a given context.","title":"Model details"},{"location":"sequence_models/week_2/#problems-with-softmax-classification","text":"It turns out, there are a couple problems with the algorithm as we have described it, primarily due to the expensive computation of the softmax layer. Recall our softmax calculation: \\[p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] Every time we wish to perform this softmax classification (that is, every step during training or testing), we need to perform a sum over \\(|V|\\) elements. This quickly becomes a problem when our vocabulary reaches sizes in the milllions or even billions. One solution is to use a hierarchial softmax classifier. The basic idea is to build a Huffman tree based on word frequencies. In this scheme, the number of computations to perform in the softmax layers scales as \\(\\log |V|\\) instead of \\(V\\). I don't really understand this.","title":"Problems with softmax classification"},{"location":"sequence_models/week_2/#how-to-sample-the-context-c","text":"Sampling our target words, \\(t\\) is straightforward once we have sampled their context, \\(c\\), but how do we choose \\(c\\) itself? Once solution is to sample uniform randomly. However, this leads to us choosing extremely common words (such as the , a , of , and , also known as stop words) much too often. This is a problem, as many updates would be made to \\(e_c\\) for these common words and many less updates would be made for less common words. In practice, we use different heuristics to balance the sampling between very common and less common words.","title":"How to sample the context c?"},{"location":"sequence_models/week_2/#summary_1","text":"In the original word2vec paper, you will find two versions of the word2vec model: the skip-gram one introduced here and another called CBow , the continuous bag-of-words model. This model takes the surrounding contexts from a middle word, and uses them to try to predict the middle word. Each model has its advantages and disadvantages. The key problem with the skip-gram model as presented so far is that the softmax step is very expensive to calculate because it sums over the entire vocabulary size.","title":"Summary"},{"location":"sequence_models/week_2/#learning-word-embeddings-negative-sampling","text":"In the last lecture, we saw how the skip-gram model allows you to construct a supervised learning task by mapping from contexts to targets, and how this in turn allows us to learn a useful word embeddings. The major the downside of this approach was that was the softmax objective was very slow to compute . Lets take a look at a modified learning problem called negative sampling , which allows us to do something similar to the skip-gram model but with a much more efficient learning algorithm. Again, most of the ideas in this lecture come from this paper: Mikolov et al., 2013 . Similar to the skip-gram model, we are going to create a supervised learning setting from unlabeled data. Explicitly, the problem is to predict whether or not a given pair of words is a context , target pair. First, we need to generate training examples: Positive examples are generated exactly how we saw with the skip-gram model, i.e., by sampling a context word and choosing a target word within some window around the context. To generate the negative examples , we take a sampled context word and then for some \\(k\\) number of times, we choose a target word randomly from our vocabulary (under the assumption that this random word won't be associated with our sampled context word). As an example, take the following sentence: \\[x: \\text{\"I want a glass of orange juice to go along with my cereal\"}\\] Then we might construct the following (context, target) training examples: \\((orange, juice, 1)\\) \\((orange, king, 0)\\) \\((orange, book, 0)\\) \\((orange, the, 0)\\) \\((orange, of, 0)\\) Where 1 denotes a positive example and 0 a negative example. Note that this leads to an obvious problem: some of our randomly chosen target words in our generated negative examples will in fact be within the context of the sampled context word. It turns out this is OK, as much more often than not our generated negative examples are truly negative examples. Next, we define a supervised learning problem, where our inputs \\(x\\) are these generated positive and negative examples, and our targets \\(y\\) are whether or not the input represents a true ( context , target ) pair (1) or not (0): \\[x \\rightarrow y\\] \\[(context, target) \\rightarrow 1 \\text{ or } 0\\] Explicitly, we are asking the model to predict whether or not the two words came from a distribution generated by sampling from within a context (defined3 as some window around the words) or a distribution generated by sampling words from the vocabulary at random. How do you choose \\(k\\)? Mikolov et. al suggest \\(k=5-20\\) for small datasets, and \\(k=2-5\\) for large datasets. You can think of \\(k\\) as a 1:\\(k\\) ratio of positive to negative examples.","title":"Learning Word Embeddings: Negative Sampling"},{"location":"sequence_models/week_2/#model-details_1","text":"Recall the softmax classifier from the skip-gram model: \\[\\text{Softmax: } p(t | c) = \\frac{e^{\\theta_t^Te_c}}{\\sum^{10000}_{j=1}e^{\\theta_j^Te_c}}\\] For our model which uses negative sampling, first define each input, output pair as \\(c, t\\) and \\(y\\) respectively. Then, we define a logistic regression classifier: \\[p(y = 1 | c, t) = \\sigma(\\theta_t^Te_c)\\] Where \\(\\theta_t\\) represents the parameter vector for a possible target word \\(t\\), and \\(e_c\\) the embedding for each possible context word. NOTE: totally lost around the 7 min mark. Review this. This technique is called negative sampling because we generate our training data for the supervised learning setting by first creating a positive example and then sampling \\(k\\) negative examples.","title":"Model details"},{"location":"sequence_models/week_2/#selecting-negative-examples","text":"The final import point is how we actually sample negative examples in practice . One option is to sample the target word based on the empirical frequency of words in your training corpus. The problem of this solution is that we end up sampling many highly frequent stop words, such as \"and\", \"of\", \"or\", \"but\", etc. Another extreme is to sample the negative examples uniformly random. However, this also leads to a very non-representive sampling of target words. What the authors found to work best is something in between: \\[P(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum^{|V|}_{j=1}f(w_j)^{\\frac{3}{4}}}\\] Here, we sample proportional to the frequency of a word to the power of \\(\\frac{3}{4}\\). This is somewhere between the two extremes of sampling words by their frequency and sampling words at uniform random.","title":"Selecting negative examples"},{"location":"sequence_models/week_2/#summary_2","text":"To summarize, we've seen how to learn word vectors with a softmax classier , but it's very computationally expensive. we've seen that by changing from a softmax classification to a bunch of binary classification problems, we can very efficiently learn words vectors. as is the case in other areas of deep learning, there are open source implementations of the discussed algorithms you can use to learn these embeddings. There are also pre-trained word vectors that others have trained and released online under permissive licenses.","title":"Summary"},{"location":"sequence_models/week_2/#learning-word-embeddings-glove-word-vectors","text":"The final algorithm we will look at for learning word embeddings is global vectors for word representation ( GloVe ). While not used as much as word2vec models, it has its enthusiasts -- in part because of its simplicity. This algorithm was original presented here . Previously, we were sampling pairs of words ( context , target ) by picking two words that appear in close proximity to one another in our text corpus. In the GloVe algorithm, we define: \\(X_{ij}\\): the number of times word \\(i\\) appears in the context of word \\(j\\). \\(X_{ij}\\) == \\(X_{ji}\\) Note that \\(X_{ij}\\) == \\(X_{ji}\\) is not necessarily true in other algorithms (e.g., if we were to define the context as being the immediate next word). Notice that \\(i\\) and \\(j\\) play the role of \\(c\\) and \\(t\\).","title":"Learning Word Embeddings: GloVe word vectors"},{"location":"sequence_models/week_2/#model","text":"The models objective is as follows: Minimize \\sum^{|V|}_{i=1} \\sum^{|V|}_{j=1} f(X_{ij}) (\\theta_i^Te_j + b_i + b_j' - \\log X_{ij})^2 \\sum^{|V|}_{i=1} \\sum^{|V|}_{j=1} f(X_{ij}) (\\theta_i^Te_j + b_i + b_j' - \\log X_{ij})^2 Think of \\(\\theta_i^Te_j\\) as a measure of how similar two words are, based on how often the occur together: \\(\\log X_{ij}\\). More specifically, we are trying to minimize this difference using gradient descent by searching for the pair of words \\(i, j\\) whose inner product \\(\\theta_i^Te_j\\) is a good predictor of how often they are going to appear together, \\(\\log X_{ij}\\). If \\(X_{ij} = 0\\), \\(\\log X_{ij} = \\log 0 = - \\infty\\) which is undefined. We use \\(f(X_{ij})\\) as weighting term, which is 0 when \\(X_{ij}\\) = 0, so we don't sum over pairs of words \\(i, j\\) when \\(X_{ij} = 0\\). \\(f(X_{ij})\\) is also used to weight words, such that extremely common words don't \"drown\" out uncommon words. There are various heuristics for choosing \\(f(X_{ij})\\). You can look at the original paper for details for how to choose this heuristic. Note, we use the convention \\(0 \\log 0 = 0\\) Something to note about this algorithm is that the roles of \\(theta\\) and \\(e\\) are now completely symmetric . So, \\(\\theta_i\\) and \\(e_j\\) are symmetric in that, if you look at the math, they play pretty much the same role and you could reverse them or sort them around, and they actually end up with the same optimization objective. In fact, one way to train the algorithm is to initialize \\(\\theta\\) and \\(e\\) both uniformly and use gradient descent to minimize its objective, and then when you're done for every word, to then take the average: \\[e_w^{final} = \\frac{e_w + \\theta_w}{2}\\] because \\(theta\\) and \\(e\\) in this particular formulation play symmetric roles unlike the earlier models we saw in the previous videos, where \\(theta\\) and \\(e\\) actually play different roles and couldn't just be averaged like that.","title":"Model"},{"location":"sequence_models/week_2/#a-note-of-the-featurization-view-of-word-embeddings","text":"Recall that when we first introduced the idea of word embeddings, we used a sort of featurization view to motivate the reason why we learn word embeddings in the first place. We said, \"Well, maybe the first dimension of the vector captures gender, the second, age...\", so forth and so on. However in practice, we cannot guarantee that the individual components of the embeddings are interpretable. Why? Lets say that there is some \"space\" where the first axis of the embedding vector is gender, and the second age. There is no way to guarantee that the actual dimension for each \"feature\" that the algorithm arrives at will be easily interpretable by humans. Indeed, if we consider the learned representation of each context, target pair, we note that: \\[\\theta_i^Te_j = (A\\theta_i)^T(A^{-T}e_j) = \\theta_i^TA^TA^{-T}e_j\\] Where \\(A\\) is some arbitrary invertible matrix. The key take away is that the dimensions learned by the algorithm are not human interpretable, and each dimension typically encodes some part of what we might think of a feature, as opposed to encoding an entire feature itself.","title":"A note of the featurization view of word embeddings"},{"location":"sequence_models/week_2/#applications-using-word-embeddings-sentiment-classification","text":"Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they're talking about. It is one of the most important building blocks in NLP and is used in many applications. One of the challenges of sentiment classification is a lack of labeled data. However, with word embeddings, you're able to build good sentiment classifiers even with only modest-size label training sets. Lets look at an example: \\[x: \\text{\"The dessert is excellent.\", } y: 4/5 \\text{ stars}\\] \\[x: \\text{\"Service was quite slow.\", } y: 2/5 \\text{ stars}\\] \\[x: \\text{\"Good for a quick meal, but nothing special.\", } y: 3/5 \\text{ stars}\\] \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 \\text{ stars}\\] While we are using restaurant reviews as an example, sentiment analysis is often applied to voice of the customer materials such as reviews and social media. Common training set sizes for sentiment classification would be around 10,000 to 100,000 words. Given these small training set sizes, word embeddings can be extremely useful. Lets use the above examples to introduce a couple of different algorithms","title":"Applications using Word Embeddings: Sentiment Classification"},{"location":"sequence_models/week_2/#simple-sentiment-classification-model","text":"Take, \\[x: \\text{\"The dessert is excellent.\", } y: 4/5 \\text{ stars}\\] As usual, we map the tokens in the input examples to one-hot vectors, multiply this by a pre-trained embedding matrix and obtain our embeddings, \\(e_w\\). Using a pre-trained matrix is essentially a form of transfer learning, as we are able to encode information learned on a much larger corpus (say, 100B tokens) and use it for learning on a much smaller corpus (say, 10,000 tokens). We could then average or sum these embeddings, and pass the result to a softmax classifier which outputs \\(\\hat y\\), the probability of the review being rated as 1, 2, 3, 4 or 5 stars. This algorithm will work OK, but fails to capture negation of positive words (as it does not take into account word order). For example: \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 stars\\] might incorrectly be predicted to correspond with a high star rating, because of the appearance of \"good\" three times.","title":"Simple sentiment classification model"},{"location":"sequence_models/week_2/#rnn-sentiment-classification-model","text":"A more sophisticated model involves using the embeddings as inputs to an RNN, which uses a softmax layer at the last timestep to predict a star rating: Recall that we actually saw this example when discussing many-to-one RNN architectures. Unlike the previous, simpler model, this model takes into account word order and performs much better on examples such as: \\[x: \\text{\"Completely lacking in good taste, good service, and good ambience.\", } y: 1/5 \\text{ stars}\\] which contain many negated, positive words. When paired with pre-trained word embeddings, this model works quite while.","title":"RNN sentiment classification model"},{"location":"sequence_models/week_2/#summary_3","text":"Pre-trained word embeddings are especially useful for NLP tasks where we don't have a lot of training data. In this lecture, we motivated that idea by showing how pre-trained word embeddings can be used as inputs to very simple models to perform sentiment classification.","title":"Summary"},{"location":"sequence_models/week_2/#applications-using-word-embeddings-debiasing-word-embeddings","text":"Machine learning and AI algorithms are increasingly trusted to help with, or to make, extremely important decisions. As such, we would like to make sure that, as much as possible, they're free of undesirable forms of bias, such as gender bias, ethnicity bias and so on. Lets take a look at reducing bias in word embeddings. Most of the ideas in this lecture came from this paper . When we first introduced the idea of word embeddings, we leaned heavily on the idea of analogical reasoning to build our intuition. For example, we were able to ask \"Man is to woman as king is to ____?\" and using word emebddings, arrive at the example queen. However, we can also ask other questions that reveal a bias in embeddings. Take the following analogies encoding in some learned word embeddings: \\[\\text{\"Man is to computer programmer as woman is to homemaker\"}\\] \\[\\text{\"Father is to doctor as mother is to nurse\"}\\] Clearly, these embeddings are encoding unfortunate gender stereotypes. Note that these are only examples, biases against ethnicity, age, sexual orientation, etc. can also become encoded by the learned word embeddings. In order for these biases to be learned by the model, they must first exist in the data used to train it.","title":"Applications using Word Embeddings: Debiasing word embeddings"},{"location":"sequence_models/week_2/#addressing-bias-in-word-embeddings","text":"Lets say we have already learned 300D embeddings. We are going to stick to gender bias for simplicities sake. The process for debiasing these embeddings is as follows: 1 Identify bias direction : Take a few examples where the only difference (or only major difference) between word embeddings is gender, and subtract them: e_{he} - e_{she} e_{he} - e_{she} e_{male} - e_{female} e_{male} - e_{female} ... Average the differences. The resulting vector encodes a 1D subspace that may be the bias axis. The remaining 299 axes are the non-bias direction Note in the original paper, averaging is replaced by SVD, and the bias axis is not necessarily 1D. 2 Neutralize : For every word that is not definitional, project them onto non-bias direction or axis to get rid of bias. These do not include words that have a legitimate gender component, such as \"grandmother\" but do include words for which we want to eliminate a learned bias, such as \"doctor\" or \"babysitter\" (in this case a gender bias, but it could also be a sexual orientation bias, for example). Choosing which words to neutralize is challenging. For example, \"beard\" is characteristically male, so its likely not a good idea to neutralize it with respect to gender. The authors of the original paper actually trained a classifier to determine which words were definitional with respect to the bias (in our case gender). It turns out that english does not contain many words that are definitional with respect to gender. 3 Equalize pairs : Take pairs of definitional words (such as \"grandmother\" and \"grandfather\" and equalize their difference to the non-bias direction or axis. This ensures that these words are equidistant to all other words for which we have \"neturalized\" and encoded bias. This process is a little complicated, but the end results is that these pairs of words, (e.g. \"grandmother\" and \"grandfather\" ) are moved to a pair of points that are equidistant from the non-bias direction or axis. It turns out, the number of these pairs is very small. It is quite feasible to pick this out by hand.","title":"Addressing bias in word embeddings"},{"location":"sequence_models/week_2/#summary_4","text":"Reducing or eliminating bias of our learning algorithms is a very important problem because these algorithms are being asked to help with or to make more and more important decisions in society. In this lecture we saw just one set of ideas for how to go about trying to address this problem, but this is still a very much an ongoing area of active research by many researchers.","title":"Summary"},{"location":"sequence_models/week_3/","text":"Week 3: Sequence models & Attention mechanism In this series, we will look primarily at sequence models, which are useful for everything from machine translation to speech recognition. We will also look at attention models. Sequence models & Attention mechanism: Basic Models Machine translation Lets say we want to translate from french to english . We will use the following examples: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} y: \\text{\"Jane is visiting Africa in September\"} y: \\text{\"Jane is visiting Africa in September\"} Recall that we represent individual elements in the input sequence as x^{<t>} x^{<t>} , and in the output sequence as y^{<t>} y^{<t>} . Most of the ideas presented in this lecture are from this paper and this paper . First, we build our encoder , an RNN (LSTM or GRU) which processes the input sequence. The encoder outputs a vector that represents the learned representation of the input sequence. A decoder takes this output sequence, and outputs the translation one word at a time. Note that the outputs from each timestep in the decoder network are actually passed as input for the next timestep in the case of language modelling. Given enough data, this sequence to sequence architecture actually works quite well. Image captioning The task of image captioning involves inputing an image, and having the network generate a natural language caption. x: \\text{an image of a cat of a chair} x: \\text{an image of a cat of a chair} y: \\text{\"A cat sitting on a chair\"} y: \\text{\"A cat sitting on a chair\"} Typically, we use a CNN as our encoder , without any classification layer at the end. We feed the learned representation to a decoder , an RNN, which generates and output sequence which is our image caption. Summary Simple sequence to sequence (seq2seq) models are comprised of an encoder and decoder , which themselves are neural networks (typically recurrent or convolutional). Seq2seq architectures are able to preform reasonably well when given enough data. With certain modifications (attention) and additions (beam search), they are able to achieve state-of-the-art performance on tasks like machine translation and image captioning . These architecture difference slightly to some of the sequence generation architectures we saw previously however, and we will explore the difference in the next lecture. Sequence models & Attention mechanism: Picking the most likely sentence While there are some similarities between the sequence to sequence machine translation model and the language models that you have worked within the first week of this course, there are some significant differences as well. Machine translation as a conditional language model We can think of machine translation as building a conditional language model . First, recall the language model we worked with previously: The machine translation model looks like: Notice that the decoder network looks pretty much identical to the language model. However, instead of the initial hidden state being a^{<t>} = 0 a^{<t>} = 0 , the initial hidden state is initialized with the learned representation from the encoder. That is why we can think of this as a conditional language model . Instead of modeling the probability of any sentence, it is now modeling the probability of say, the output English translation, conditioned on some input French sentence. Finding the most likely translation So how does this work in practice? Our model produces the following: P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) The probability of different corresponding translated sentences based on the input sentence, x x . Unlike the language model we saw earlier, we do not want to sample words randomly . Instead, we want to choose the sentence y y that maximizes P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) , argmax_y P(y^{<1>}, ..., y^{<T_y>} | x) argmax_y P(y^{<1>}, ..., y^{<T_y>} | x) The most common algorithm for solving this problem is called the beam search , which we will look at in the next lecture. Before we look at beam search, you might be wondering, why not use a greedy search ? In our case, this would involve choosing the most likely output, \\hat y^{<t>} \\hat y^{<t>} at each timestep, one timestep at a time, starting from t=1 t=1 . In simple terms, this approach does not maximize the joint probability P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) , which is what we are really after. To see why this so, take our input example: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} And our outputs generated by beam search and greedy search respectively: beam: \\text{\"Jane is visiting Africa in September\"} beam: \\text{\"Jane is visiting Africa in September\"} greedy: \\text{\"Jane is going to be visiting Africa in September\"} greedy: \\text{\"Jane is going to be visiting Africa in September\"} In greedy search, we are optimizing the output at each timestep without regard for the overall sequence. As such, we will often produce less succicent, and more verbose sentences. In this case, p(\\text{\"Jane is\"} | \\text{\"going\"}) p(\\text{\"Jane is\"} | \\text{\"going\"}) > p(\\text{\"Jane is\"} | \\text{\"visiting\"}) p(\\text{\"Jane is\"} | \\text{\"visiting\"}) , which is why we produce the more verbose \" is going to be visiting \" as opposed to \" is visiting \". While this is a contrived example, it captures a larger phenomena. When generating sequences, it is often a good idea to maximize the joint probability across the entire sequence. Otherwise, we end up with sub-optimal performance tasks in which there is strong dependencies between elements of a sequence. Another important point, is that the space of all possible output sentences is huge. For a ten-word sentence drawn and a vocabulary of 10,000 words, we have 10,000^{10} 10,000^{10} possible sentences. For this reason, we need an approximate search algorithm (a heuristic ). While this won't always succeed in finding the sentence \\hat y \\hat y that maximizes that conditional probability, it will typically do a \"good enough\" job without needed to enumerating all possible sentences. Summary In this lecture, you saw how machine translation can be posed as a conditional language modeling problem . One major difference between this and the earlier language modeling problems is rather than generating a sentence at random, you try to find the most likely translation. The set of all sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to an approximate search algorithm. Sequence models & Attention mechanism: Beam Search Recall that, for a machine translation system given an input French sentence, you don't want to output a random English translation, you want to output the best and the most likely English translation. The same is also true for speech recognition where, given an input audio clip, you don't want to output a random text transcript of that audio, you want to output the best , maybe the most likely , text transcript. Beam search is the most widely used algorithm to do this. Beam search algorithm Step 1 The first thing beam search needs to do is pick the first word, \\hat y^{<1>} \\hat y^{<1>} of the translation given our input sequence x x , P(\\hat y^{<1>} | x) P(\\hat y^{<1>} | x) . Contrary to greedy search however, we can evaluate multiple choices at the same time. The number of choices is designated by a parameter of the algorithm, the beam width B B . Lets say for our purposes that we choose B = 3 B = 3 . In practice, we run our sentence to be translated through the encoder-decoder network and store in memory the three most likely, first words of the translated sentence. Step 2 For each of these three choices, we consider what should be the second word in the translated output sequence, \\hat y^{<2>} \\hat y^{<2>} . Recall that the previously selected output, \\hat y^{<1>} \\hat y^{<1>} is fed as input to the next timestep in the decoder network. In practice, we are looking to compute: P(\\hat y^{<1>},\\; \\hat y^{<2>} | x) = P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<1>},\\; \\hat y^{<2>} | x) = P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) Where P(\\hat y^{<1>}| x) P(\\hat y^{<1>}| x) is computed by the decoder network in the first step and P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) computed by the decoder network in the second step. In total, we evaluate B \\cdot |V| B \\cdot |V| possible choices for the second word, but we only save the top B B most likely choices for \\hat y^{<1>},\\; \\hat y^{<2>} \\hat y^{<1>},\\; \\hat y^{<2>} . We may actually end up dropping one or more possible choices we previously made for \\hat y^{<1>} \\hat y^{<1>} . An implementation detail: we instantiate B B copies of the seq2seq model to compute P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) , one for each choice of \\hat y^{<1>} \\hat y^{<1>} . Step n We continue this process, at each step n n computing P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) using the chain rule for conditional probabilities: P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) = P(\\hat y^{<n>} | \\hat y^{<1>}, ... , \\hat y^{<n-1>}, x) ... P(\\hat y^{<1>} | x) P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) = P(\\hat y^{<n>} | \\hat y^{<1>}, ... , \\hat y^{<n-1>}, x) ... P(\\hat y^{<1>} | x) We store the top B B most likely sequences \\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n-1>} \\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n-1>} and their corresponsding seq2seq model to use in the next step, where we again evaluate |V| |V| possible words for our B B number of previous choices. The outcome of this process, hopefully, is a prediction which is terminated by the special token. Notice that, when B = 1 B = 1 , this process essentially becomes the greedy search algorithm seen previously. Sequence models & Attention mechanism: Refinements to Beam Search In the last lecture, we saw the basic beam search algorithm. In this lecture, we will look at some little changes that make it work even better. Length normalization Recall that in beam search, we are trying to compute the following: argmax_y \\prod_{t=1}^{T_y}P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>}) argmax_y \\prod_{t=1}^{T_y}P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>}) The product term is capturing the idea that P(y^{<1>}, ..., y^{<T_y>} | x) = P(y^{<1>} | x) ... P(y^{<T_y>} | x, y^{<1>}, ..., y^{<T_y-1>}) P(y^{<1>}, ..., y^{<T_y>} | x) = P(y^{<1>} | x) ... P(y^{<T_y>} | x, y^{<1>}, ..., y^{<T_y-1>}) , which itself is just the chain rule for conditional probabilities. The problem here, is that many many small probabilities can lead to numerical underflow . For this reason, we take the log of the product (which becomes the sum of the log of the individual elements): argmax_y \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) This leads to a more numerically stable algorithm, which reduces the chance of rounding errors. Because \\log P(y|x) \\log P(y|x) and P(y|x) P(y|x) are monotonically increasing functions, the value that maximizes one also maximizes the other. Notice as well that the optimization objective actually favors short sentences (this is true of both formulations listed above). In machine translation, for example, This has the negative consequence of promoting short translated sentences over long translated sentences even when the longer sentence is preferable. A solution to this problem is to normalize our objective objective with respect to output length: argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) In practice, we typically introduce a new parameter \\alpha \\alpha which softens the length normalization, to retain a penalty on very very long output sequences. Beam search recap So, in full: As you run beam search you see a lot of sentences with length equal 1, 2, 3, and so on. Maybe you run beam search for 30 steps and you consider output sentences up to length 30, let's say. With a beam width of 3, you will be keeping track of the top three possibilities for each of these possible sentence lengths, 1, 2, 3, 4 and so on, up to 30. Then, you would look at all of the output sentences and score them against our objective: argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) Finally, of all of these sentences that we encounter through beam search, we pick the one that achieves the highest value on this normalized log probability objective (sometimes called a normalized log likelihood objective ). This brings us to the final translation, your outputs. Beam width discussion Finally, an implementational detail. How do you choose the beam width B B ? The larger B B is, the more possibilities you're considering, and thus the better the sentence you will probably find. But the larger B B is, the more computationally expensive your algorithm is, because you're also keeping a lot more possibilities around. So here are the pros and cons of setting B B to be very large versus very small: If the beam width is very large, then you consider a lot of possibilities, and so you tend to get a better result, but it will be slower, the memory requirements will also grow, and it will also be computationally slower. If you use a small beam width, then you are likely to get a worse result because you're keeping less possibilities in mind as the algorithm is running. But you get a result faster and the memory requirements will be lower. In the previous video, we used in our running example a beam width of three. In practice, that is on the small side. In production systems, it's not uncommon to see a beam width maybe around 10 -- 100 would be considered very large for a production system. For research systems, where people want to squeeze out every last drop of performance in order to publish a paper, tt's not uncommon to see people use beam widths of 1,000 to 3,000. In truth, you just need to try a variety of values of B B as you work through your application. Beware that as B B gets very large, there is often diminishing returns. Expect to see a huge gain as you go from a beam width of 1, which is essentially greedy search, to 3, to maybe 10, but gains may diminish from there on out. Sequence models & Attention mechanism: Error analysis on beam search In the third course of this sequence of five courses, we saw how error analysis can help you focus your time on doing the most useful work for your project. Because beam search is an approximate search algorithm, also called a heuristic , it doesn't always output the most likely sentence. So what if beam search makes a mistake? In this lecture, you'll learn how error analysis interacts with beam search and how you can figure out whether it is the beam search algorithm that's causing problems and worth spending time on, or whether it might be your RNN model that is causing problems and worth spending time on. Lets use the following, simple example to see how error analysis works with beam search: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} Assume further that our human-provided ( y*) y*) and machine provided ( \\hat y \\hat y ) translations for this sentence are: y*: \\text{\"Jane is visiting Africa in September\"} y*: \\text{\"Jane is visiting Africa in September\"} \\hat y: \\text{\"Jane visited Africa last September\"} \\hat y: \\text{\"Jane visited Africa last September\"} Our model has to two main components: our encoder-decoder architecture (which we will simply refer to as our 'RNN' from here on out) and the beam search algorithm applied to the outputs of this RNN. It would be extremely helpful if we could assign blame to one of these components individually when we get a bad translation, like our example \\hat y \\hat y . Similar to how it may be tempting to collect more training data when our models underperform (it 'never hurts'!) it is also tempting in this case to increase the beam width (again, it never seems to hurt!). Error analysis is a more principled way to improve our model, by helping us focus our attention on what is currently hurting performance the most. Error analysis applied to machine translation Recall that the RNN computes P(y | x) P(y | x) . The first step in our analysis is to compute P(y* | x) P(y* | x) and P(\\hat y | x) P(\\hat y | x) . We then ask, which probability is larger? If Case 1: P(y* | x) P(y* | x) > P(\\hat y | x) P(\\hat y | x) Beam search chose \\hat y \\hat y (recall, its optimization objective is \\hat y = argmax_y P(y|x) \\hat y = argmax_y P(y|x) ), but y* y* is more likely according to your model. Conclusion : We conclude that beam search is at fault. It faild to find a value for y y which maximizes P(y|x) P(y|x) . Case 2: P(y* | x) \\le P(\\hat y | x) P(y* | x) \\le P(\\hat y | x) y* y* is a better translation than \\hat y \\hat y . But our RNN predicted P(y* | x) \\le P(\\hat y | x) P(y* | x) \\le P(\\hat y | x) Conclusion : We conclude that RNN model is at fault. It failed to model a better translation as being more likely than a worse translation for a given input sentence. Note, we are ignoring length normalization here for simplicity. In reality, you would use the entire optimization objective with length normalization instead of just P(y|x) P(y|x) in your error analysis. In practice, we might collect some examples of our gold translations (provided by a human) and compare them in a table to the corresponding machine-provided translations, tallying P(y* | x) P(y* | x) and P(\\hat y | x) P(\\hat y | x) for each example. We can then use the rules provided above to assign blame to either the RNN or to beam search in each case: Through this process, you can carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model . For every example in your dev sets where the algorithm gives a much worse output than the human translation, you can try to ascribe the error to either the search algorithm or to the objective function, or to the RNN model that generates the objective function that beam search is supposed to be maximizing. If you find that beam search is responsible for a lot of errors, then maybe it is worth increasing the beam width. Whereas in contrast, if you find that the RNN model is at fault, you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else. Sequence models & Attention mechanism: Attention Model Intuition For most of this week, we have been looking at seq2seq models. A simple modification to this architecture makes it much more powerful, lets take a look. The problem of long sequences Given a very long French sentence, the encoder in a seq2seq network must read in the whole sentence and then, essentially memorize it by storing its learned representation the activations values. Then the decoder network will then generate the English translation. x: \\text{\"Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too.\"} x: \\text{\"Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too.\"} Example output (translated sentence) Now, the way a human translator would translate this sentence is not by reading and memorizing the entire sentence before beginning translation. Instead, the human translator would read the sentence bit-by-bit, translating words as they go, and paying special attention to certain parts of the input sentence when deciding what the ouput sentence should be. For the seq2seq architecture we introduced earlier, we finde that it works quite well for short sentences, but for very long sentences (maybe longer than 30 or 40 words) performance drops. Short sentences are just hard to translate in general due to the lack of context. For long sentences, the vanilla seq2seq model doesn't do well because it's difficult to for the network to memorize a very long sentence. Blue line: seq2seq architectures without attention, Green line: seq2seq architectures with attention. In this and the next lecture, you'll see the Attention Model which translates maybe a bit more like humans might, by looking at part of the sentence at a time. With an Attention model, machine translation systems performance stabilizes across sentence lengths. This is mostly due to the fact that, without attention, we are inadvertantly measuring the ability of a neural network to memorize a long sentence which isn't what is most important. Attention model intuition Lets build our intuition for the attention model with a simple example: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} Attention was first introduced here . Note that attention is typically more useful for longer sequences, but for the purposes of illustration we will use a rather short one. Say that we use a bi-directional RNN to compute some sort of rich feature set for a given input. Lets introduce a new notation: \\alpha^{<i, j>} \\alpha^{<i, j>} can be thought of as an attention measure, e.g., when we are looking to produce the translated word i i in the output sequence, how much attention should we pay to the word j j in the input sequence? Broadly speaking, our decoder takes these measures of attention, along with the output from the encoder network to compute a new input that it uses when determining its outputs. More specifically, \\alpha^{<i, j>} \\alpha^{<i, j>} is influenced by the forward and backward activations of the encoder network at timestep i i and the output from the previous timestep of the decoder network, S^{<t-1>} S^{<t-1>} . Note, we have denoted the activations of the decoder network as S^{<t>} S^{<t>} to avoid confusion. The key intuition is that the decoder network marches away, generating an output at every timestep. The attention weights help the decoder determine what information to pay attention to in the encoders output, as opposed to using all information in the learned representation from the entire input sequence. Sequence models & Attention mechanism: Attention Model In the last lecture, we saw how the attention model allows a neural network to pay attention to only part of an input sentence while it's generating a translation, much like a human translator might. Let's now formalize that intuition, and see how we might implement this attention model ourselves. Attention model Similar to the last lecture, lets assume we have a bi-directional RNN (be it LSTM or GRU) that we use to compute features for the input sequence. To simplify the notation, at every timestep we are going to denote the activations for both the forward and backward recurrent networks as a^{<t>} a^{<t>} , such that: a^{<t'>} = (\\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>}) a^{<t'>} = (\\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>}) We also have a second RNN (in the forward direction only) with states S^{<t>} S^{<t>} which takes as input some context c c , where c c depends on the attention parameters ( \\alpha^{<1,1>}, \\alpha^{<1,2>}, etc) \\alpha^{<1,1>}, \\alpha^{<1,2>}, etc) . These \\alpha \\alpha parameters tell us how much the context c c should depend on the learned features across the timesteps, which is a weighted sum. More formally: \\sum_{t'} \\alpha^{<1, t'>} = 1 \\sum_{t'} \\alpha^{<1, t'>} = 1 c^{<1>} = \\sum_{t'} \\alpha^{<1, t'>}a^{<t'>} c^{<1>} = \\sum_{t'} \\alpha^{<1, t'>}a^{<t'>} In plain english, \\alpha^{<t, t'>} \\alpha^{<t, t'>} is the amount of \"attention\" that y^{<t>} y^{<t>} should pay to a^{<t'>} a^{<t'>} . We compute the context for the second timestep, c^{<2>} c^{<2>} in a similar manner: c^{<2>} = \\sum_{t'} \\alpha^{<2, t'>}a^{<t'>} c^{<2>} = \\sum_{t'} \\alpha^{<2, t'>}a^{<t'>} And so forth and so on. The only thing left to define is how we actually compute the attention weights, \\alpha^{<t, t'>} \\alpha^{<t, t'>} Computing attentions \\alpha^{<t, t'>} \\alpha^{<t, t'>} Let us first present the formula and then explain it: \\alpha^{<t, t'>} = \\frac{exp(e^{<t, t'>})}{\\sum_{t'=1}^{T_x}exp(e^{<t, t'>})} \\alpha^{<t, t'>} = \\frac{exp(e^{<t, t'>})}{\\sum_{t'=1}^{T_x}exp(e^{<t, t'>})} It is helpful to keep in mind that \\alpha^{<t, t'>} \\alpha^{<t, t'>} is the amount of \"attention\" that y^{<t>} y^{<t>} should pay to a^{<t'>} a^{<t'>} We first compute e^{<t, t'>} e^{<t, t'>} and then use what is essentially a softmax over all t' t' to guarantee that \\alpha^{<t, t'>} \\alpha^{<t, t'>} sums to zero over all t' t' . But how do we compute e^{<t, t'>} e^{<t, t'>} ? Typically we use a small neural network that looks something like the following: The intuition is, if you want to decide how much attention to pay to the activation of t' t' , we should depend on the hidden state activation from the previous time step, S^{<t-1>} S^{<t-1>} and the learned features of the word at t' t' , a^{<t'>} a^{<t'>} . What we don't know is the exact function which takes in ( S^{<t-1>}, a^{<t'>} S^{<t-1>}, a^{<t'>} ) and maps it to e^{<t, t'>} e^{<t, t'>} . Therefore, we use a small neural network to learn this function for us. The network MUST be small as we need to use it to make a prediction at every timestep t t . Further reading: Neural machine translation by jointly learning to align and translate and Show, attend and tell: Neural image caption generation with visual attention . Speech recognition - Audio data: Speech recognition One of the most exciting developments with sequence-to-sequence models has been the rise of very accurate speech recognition. We will spend the last two lectures building a sense of how these sequence-to-sequence models are applied to audio data, such as the speech. Speech recognition problem In speech recognition, produce a transcript y y given an audio clip, x x . The audio transcript is generated from a microphone (i.e., someone speaks into a microphone, speech, like all sound, is a pressure wave which the microphone pics up on and generates a spectrum). In truth, even the human brain doesn't process raw sound . We typically perform a series of pre-processing steps, eventually generating a spectrogram, a 3D representation of sound (x: time, y: frequency, z: amplitude) The human ear performs something similar to this preprocessing. Traditionally, speech recognition was solved by first learning to classify phonemes (typically we expensive feature engineering), the individual units of speech. However, with more powerful end-to-end architectures we are finding that explicit phoneme recognition is no longer necessary, or even a good idea. With these arcitectures, a large amount of data is especially important. A large academic dataset may be around 3000h of speech, while commercial systems (e.g. Alex) may be trained on datasets containing as many as 100,000h of speech. Attention model for speech recognition One way to build a speech recognition system is to use the seq2seq model with attention that we explored previously, where the audio clip (divided into small timeframes) is the input and the audio transcript is the output CTC cost for speech recognition Lets say our audio clip is someone saying the sentence \"the quick brown fox\". Typically in speech recognition, the length of our input sequence is much much larger than the length of our output sequence. For example, a 10 second, 100 Hz audio clip becomes a 1000 inputs. Connectionist temporal classification (CTC) allows us use a bi-directional RNN where T_x == T_y T_x == T_y , by allowing the model to predict both repeated characters and \"blanks\". For our examples, the predicted transcript might look like: \\text{ttt _ h _ eee _ _ _ _ <SPACE> _ _ _ qqq _ _ ...} \\text{ttt _ h _ eee _ _ _ _ <SPACE> _ _ _ qqq _ _ ...} Where _ denotes a \"blank\". The basic rule is to collapse repeated characters not separated by a \"blank\" . This idea was originally introduced here Summary In this lecture we tried to get a rough sense of how speech recognition models work. We saw: two methods for building a speech recognition system (both involved bi-directional RNNs): a seq2seq model with attention and a CTC model . that deep learning has had a dramatic impact of the viability of commercial speech recognition systems. building effective speech recognition system is still requires a very significant effort and a very large data set. Speech recognition - Audio data: Trigger word detection Trigger word detection systems are used to identify wake (or trigger) words (e.g. \"Hey Google\", \"Alexa\"), typically for digital assistants like Alexa, Google Home and Siri. The literature on trigger word detection is still evolving, and there is not widespread consensus or a universally agreed upon algorithm for trigger detection. We are just going to look at one example. Our task is to take an audio clip, possibly perform preprocessing to compute a spectrogram and then the features that we will eventually pass to an RNN, and predict at which timesteps the wake word was uttered. One strategy, is to have the neural network output the label 0 for all timesteps before a wake word is mentioned and then 1 directly after it is mentioned. The slight problem with this is that our training set becomes very unbalanced (many more 0's than 1's). Instead, we typically have the model predict a few 1's for the timesteps that come directly after the wake word was mentioned.","title":"Week 3"},{"location":"sequence_models/week_3/#week-3-sequence-models-attention-mechanism","text":"In this series, we will look primarily at sequence models, which are useful for everything from machine translation to speech recognition. We will also look at attention models.","title":"Week 3: Sequence models &amp; Attention mechanism"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-basic-models","text":"","title":"Sequence models &amp; Attention mechanism: Basic Models"},{"location":"sequence_models/week_3/#machine-translation","text":"Lets say we want to translate from french to english . We will use the following examples: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} y: \\text{\"Jane is visiting Africa in September\"} y: \\text{\"Jane is visiting Africa in September\"} Recall that we represent individual elements in the input sequence as x^{<t>} x^{<t>} , and in the output sequence as y^{<t>} y^{<t>} . Most of the ideas presented in this lecture are from this paper and this paper . First, we build our encoder , an RNN (LSTM or GRU) which processes the input sequence. The encoder outputs a vector that represents the learned representation of the input sequence. A decoder takes this output sequence, and outputs the translation one word at a time. Note that the outputs from each timestep in the decoder network are actually passed as input for the next timestep in the case of language modelling. Given enough data, this sequence to sequence architecture actually works quite well.","title":"Machine translation"},{"location":"sequence_models/week_3/#image-captioning","text":"The task of image captioning involves inputing an image, and having the network generate a natural language caption. x: \\text{an image of a cat of a chair} x: \\text{an image of a cat of a chair} y: \\text{\"A cat sitting on a chair\"} y: \\text{\"A cat sitting on a chair\"} Typically, we use a CNN as our encoder , without any classification layer at the end. We feed the learned representation to a decoder , an RNN, which generates and output sequence which is our image caption.","title":"Image captioning"},{"location":"sequence_models/week_3/#summary","text":"Simple sequence to sequence (seq2seq) models are comprised of an encoder and decoder , which themselves are neural networks (typically recurrent or convolutional). Seq2seq architectures are able to preform reasonably well when given enough data. With certain modifications (attention) and additions (beam search), they are able to achieve state-of-the-art performance on tasks like machine translation and image captioning . These architecture difference slightly to some of the sequence generation architectures we saw previously however, and we will explore the difference in the next lecture.","title":"Summary"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-picking-the-most-likely-sentence","text":"While there are some similarities between the sequence to sequence machine translation model and the language models that you have worked within the first week of this course, there are some significant differences as well.","title":"Sequence models &amp; Attention mechanism: Picking the most likely sentence"},{"location":"sequence_models/week_3/#machine-translation-as-a-conditional-language-model","text":"We can think of machine translation as building a conditional language model . First, recall the language model we worked with previously: The machine translation model looks like: Notice that the decoder network looks pretty much identical to the language model. However, instead of the initial hidden state being a^{<t>} = 0 a^{<t>} = 0 , the initial hidden state is initialized with the learned representation from the encoder. That is why we can think of this as a conditional language model . Instead of modeling the probability of any sentence, it is now modeling the probability of say, the output English translation, conditioned on some input French sentence.","title":"Machine translation as a conditional language model"},{"location":"sequence_models/week_3/#finding-the-most-likely-translation","text":"So how does this work in practice? Our model produces the following: P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) The probability of different corresponding translated sentences based on the input sentence, x x . Unlike the language model we saw earlier, we do not want to sample words randomly . Instead, we want to choose the sentence y y that maximizes P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) , argmax_y P(y^{<1>}, ..., y^{<T_y>} | x) argmax_y P(y^{<1>}, ..., y^{<T_y>} | x) The most common algorithm for solving this problem is called the beam search , which we will look at in the next lecture. Before we look at beam search, you might be wondering, why not use a greedy search ? In our case, this would involve choosing the most likely output, \\hat y^{<t>} \\hat y^{<t>} at each timestep, one timestep at a time, starting from t=1 t=1 . In simple terms, this approach does not maximize the joint probability P(y^{<1>}, ..., y^{<T_y>} | x) P(y^{<1>}, ..., y^{<T_y>} | x) , which is what we are really after. To see why this so, take our input example: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} And our outputs generated by beam search and greedy search respectively: beam: \\text{\"Jane is visiting Africa in September\"} beam: \\text{\"Jane is visiting Africa in September\"} greedy: \\text{\"Jane is going to be visiting Africa in September\"} greedy: \\text{\"Jane is going to be visiting Africa in September\"} In greedy search, we are optimizing the output at each timestep without regard for the overall sequence. As such, we will often produce less succicent, and more verbose sentences. In this case, p(\\text{\"Jane is\"} | \\text{\"going\"}) p(\\text{\"Jane is\"} | \\text{\"going\"}) > p(\\text{\"Jane is\"} | \\text{\"visiting\"}) p(\\text{\"Jane is\"} | \\text{\"visiting\"}) , which is why we produce the more verbose \" is going to be visiting \" as opposed to \" is visiting \". While this is a contrived example, it captures a larger phenomena. When generating sequences, it is often a good idea to maximize the joint probability across the entire sequence. Otherwise, we end up with sub-optimal performance tasks in which there is strong dependencies between elements of a sequence. Another important point, is that the space of all possible output sentences is huge. For a ten-word sentence drawn and a vocabulary of 10,000 words, we have 10,000^{10} 10,000^{10} possible sentences. For this reason, we need an approximate search algorithm (a heuristic ). While this won't always succeed in finding the sentence \\hat y \\hat y that maximizes that conditional probability, it will typically do a \"good enough\" job without needed to enumerating all possible sentences.","title":"Finding the most likely translation"},{"location":"sequence_models/week_3/#summary_1","text":"In this lecture, you saw how machine translation can be posed as a conditional language modeling problem . One major difference between this and the earlier language modeling problems is rather than generating a sentence at random, you try to find the most likely translation. The set of all sentences of a certain length is too large to exhaustively enumerate. So, we have to resort to an approximate search algorithm.","title":"Summary"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-beam-search","text":"Recall that, for a machine translation system given an input French sentence, you don't want to output a random English translation, you want to output the best and the most likely English translation. The same is also true for speech recognition where, given an input audio clip, you don't want to output a random text transcript of that audio, you want to output the best , maybe the most likely , text transcript. Beam search is the most widely used algorithm to do this.","title":"Sequence models &amp; Attention mechanism: Beam Search"},{"location":"sequence_models/week_3/#beam-search-algorithm","text":"Step 1 The first thing beam search needs to do is pick the first word, \\hat y^{<1>} \\hat y^{<1>} of the translation given our input sequence x x , P(\\hat y^{<1>} | x) P(\\hat y^{<1>} | x) . Contrary to greedy search however, we can evaluate multiple choices at the same time. The number of choices is designated by a parameter of the algorithm, the beam width B B . Lets say for our purposes that we choose B = 3 B = 3 . In practice, we run our sentence to be translated through the encoder-decoder network and store in memory the three most likely, first words of the translated sentence. Step 2 For each of these three choices, we consider what should be the second word in the translated output sequence, \\hat y^{<2>} \\hat y^{<2>} . Recall that the previously selected output, \\hat y^{<1>} \\hat y^{<1>} is fed as input to the next timestep in the decoder network. In practice, we are looking to compute: P(\\hat y^{<1>},\\; \\hat y^{<2>} | x) = P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<1>},\\; \\hat y^{<2>} | x) = P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) Where P(\\hat y^{<1>}| x) P(\\hat y^{<1>}| x) is computed by the decoder network in the first step and P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) computed by the decoder network in the second step. In total, we evaluate B \\cdot |V| B \\cdot |V| possible choices for the second word, but we only save the top B B most likely choices for \\hat y^{<1>},\\; \\hat y^{<2>} \\hat y^{<1>},\\; \\hat y^{<2>} . We may actually end up dropping one or more possible choices we previously made for \\hat y^{<1>} \\hat y^{<1>} . An implementation detail: we instantiate B B copies of the seq2seq model to compute P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) P(\\hat y^{<1>}| x) P(\\hat y^{<2>} | x, \\; \\hat y^{<1>}) , one for each choice of \\hat y^{<1>} \\hat y^{<1>} . Step n We continue this process, at each step n n computing P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) using the chain rule for conditional probabilities: P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) = P(\\hat y^{<n>} | \\hat y^{<1>}, ... , \\hat y^{<n-1>}, x) ... P(\\hat y^{<1>} | x) P(\\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n>}| x) = P(\\hat y^{<n>} | \\hat y^{<1>}, ... , \\hat y^{<n-1>}, x) ... P(\\hat y^{<1>} | x) We store the top B B most likely sequences \\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n-1>} \\hat y^{<1>},\\hat y^{<2>}, ... \\hat y^{<n-1>} and their corresponsding seq2seq model to use in the next step, where we again evaluate |V| |V| possible words for our B B number of previous choices. The outcome of this process, hopefully, is a prediction which is terminated by the special token. Notice that, when B = 1 B = 1 , this process essentially becomes the greedy search algorithm seen previously.","title":"Beam search algorithm"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-refinements-to-beam-search","text":"In the last lecture, we saw the basic beam search algorithm. In this lecture, we will look at some little changes that make it work even better.","title":"Sequence models &amp; Attention mechanism: Refinements to Beam Search"},{"location":"sequence_models/week_3/#length-normalization","text":"Recall that in beam search, we are trying to compute the following: argmax_y \\prod_{t=1}^{T_y}P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>}) argmax_y \\prod_{t=1}^{T_y}P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>}) The product term is capturing the idea that P(y^{<1>}, ..., y^{<T_y>} | x) = P(y^{<1>} | x) ... P(y^{<T_y>} | x, y^{<1>}, ..., y^{<T_y-1>}) P(y^{<1>}, ..., y^{<T_y>} | x) = P(y^{<1>} | x) ... P(y^{<T_y>} | x, y^{<1>}, ..., y^{<T_y-1>}) , which itself is just the chain rule for conditional probabilities. The problem here, is that many many small probabilities can lead to numerical underflow . For this reason, we take the log of the product (which becomes the sum of the log of the individual elements): argmax_y \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) This leads to a more numerically stable algorithm, which reduces the chance of rounding errors. Because \\log P(y|x) \\log P(y|x) and P(y|x) P(y|x) are monotonically increasing functions, the value that maximizes one also maximizes the other. Notice as well that the optimization objective actually favors short sentences (this is true of both formulations listed above). In machine translation, for example, This has the negative consequence of promoting short translated sentences over long translated sentences even when the longer sentence is preferable. A solution to this problem is to normalize our objective objective with respect to output length: argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) In practice, we typically introduce a new parameter \\alpha \\alpha which softens the length normalization, to retain a penalty on very very long output sequences.","title":"Length normalization"},{"location":"sequence_models/week_3/#beam-search-recap","text":"So, in full: As you run beam search you see a lot of sentences with length equal 1, 2, 3, and so on. Maybe you run beam search for 30 steps and you consider output sentences up to length 30, let's say. With a beam width of 3, you will be keeping track of the top three possibilities for each of these possible sentence lengths, 1, 2, 3, 4 and so on, up to 30. Then, you would look at all of the output sentences and score them against our objective: argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) argmax_y \\frac{1}{T_y^{\\alpha}} \\sum_{t=1}^{T_y} \\log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1}) Finally, of all of these sentences that we encounter through beam search, we pick the one that achieves the highest value on this normalized log probability objective (sometimes called a normalized log likelihood objective ). This brings us to the final translation, your outputs.","title":"Beam search recap"},{"location":"sequence_models/week_3/#beam-width-discussion","text":"Finally, an implementational detail. How do you choose the beam width B B ? The larger B B is, the more possibilities you're considering, and thus the better the sentence you will probably find. But the larger B B is, the more computationally expensive your algorithm is, because you're also keeping a lot more possibilities around. So here are the pros and cons of setting B B to be very large versus very small: If the beam width is very large, then you consider a lot of possibilities, and so you tend to get a better result, but it will be slower, the memory requirements will also grow, and it will also be computationally slower. If you use a small beam width, then you are likely to get a worse result because you're keeping less possibilities in mind as the algorithm is running. But you get a result faster and the memory requirements will be lower. In the previous video, we used in our running example a beam width of three. In practice, that is on the small side. In production systems, it's not uncommon to see a beam width maybe around 10 -- 100 would be considered very large for a production system. For research systems, where people want to squeeze out every last drop of performance in order to publish a paper, tt's not uncommon to see people use beam widths of 1,000 to 3,000. In truth, you just need to try a variety of values of B B as you work through your application. Beware that as B B gets very large, there is often diminishing returns. Expect to see a huge gain as you go from a beam width of 1, which is essentially greedy search, to 3, to maybe 10, but gains may diminish from there on out.","title":"Beam width discussion"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-error-analysis-on-beam-search","text":"In the third course of this sequence of five courses, we saw how error analysis can help you focus your time on doing the most useful work for your project. Because beam search is an approximate search algorithm, also called a heuristic , it doesn't always output the most likely sentence. So what if beam search makes a mistake? In this lecture, you'll learn how error analysis interacts with beam search and how you can figure out whether it is the beam search algorithm that's causing problems and worth spending time on, or whether it might be your RNN model that is causing problems and worth spending time on. Lets use the following, simple example to see how error analysis works with beam search: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} Assume further that our human-provided ( y*) y*) and machine provided ( \\hat y \\hat y ) translations for this sentence are: y*: \\text{\"Jane is visiting Africa in September\"} y*: \\text{\"Jane is visiting Africa in September\"} \\hat y: \\text{\"Jane visited Africa last September\"} \\hat y: \\text{\"Jane visited Africa last September\"} Our model has to two main components: our encoder-decoder architecture (which we will simply refer to as our 'RNN' from here on out) and the beam search algorithm applied to the outputs of this RNN. It would be extremely helpful if we could assign blame to one of these components individually when we get a bad translation, like our example \\hat y \\hat y . Similar to how it may be tempting to collect more training data when our models underperform (it 'never hurts'!) it is also tempting in this case to increase the beam width (again, it never seems to hurt!). Error analysis is a more principled way to improve our model, by helping us focus our attention on what is currently hurting performance the most.","title":"Sequence models &amp; Attention mechanism: Error analysis on beam search"},{"location":"sequence_models/week_3/#error-analysis-applied-to-machine-translation","text":"Recall that the RNN computes P(y | x) P(y | x) . The first step in our analysis is to compute P(y* | x) P(y* | x) and P(\\hat y | x) P(\\hat y | x) . We then ask, which probability is larger? If","title":"Error analysis applied to machine translation"},{"location":"sequence_models/week_3/#case-1-py-xpy-x-phat-y-xphat-y-x","text":"Beam search chose \\hat y \\hat y (recall, its optimization objective is \\hat y = argmax_y P(y|x) \\hat y = argmax_y P(y|x) ), but y* y* is more likely according to your model. Conclusion : We conclude that beam search is at fault. It faild to find a value for y y which maximizes P(y|x) P(y|x) .","title":"Case 1: P(y* | x)P(y* | x) &gt; P(\\hat y | x)P(\\hat y | x)"},{"location":"sequence_models/week_3/#case-2-py-x-le-phat-y-xpy-x-le-phat-y-x","text":"y* y* is a better translation than \\hat y \\hat y . But our RNN predicted P(y* | x) \\le P(\\hat y | x) P(y* | x) \\le P(\\hat y | x) Conclusion : We conclude that RNN model is at fault. It failed to model a better translation as being more likely than a worse translation for a given input sentence. Note, we are ignoring length normalization here for simplicity. In reality, you would use the entire optimization objective with length normalization instead of just P(y|x) P(y|x) in your error analysis. In practice, we might collect some examples of our gold translations (provided by a human) and compare them in a table to the corresponding machine-provided translations, tallying P(y* | x) P(y* | x) and P(\\hat y | x) P(\\hat y | x) for each example. We can then use the rules provided above to assign blame to either the RNN or to beam search in each case: Through this process, you can carry out error analysis to figure out what fraction of errors are due to beam search versus the RNN model . For every example in your dev sets where the algorithm gives a much worse output than the human translation, you can try to ascribe the error to either the search algorithm or to the objective function, or to the RNN model that generates the objective function that beam search is supposed to be maximizing. If you find that beam search is responsible for a lot of errors, then maybe it is worth increasing the beam width. Whereas in contrast, if you find that the RNN model is at fault, you could do a deeper layer of analysis to try to figure out if you want to add regularization, or get more training data, or try a different network architecture, or something else.","title":"Case 2: P(y* | x) \\le P(\\hat y | x)P(y* | x) \\le P(\\hat y | x)"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-attention-model-intuition","text":"For most of this week, we have been looking at seq2seq models. A simple modification to this architecture makes it much more powerful, lets take a look.","title":"Sequence models &amp; Attention mechanism: Attention Model Intuition"},{"location":"sequence_models/week_3/#the-problem-of-long-sequences","text":"Given a very long French sentence, the encoder in a seq2seq network must read in the whole sentence and then, essentially memorize it by storing its learned representation the activations values. Then the decoder network will then generate the English translation. x: \\text{\"Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too.\"} x: \\text{\"Jane went to Africa last September and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too.\"} Example output (translated sentence) Now, the way a human translator would translate this sentence is not by reading and memorizing the entire sentence before beginning translation. Instead, the human translator would read the sentence bit-by-bit, translating words as they go, and paying special attention to certain parts of the input sentence when deciding what the ouput sentence should be. For the seq2seq architecture we introduced earlier, we finde that it works quite well for short sentences, but for very long sentences (maybe longer than 30 or 40 words) performance drops. Short sentences are just hard to translate in general due to the lack of context. For long sentences, the vanilla seq2seq model doesn't do well because it's difficult to for the network to memorize a very long sentence. Blue line: seq2seq architectures without attention, Green line: seq2seq architectures with attention. In this and the next lecture, you'll see the Attention Model which translates maybe a bit more like humans might, by looking at part of the sentence at a time. With an Attention model, machine translation systems performance stabilizes across sentence lengths. This is mostly due to the fact that, without attention, we are inadvertantly measuring the ability of a neural network to memorize a long sentence which isn't what is most important.","title":"The problem of long sequences"},{"location":"sequence_models/week_3/#attention-model-intuition","text":"Lets build our intuition for the attention model with a simple example: x: \\text{\"Jane visite l'Afrique en septembre\"} x: \\text{\"Jane visite l'Afrique en septembre\"} Attention was first introduced here . Note that attention is typically more useful for longer sequences, but for the purposes of illustration we will use a rather short one. Say that we use a bi-directional RNN to compute some sort of rich feature set for a given input. Lets introduce a new notation: \\alpha^{<i, j>} \\alpha^{<i, j>} can be thought of as an attention measure, e.g., when we are looking to produce the translated word i i in the output sequence, how much attention should we pay to the word j j in the input sequence? Broadly speaking, our decoder takes these measures of attention, along with the output from the encoder network to compute a new input that it uses when determining its outputs. More specifically, \\alpha^{<i, j>} \\alpha^{<i, j>} is influenced by the forward and backward activations of the encoder network at timestep i i and the output from the previous timestep of the decoder network, S^{<t-1>} S^{<t-1>} . Note, we have denoted the activations of the decoder network as S^{<t>} S^{<t>} to avoid confusion. The key intuition is that the decoder network marches away, generating an output at every timestep. The attention weights help the decoder determine what information to pay attention to in the encoders output, as opposed to using all information in the learned representation from the entire input sequence.","title":"Attention model intuition"},{"location":"sequence_models/week_3/#sequence-models-attention-mechanism-attention-model","text":"In the last lecture, we saw how the attention model allows a neural network to pay attention to only part of an input sentence while it's generating a translation, much like a human translator might. Let's now formalize that intuition, and see how we might implement this attention model ourselves.","title":"Sequence models &amp; Attention mechanism: Attention Model"},{"location":"sequence_models/week_3/#attention-model","text":"Similar to the last lecture, lets assume we have a bi-directional RNN (be it LSTM or GRU) that we use to compute features for the input sequence. To simplify the notation, at every timestep we are going to denote the activations for both the forward and backward recurrent networks as a^{<t>} a^{<t>} , such that: a^{<t'>} = (\\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>}) a^{<t'>} = (\\overrightarrow{a}^{<t'>}, \\overleftarrow{a}^{<t'>}) We also have a second RNN (in the forward direction only) with states S^{<t>} S^{<t>} which takes as input some context c c , where c c depends on the attention parameters ( \\alpha^{<1,1>}, \\alpha^{<1,2>}, etc) \\alpha^{<1,1>}, \\alpha^{<1,2>}, etc) . These \\alpha \\alpha parameters tell us how much the context c c should depend on the learned features across the timesteps, which is a weighted sum. More formally: \\sum_{t'} \\alpha^{<1, t'>} = 1 \\sum_{t'} \\alpha^{<1, t'>} = 1 c^{<1>} = \\sum_{t'} \\alpha^{<1, t'>}a^{<t'>} c^{<1>} = \\sum_{t'} \\alpha^{<1, t'>}a^{<t'>} In plain english, \\alpha^{<t, t'>} \\alpha^{<t, t'>} is the amount of \"attention\" that y^{<t>} y^{<t>} should pay to a^{<t'>} a^{<t'>} . We compute the context for the second timestep, c^{<2>} c^{<2>} in a similar manner: c^{<2>} = \\sum_{t'} \\alpha^{<2, t'>}a^{<t'>} c^{<2>} = \\sum_{t'} \\alpha^{<2, t'>}a^{<t'>} And so forth and so on. The only thing left to define is how we actually compute the attention weights, \\alpha^{<t, t'>} \\alpha^{<t, t'>}","title":"Attention model"},{"location":"sequence_models/week_3/#computing-attentions-alphat-talphat-t","text":"Let us first present the formula and then explain it: \\alpha^{<t, t'>} = \\frac{exp(e^{<t, t'>})}{\\sum_{t'=1}^{T_x}exp(e^{<t, t'>})} \\alpha^{<t, t'>} = \\frac{exp(e^{<t, t'>})}{\\sum_{t'=1}^{T_x}exp(e^{<t, t'>})} It is helpful to keep in mind that \\alpha^{<t, t'>} \\alpha^{<t, t'>} is the amount of \"attention\" that y^{<t>} y^{<t>} should pay to a^{<t'>} a^{<t'>} We first compute e^{<t, t'>} e^{<t, t'>} and then use what is essentially a softmax over all t' t' to guarantee that \\alpha^{<t, t'>} \\alpha^{<t, t'>} sums to zero over all t' t' . But how do we compute e^{<t, t'>} e^{<t, t'>} ? Typically we use a small neural network that looks something like the following: The intuition is, if you want to decide how much attention to pay to the activation of t' t' , we should depend on the hidden state activation from the previous time step, S^{<t-1>} S^{<t-1>} and the learned features of the word at t' t' , a^{<t'>} a^{<t'>} . What we don't know is the exact function which takes in ( S^{<t-1>}, a^{<t'>} S^{<t-1>}, a^{<t'>} ) and maps it to e^{<t, t'>} e^{<t, t'>} . Therefore, we use a small neural network to learn this function for us. The network MUST be small as we need to use it to make a prediction at every timestep t t . Further reading: Neural machine translation by jointly learning to align and translate and Show, attend and tell: Neural image caption generation with visual attention .","title":"Computing attentions \\alpha^{&lt;t, t'&gt;}\\alpha^{&lt;t, t'&gt;}"},{"location":"sequence_models/week_3/#speech-recognition-audio-data-speech-recognition","text":"One of the most exciting developments with sequence-to-sequence models has been the rise of very accurate speech recognition. We will spend the last two lectures building a sense of how these sequence-to-sequence models are applied to audio data, such as the speech.","title":"Speech recognition - Audio data: Speech recognition"},{"location":"sequence_models/week_3/#speech-recognition-problem","text":"In speech recognition, produce a transcript y y given an audio clip, x x . The audio transcript is generated from a microphone (i.e., someone speaks into a microphone, speech, like all sound, is a pressure wave which the microphone pics up on and generates a spectrum). In truth, even the human brain doesn't process raw sound . We typically perform a series of pre-processing steps, eventually generating a spectrogram, a 3D representation of sound (x: time, y: frequency, z: amplitude) The human ear performs something similar to this preprocessing. Traditionally, speech recognition was solved by first learning to classify phonemes (typically we expensive feature engineering), the individual units of speech. However, with more powerful end-to-end architectures we are finding that explicit phoneme recognition is no longer necessary, or even a good idea. With these arcitectures, a large amount of data is especially important. A large academic dataset may be around 3000h of speech, while commercial systems (e.g. Alex) may be trained on datasets containing as many as 100,000h of speech.","title":"Speech recognition problem"},{"location":"sequence_models/week_3/#attention-model-for-speech-recognition","text":"One way to build a speech recognition system is to use the seq2seq model with attention that we explored previously, where the audio clip (divided into small timeframes) is the input and the audio transcript is the output","title":"Attention model for speech recognition"},{"location":"sequence_models/week_3/#ctc-cost-for-speech-recognition","text":"Lets say our audio clip is someone saying the sentence \"the quick brown fox\". Typically in speech recognition, the length of our input sequence is much much larger than the length of our output sequence. For example, a 10 second, 100 Hz audio clip becomes a 1000 inputs. Connectionist temporal classification (CTC) allows us use a bi-directional RNN where T_x == T_y T_x == T_y , by allowing the model to predict both repeated characters and \"blanks\". For our examples, the predicted transcript might look like: \\text{ttt _ h _ eee _ _ _ _ <SPACE> _ _ _ qqq _ _ ...} \\text{ttt _ h _ eee _ _ _ _ <SPACE> _ _ _ qqq _ _ ...} Where _ denotes a \"blank\". The basic rule is to collapse repeated characters not separated by a \"blank\" . This idea was originally introduced here","title":"CTC cost for speech recognition"},{"location":"sequence_models/week_3/#summary_2","text":"In this lecture we tried to get a rough sense of how speech recognition models work. We saw: two methods for building a speech recognition system (both involved bi-directional RNNs): a seq2seq model with attention and a CTC model . that deep learning has had a dramatic impact of the viability of commercial speech recognition systems. building effective speech recognition system is still requires a very significant effort and a very large data set.","title":"Summary"},{"location":"sequence_models/week_3/#speech-recognition-audio-data-trigger-word-detection","text":"Trigger word detection systems are used to identify wake (or trigger) words (e.g. \"Hey Google\", \"Alexa\"), typically for digital assistants like Alexa, Google Home and Siri. The literature on trigger word detection is still evolving, and there is not widespread consensus or a universally agreed upon algorithm for trigger detection. We are just going to look at one example. Our task is to take an audio clip, possibly perform preprocessing to compute a spectrogram and then the features that we will eventually pass to an RNN, and predict at which timesteps the wake word was uttered. One strategy, is to have the neural network output the label 0 for all timesteps before a wake word is mentioned and then 1 directly after it is mentioned. The slight problem with this is that our training set becomes very unbalanced (many more 0's than 1's). Instead, we typically have the model predict a few 1's for the timesteps that come directly after the wake word was mentioned.","title":"Speech recognition - Audio data: Trigger word detection"},{"location":"structuring_machine_learning_projects/week_1/","text":"Week 1: ML Strategy (1) What is machine learning strategy? Lets start with a motivating example. Introduction to ML strategy Why ML strategy Lets say you are working on a cat classifier . You have achieved 90% accuracy, but would like to improve performance even further. Your ideas for achieveing this are: collect more data collect more diverse training set train the algorithm longer with gradient descent try adam (or other optimizers) instead of gradient descent try dropout, add L2 regularization, change network architecture, ... This list is long, and so it becomes incredibly important to be able to identify ideas that are worth our time, and which ones we can likely discard. This course will attempt to introduce a framework for making these decisions. In particular, we will focus on the organization of deep learning-based projects . Orthogonalization One of the challenges with building deep learning systems is the number of things we can tune to improve performance ( many hyperparameters notwithstanding ). Take the example of an old TV. They included many nobs for tuning the display position (x-axis position, y-axis position, rotation, etc...). Orthogonalization in this example refers to the TV designers decision to ensure each nob had one effect on the display and that these effects were relative to one another. If these nobs did more than one action and each actions magnitude was not relative to the other, it would become nearly impossible to tune the TV. Take another example, driving a car . Imagine if there was multiple joysticks. One joystick modified \\(0.3\\) X steering angle \\(- 0.8\\) speed, and another \\(2\\) X steering angle \\(+ 0.9\\) speed. In theory, by tuning these two nobs we could drive the car, but this would be much more difficult then separating the inputs into distinct input mechanisms . Orthogonal refers to the idea that the inputs are aligned to the dimensions we want to control. How does this related to machine learning? Chain of assumption in examples For a machine learning system to perform \"well\", we usually aim to make four things happen: Fit training set well on cost function (for some applications, this means comparing favorably to human-level performance). Fit dev set well on cost function Fit test set well on cost function Performs well in real world. If we relate back to the TV example, we wanted one knob to change each attribute of the display. In the same way, we can modify knobs for each of our four steps above : Train a bigger network, change the optimization algorithm, ... Regularization, bigger training set, ... Bigger dev set, ... Change the dev set or the cost function Note Andrew said when he trains neural networks, he tends not to use early stopping . The reason being is that this is not a very orthogonal \"knob\"; it simultaneously effects how well we fit the training set and the dev set. The whole idea here is that if we keep our \"knobs\" orthogonal , we can more easily come up with solutions to specific problems with our deep neural networks (i.e., if we are getting poor performance on the training set, we may opt to train a bigger [higher variance] network). Setting up your goal Single number evaluation metric When tuning neural networks (modifying hyper-parameters, trying different architectures, etc.) you will find that having a _single __evaluation metric___ will allow you to easily and quickly judge if a certain change improved performance. Note Andrew recommends deciding on a single, real-valued evaluation metric when starting out on your deep learning project. Lets look at an example. As we discussed previously, applied machine learning is a very empirical process. Lets say that we start with classifier A, and end up with classifier B after some change to the model. We could look at precision and recall as a means of improvements. What we really want is to improve both precision and recall. The problem is that it can become difficult to choose the \"best\" classifier if we are monitoring two different performance metrics, especially when we are making many modifications to our network. This is when it becomes important to chose a single performance metric. In this case specifically, we can chose the F1-score , the harmonic mean of the precision and recall (less formally, think of this as an average). We can see very quickly that classifier A has a better F1-score, and therefore we chose classifier A over classifier B. Satisficing and Optimizing metric It is not always easy to combine all the metrics we care about into a single real-numbered value. Lets introduce satisficing and optimizing metrics as a solution to this problem. Lets say we are building a classifier, and we care about both our accuracy (measured as F1-score, traditional accuracy or some other metric) and the running time to classify a new example. One thing we can do, is to combine accuracy and run-time into a single-metric , possibly by taking a weighted linear sum of the two metrics. Note As it turns out, this tends to produce a rather artificial solution (no pun intended). Another way, is to attempt to maximize accuracy while subject to the restraint that \\(\\text{running time} \\le 100\\)ms. In this case, we say that accuracy is an optimizing metric (because we want to maximize or minimize it) and running time is a satisficing metric (because it just needs to meet a certain constraint, i.e., be \"good enough\"). More generally, if we have \\(m\\) metrics that we care about, it is reasonable to choose one to be our optimizing metric , and \\(m-1\\) to be satisficing metrics . Example: Wake words We can take a concrete example to illustrate this: wake words for intelligent voice assistants . We might chose the accuracy of the model (i.e., what percent of the time does it \"wake\" when a wake word is said) to be out optimizing metric s.t. we have \\(\\le 1\\) false-positives per 24 hours of operation (our satisficing metric ). Summary To summarize, if there are multiple things you care about, we can set one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfied. This idea goes hand-in-hand with the idea of having a single real-valued performance metric whereby we can quickly and easily chose the best model given a selection of models. Train/dev/test distributions The way you set up your train, dev (sometimes called valid) and test sets can have a large impact on your development times and even model performance. In this video, we are going to focus on the dev (sometimes called the valid or hold out set) and the test set . The general workflow in machine learning is to train on the train set and test out model performance (e.g., different hyper-parameters or model architectures) on the dev set. Lets look at an example. Say we had data from multiple regions: US UK Other European countries South America India China Other Asian countries Australia If we were to build our dev set by choosing data from the first four regions and our test set from the last four regions, our data would likely be skewed and our model would likely perform poorly (at least on the test set). Why? Imagine the dev set as a target, and our job as machine learning engineers is to hit a bullseye. A dev set that is not representative of the overall general distribution is analogous to moving the bullseye away from its original location moments after we fire our bow . An ML team could spend months optimizing the model on a dev set, only to achieve very poor performance on a test set! So for our data above, a much better idea would be to sample data randomly from all regions to build our dev and test set. Guidelines Choose a dev set and test set (from the same distribution) to reflect data you expect to get in the future and consider important to do well on . Size of the dev and test sets In the last lecture we saw that the dev and test sets should come from the same distributions. But how large should they be? Size of the dev/test sets The rule of thumb in machine learning is typically 60% training , 20% dev , and 20% test (or 70/30 train / test ). In earlier eras of machine learning, this was pretty reasonable. In the modern machine learning era, we are used to working with much larger data set sizes. For example, imagine we have \\(1,000,000\\) examples. It might be totally reasonable for us to use 98% as our test set, 1% for dev and 1% for test . Note Note that 1% of \\(10^6\\) is \\(10^4\\)! Guidelines Set your test set to be big enough to give high confidence in the overall performance of your system. When to change dev/test sets and metrics Sometimes during the course of a machine learning project, you will realize that you want to change your evaluation metric (i.e., move the \"goal posts\"). Lets illustrate this with an example: Example 1 Imagine we have two models for image classification, and we are using classification performance as our evaluation metric: Algorithm A has a 3% error, but sometimes shows users pornographic images. Algorithm B has a 5% error. Cleary, algorithm A performs better by our original evaluation metric (classification performance), but showing users pornographic images is unacceptable . \\[Error = \\frac{1}{m_{dev}}\\sum^{m_{dev}} {i=1} \\ell { y {pred}^{(i)} \\ne y^{(i)} }\\] Note Our error treats all incorrect predictions the same, pornographic or otherwise. We can think of it like this: our evaluation metric prefers algorithm A, but we (and our users) prefer algorithm B. When our evaluation metric is no longer ranking the algorithms in the order we would like, it is a sign that we may want to change our evaluation metric. In our specific example, we could solve this by weighting misclassifications \\[Error = \\frac{1}{w^{(i)}}\\sum^{m_{dev}} {i=1} w^{(i)}\\ell { y {pred}^{(i)} \\ne y^{(i)} }\\] where \\(w^{(i)}\\) is 1 if \\(x^{(i)}\\) is non-porn and 10 (or even 100 or larger) if \\(x^{(i)}\\) is porn. This is actually an example of orthogonalization . We, Define a metric to evaluate our model (\"placing the target\") (In a completely separate step) Worry about how to do well on this metric. Example 2 Take the same example as above, but with a new twist. Say we train our classifier on a data set of high quality images. Then, when we deploy our model we notice it performs poorly. We narrow the problem down to the low quality images users are \"feeding\" to the model. What do we do? In general: if doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set . Comparing to human-level performance In the last few years, comparing machine learning systems to human level performance have become common place. The reasons for this include: Deep learning based approaches are making extraordinary gains in performance, so our baseline needs to be more stringent. Many of the tasks deep learning is performing well at were thought to be very difficult for machines (e.g. NLP, computer vision). Comparing performance on these tasks to a human baseline is natural. It is also instructive to look at the performance of machine learning over time (note this is an obvious abstraction) Roughly speaking, performance (e.g., in a research domain or for a certain task) progresses quickly until we reach human-level performance, and tails off quickly. Why? mainly because human level performance is typically very close to the Bayes optimal error . Bayes optimal error is the best possible error; there is no way for any function mapping from \\(x \\rightarrow y\\) to do any better. A second reason is that so long as ML performs worse than humans for a given task, we can: get labeled data from humans gain insight from manual error analysis (e.g., why did a person get this right?) better analysis of bias/variance Avoidable bias Of course, we want our learning algorithm to perform well on the training set, but not too well . Knowing where human level performance is can help us decide how well we want to perform on the training set. Let us again take the example of an image classifier. For this particular data set, assume: human-level performance is an error of 1%. our classifier is currently achieving 8% classification error on the training set and 10% classification on the dev set. Clearly, it has plenty of room to improve . Specifically, we would want to try to increase variance and reduce bias . Note For the purposes of computer vision, assume that human-level performance \\(\\approx\\) Bayes error. Now, lets take the same example, but instead, we assume that human-level performance is an error of 7.5% (this example is very contrived, as humans are extremely good at image classification). In this case, we note that our classifier performances nearly as well as a human baseline. We would likely want to to decrease variance and increase bias (in order to improve performance on the dev set.) So what did this example show us? When human-level performance (where we are using human-level performance as a proxy for Bayes error) is very high relative to our models performance on the train set, we likely want to focus on reducing \"avoidable\" bias (or increasing variance) in order to improve performance on the training set (e.g., by using a bigger network.) When human-level performance is comparable to our models performance on the train set, we likely want to focus on increasing bias (or decreasing variance) in order to improve performance on the dev set (e.g., by using a regularization technique or gathering more training data.) Understanding human-level performance The term human-level performance is used quite casually in many research articles. Lets attempt to define this term more precisely. Recall from the last lecture that human-level performance can be used as a proxy for Bayes error . Lets revisit that idea with another example. Suppose, for a medical image classification example, Typical human: 3% error Typical doctor: 1% error Experienced doctor: 0.7% error Team of experienced doctors: 0.5% error What is \"human-level\" error? Most likely, we would say 0.5% , and thus Bayes error is \\(\\le 0.05%\\). However, in certain contexts we may only wish to perform as well as the typical doctor (i.e., 1% error) and we may deem this \"human-level error\" . The takeaway is that there is sometimes more than one way to determine human-level performance; which way is appropriate will depend on the context in which we expect our algorithm to be deployed. We also note that as the performance of our algorithm improves, we may decide to move the goal posts for human-level performance higher, e.g., in this example by choosing a team of experienced doctors as the baseline. This is useful for solving the problem introduced in the previous lecture: should I focus on reducing avoidable bias? or should I focus on reducing variance between by training and dev errors. Summary Lets summarize: if you are trying to understand bias and variance when you have a human-level performance baseline: Human-level error can be used as a proxy for Bayes' error The difference between the training error and the human-level error can be thought of as the avoidable bias . The difference between the training and dev errors can be thought of as variance . Which type of error you should focus on reducing depends on how well your model perform compares to (an estimate of) human-level error. As our model approaches human-level performance, it becomes harder to determine where we should focus our efforts. Surpassing human-level performance Surpassing human-level performance is what many teams in machine learning / deep learning are inevitably trying to do. Lets take a look at a harder example to further develop our intuition for an approach to matching or surpassing human-level performance. team of humans: 0.5% error one human: 1.0% error training error: 0.3% error dev error: 0.4% error Notice that training error < team of humans error. Does this mean we have overfit the data by 0.2%? Or, does this means Bayes' error is actually lower than the team of humans error? We don't really know based on the information given, as to whether we should focus on bias or variance . This example is meant to illustrate that once we surpass human-level performance, it becomes much less clear how to improve performance further. Problems where ML significantly surpasses human-level performance Some example where ML significantly surpasses human-level performance include: Online advertising, Product recommendations Logistics (predicting transit time) Load approvals Notice that many of these tasks are learned on structured data and do not involve natural perception tasks . This appeals to our intuition, as we know humans are excellent at natural perception tasks. Note We also note that these four tasks have immensely large datasets for learning. Improving your model performance You have heard about orthogonalization. How to set up your dev and test sets, human level performance as a proxy for Bayes's error and how to estimate your avoidable bias and variance. Let's pull it all together into a set of guidelines for how to improve the performance of your learning algorithm. The two fundamental assumptions of supervised learning You can fit the training set (pretty) well, i.e., we can achieve low avoidable bias . The training set performance generalizes pretty well to the dev/test set, i.e., variance is not too bad . In the spirit of orthogonalization, there are a certain set of (separate) knobs we can use to improve bias and variance. Often, the difference between the training error and Bayes error (or a human-level proxy) is often illuminating in terms of where large improvement remain to be made. For reducing bias Train a bigger model Train longer/better optimization algorithms Change/tweak NN architecture/hyperparameter search. For reducing variance Collect more data Regularization (L2, dropout, data augmentation) Change/tweak NN architecture/hyperparameter search.","title":"Week 1"},{"location":"structuring_machine_learning_projects/week_1/#week-1-ml-strategy-1","text":"What is machine learning strategy? Lets start with a motivating example.","title":"Week 1: ML Strategy (1)"},{"location":"structuring_machine_learning_projects/week_1/#introduction-to-ml-strategy","text":"","title":"Introduction to ML strategy"},{"location":"structuring_machine_learning_projects/week_1/#why-ml-strategy","text":"Lets say you are working on a cat classifier . You have achieved 90% accuracy, but would like to improve performance even further. Your ideas for achieveing this are: collect more data collect more diverse training set train the algorithm longer with gradient descent try adam (or other optimizers) instead of gradient descent try dropout, add L2 regularization, change network architecture, ... This list is long, and so it becomes incredibly important to be able to identify ideas that are worth our time, and which ones we can likely discard. This course will attempt to introduce a framework for making these decisions. In particular, we will focus on the organization of deep learning-based projects .","title":"Why ML strategy"},{"location":"structuring_machine_learning_projects/week_1/#orthogonalization","text":"One of the challenges with building deep learning systems is the number of things we can tune to improve performance ( many hyperparameters notwithstanding ). Take the example of an old TV. They included many nobs for tuning the display position (x-axis position, y-axis position, rotation, etc...). Orthogonalization in this example refers to the TV designers decision to ensure each nob had one effect on the display and that these effects were relative to one another. If these nobs did more than one action and each actions magnitude was not relative to the other, it would become nearly impossible to tune the TV. Take another example, driving a car . Imagine if there was multiple joysticks. One joystick modified \\(0.3\\) X steering angle \\(- 0.8\\) speed, and another \\(2\\) X steering angle \\(+ 0.9\\) speed. In theory, by tuning these two nobs we could drive the car, but this would be much more difficult then separating the inputs into distinct input mechanisms . Orthogonal refers to the idea that the inputs are aligned to the dimensions we want to control. How does this related to machine learning?","title":"Orthogonalization"},{"location":"structuring_machine_learning_projects/week_1/#chain-of-assumption-in-examples","text":"For a machine learning system to perform \"well\", we usually aim to make four things happen: Fit training set well on cost function (for some applications, this means comparing favorably to human-level performance). Fit dev set well on cost function Fit test set well on cost function Performs well in real world. If we relate back to the TV example, we wanted one knob to change each attribute of the display. In the same way, we can modify knobs for each of our four steps above : Train a bigger network, change the optimization algorithm, ... Regularization, bigger training set, ... Bigger dev set, ... Change the dev set or the cost function Note Andrew said when he trains neural networks, he tends not to use early stopping . The reason being is that this is not a very orthogonal \"knob\"; it simultaneously effects how well we fit the training set and the dev set. The whole idea here is that if we keep our \"knobs\" orthogonal , we can more easily come up with solutions to specific problems with our deep neural networks (i.e., if we are getting poor performance on the training set, we may opt to train a bigger [higher variance] network).","title":"Chain of assumption in examples"},{"location":"structuring_machine_learning_projects/week_1/#setting-up-your-goal","text":"","title":"Setting up your goal"},{"location":"structuring_machine_learning_projects/week_1/#single-number-evaluation-metric","text":"When tuning neural networks (modifying hyper-parameters, trying different architectures, etc.) you will find that having a _single __evaluation metric___ will allow you to easily and quickly judge if a certain change improved performance. Note Andrew recommends deciding on a single, real-valued evaluation metric when starting out on your deep learning project. Lets look at an example. As we discussed previously, applied machine learning is a very empirical process. Lets say that we start with classifier A, and end up with classifier B after some change to the model. We could look at precision and recall as a means of improvements. What we really want is to improve both precision and recall. The problem is that it can become difficult to choose the \"best\" classifier if we are monitoring two different performance metrics, especially when we are making many modifications to our network. This is when it becomes important to chose a single performance metric. In this case specifically, we can chose the F1-score , the harmonic mean of the precision and recall (less formally, think of this as an average). We can see very quickly that classifier A has a better F1-score, and therefore we chose classifier A over classifier B.","title":"Single number evaluation metric"},{"location":"structuring_machine_learning_projects/week_1/#satisficing-and-optimizing-metric","text":"It is not always easy to combine all the metrics we care about into a single real-numbered value. Lets introduce satisficing and optimizing metrics as a solution to this problem. Lets say we are building a classifier, and we care about both our accuracy (measured as F1-score, traditional accuracy or some other metric) and the running time to classify a new example. One thing we can do, is to combine accuracy and run-time into a single-metric , possibly by taking a weighted linear sum of the two metrics. Note As it turns out, this tends to produce a rather artificial solution (no pun intended). Another way, is to attempt to maximize accuracy while subject to the restraint that \\(\\text{running time} \\le 100\\)ms. In this case, we say that accuracy is an optimizing metric (because we want to maximize or minimize it) and running time is a satisficing metric (because it just needs to meet a certain constraint, i.e., be \"good enough\"). More generally, if we have \\(m\\) metrics that we care about, it is reasonable to choose one to be our optimizing metric , and \\(m-1\\) to be satisficing metrics .","title":"Satisficing and Optimizing metric"},{"location":"structuring_machine_learning_projects/week_1/#example-wake-words","text":"We can take a concrete example to illustrate this: wake words for intelligent voice assistants . We might chose the accuracy of the model (i.e., what percent of the time does it \"wake\" when a wake word is said) to be out optimizing metric s.t. we have \\(\\le 1\\) false-positives per 24 hours of operation (our satisficing metric ).","title":"Example: Wake words"},{"location":"structuring_machine_learning_projects/week_1/#summary","text":"To summarize, if there are multiple things you care about, we can set one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfied. This idea goes hand-in-hand with the idea of having a single real-valued performance metric whereby we can quickly and easily chose the best model given a selection of models.","title":"Summary"},{"location":"structuring_machine_learning_projects/week_1/#traindevtest-distributions","text":"The way you set up your train, dev (sometimes called valid) and test sets can have a large impact on your development times and even model performance. In this video, we are going to focus on the dev (sometimes called the valid or hold out set) and the test set . The general workflow in machine learning is to train on the train set and test out model performance (e.g., different hyper-parameters or model architectures) on the dev set. Lets look at an example. Say we had data from multiple regions: US UK Other European countries South America India China Other Asian countries Australia If we were to build our dev set by choosing data from the first four regions and our test set from the last four regions, our data would likely be skewed and our model would likely perform poorly (at least on the test set). Why? Imagine the dev set as a target, and our job as machine learning engineers is to hit a bullseye. A dev set that is not representative of the overall general distribution is analogous to moving the bullseye away from its original location moments after we fire our bow . An ML team could spend months optimizing the model on a dev set, only to achieve very poor performance on a test set! So for our data above, a much better idea would be to sample data randomly from all regions to build our dev and test set.","title":"Train/dev/test distributions"},{"location":"structuring_machine_learning_projects/week_1/#guidelines","text":"Choose a dev set and test set (from the same distribution) to reflect data you expect to get in the future and consider important to do well on .","title":"Guidelines"},{"location":"structuring_machine_learning_projects/week_1/#size-of-the-dev-and-test-sets","text":"In the last lecture we saw that the dev and test sets should come from the same distributions. But how large should they be?","title":"Size of the dev and test sets"},{"location":"structuring_machine_learning_projects/week_1/#size-of-the-devtest-sets","text":"The rule of thumb in machine learning is typically 60% training , 20% dev , and 20% test (or 70/30 train / test ). In earlier eras of machine learning, this was pretty reasonable. In the modern machine learning era, we are used to working with much larger data set sizes. For example, imagine we have \\(1,000,000\\) examples. It might be totally reasonable for us to use 98% as our test set, 1% for dev and 1% for test . Note Note that 1% of \\(10^6\\) is \\(10^4\\)!","title":"Size of the dev/test sets"},{"location":"structuring_machine_learning_projects/week_1/#guidelines_1","text":"Set your test set to be big enough to give high confidence in the overall performance of your system.","title":"Guidelines"},{"location":"structuring_machine_learning_projects/week_1/#when-to-change-devtest-sets-and-metrics","text":"Sometimes during the course of a machine learning project, you will realize that you want to change your evaluation metric (i.e., move the \"goal posts\"). Lets illustrate this with an example:","title":"When to change dev/test sets and metrics"},{"location":"structuring_machine_learning_projects/week_1/#example-1","text":"Imagine we have two models for image classification, and we are using classification performance as our evaluation metric: Algorithm A has a 3% error, but sometimes shows users pornographic images. Algorithm B has a 5% error. Cleary, algorithm A performs better by our original evaluation metric (classification performance), but showing users pornographic images is unacceptable . \\[Error = \\frac{1}{m_{dev}}\\sum^{m_{dev}} {i=1} \\ell { y {pred}^{(i)} \\ne y^{(i)} }\\] Note Our error treats all incorrect predictions the same, pornographic or otherwise. We can think of it like this: our evaluation metric prefers algorithm A, but we (and our users) prefer algorithm B. When our evaluation metric is no longer ranking the algorithms in the order we would like, it is a sign that we may want to change our evaluation metric. In our specific example, we could solve this by weighting misclassifications \\[Error = \\frac{1}{w^{(i)}}\\sum^{m_{dev}} {i=1} w^{(i)}\\ell { y {pred}^{(i)} \\ne y^{(i)} }\\] where \\(w^{(i)}\\) is 1 if \\(x^{(i)}\\) is non-porn and 10 (or even 100 or larger) if \\(x^{(i)}\\) is porn. This is actually an example of orthogonalization . We, Define a metric to evaluate our model (\"placing the target\") (In a completely separate step) Worry about how to do well on this metric.","title":"Example 1"},{"location":"structuring_machine_learning_projects/week_1/#example-2","text":"Take the same example as above, but with a new twist. Say we train our classifier on a data set of high quality images. Then, when we deploy our model we notice it performs poorly. We narrow the problem down to the low quality images users are \"feeding\" to the model. What do we do? In general: if doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set .","title":"Example 2"},{"location":"structuring_machine_learning_projects/week_1/#comparing-to-human-level-performance","text":"In the last few years, comparing machine learning systems to human level performance have become common place. The reasons for this include: Deep learning based approaches are making extraordinary gains in performance, so our baseline needs to be more stringent. Many of the tasks deep learning is performing well at were thought to be very difficult for machines (e.g. NLP, computer vision). Comparing performance on these tasks to a human baseline is natural. It is also instructive to look at the performance of machine learning over time (note this is an obvious abstraction) Roughly speaking, performance (e.g., in a research domain or for a certain task) progresses quickly until we reach human-level performance, and tails off quickly. Why? mainly because human level performance is typically very close to the Bayes optimal error . Bayes optimal error is the best possible error; there is no way for any function mapping from \\(x \\rightarrow y\\) to do any better. A second reason is that so long as ML performs worse than humans for a given task, we can: get labeled data from humans gain insight from manual error analysis (e.g., why did a person get this right?) better analysis of bias/variance","title":"Comparing to human-level performance"},{"location":"structuring_machine_learning_projects/week_1/#avoidable-bias","text":"Of course, we want our learning algorithm to perform well on the training set, but not too well . Knowing where human level performance is can help us decide how well we want to perform on the training set. Let us again take the example of an image classifier. For this particular data set, assume: human-level performance is an error of 1%. our classifier is currently achieving 8% classification error on the training set and 10% classification on the dev set. Clearly, it has plenty of room to improve . Specifically, we would want to try to increase variance and reduce bias . Note For the purposes of computer vision, assume that human-level performance \\(\\approx\\) Bayes error. Now, lets take the same example, but instead, we assume that human-level performance is an error of 7.5% (this example is very contrived, as humans are extremely good at image classification). In this case, we note that our classifier performances nearly as well as a human baseline. We would likely want to to decrease variance and increase bias (in order to improve performance on the dev set.) So what did this example show us? When human-level performance (where we are using human-level performance as a proxy for Bayes error) is very high relative to our models performance on the train set, we likely want to focus on reducing \"avoidable\" bias (or increasing variance) in order to improve performance on the training set (e.g., by using a bigger network.) When human-level performance is comparable to our models performance on the train set, we likely want to focus on increasing bias (or decreasing variance) in order to improve performance on the dev set (e.g., by using a regularization technique or gathering more training data.)","title":"Avoidable bias"},{"location":"structuring_machine_learning_projects/week_1/#understanding-human-level-performance","text":"The term human-level performance is used quite casually in many research articles. Lets attempt to define this term more precisely. Recall from the last lecture that human-level performance can be used as a proxy for Bayes error . Lets revisit that idea with another example. Suppose, for a medical image classification example, Typical human: 3% error Typical doctor: 1% error Experienced doctor: 0.7% error Team of experienced doctors: 0.5% error What is \"human-level\" error? Most likely, we would say 0.5% , and thus Bayes error is \\(\\le 0.05%\\). However, in certain contexts we may only wish to perform as well as the typical doctor (i.e., 1% error) and we may deem this \"human-level error\" . The takeaway is that there is sometimes more than one way to determine human-level performance; which way is appropriate will depend on the context in which we expect our algorithm to be deployed. We also note that as the performance of our algorithm improves, we may decide to move the goal posts for human-level performance higher, e.g., in this example by choosing a team of experienced doctors as the baseline. This is useful for solving the problem introduced in the previous lecture: should I focus on reducing avoidable bias? or should I focus on reducing variance between by training and dev errors.","title":"Understanding human-level performance"},{"location":"structuring_machine_learning_projects/week_1/#summary_1","text":"Lets summarize: if you are trying to understand bias and variance when you have a human-level performance baseline: Human-level error can be used as a proxy for Bayes' error The difference between the training error and the human-level error can be thought of as the avoidable bias . The difference between the training and dev errors can be thought of as variance . Which type of error you should focus on reducing depends on how well your model perform compares to (an estimate of) human-level error. As our model approaches human-level performance, it becomes harder to determine where we should focus our efforts.","title":"Summary"},{"location":"structuring_machine_learning_projects/week_1/#surpassing-human-level-performance","text":"Surpassing human-level performance is what many teams in machine learning / deep learning are inevitably trying to do. Lets take a look at a harder example to further develop our intuition for an approach to matching or surpassing human-level performance. team of humans: 0.5% error one human: 1.0% error training error: 0.3% error dev error: 0.4% error Notice that training error < team of humans error. Does this mean we have overfit the data by 0.2%? Or, does this means Bayes' error is actually lower than the team of humans error? We don't really know based on the information given, as to whether we should focus on bias or variance . This example is meant to illustrate that once we surpass human-level performance, it becomes much less clear how to improve performance further.","title":"Surpassing human-level performance"},{"location":"structuring_machine_learning_projects/week_1/#problems-where-ml-significantly-surpasses-human-level-performance","text":"Some example where ML significantly surpasses human-level performance include: Online advertising, Product recommendations Logistics (predicting transit time) Load approvals Notice that many of these tasks are learned on structured data and do not involve natural perception tasks . This appeals to our intuition, as we know humans are excellent at natural perception tasks. Note We also note that these four tasks have immensely large datasets for learning.","title":"Problems where ML significantly surpasses human-level performance"},{"location":"structuring_machine_learning_projects/week_1/#improving-your-model-performance","text":"You have heard about orthogonalization. How to set up your dev and test sets, human level performance as a proxy for Bayes's error and how to estimate your avoidable bias and variance. Let's pull it all together into a set of guidelines for how to improve the performance of your learning algorithm.","title":"Improving your model performance"},{"location":"structuring_machine_learning_projects/week_1/#the-two-fundamental-assumptions-of-supervised-learning","text":"You can fit the training set (pretty) well, i.e., we can achieve low avoidable bias . The training set performance generalizes pretty well to the dev/test set, i.e., variance is not too bad . In the spirit of orthogonalization, there are a certain set of (separate) knobs we can use to improve bias and variance. Often, the difference between the training error and Bayes error (or a human-level proxy) is often illuminating in terms of where large improvement remain to be made. For reducing bias Train a bigger model Train longer/better optimization algorithms Change/tweak NN architecture/hyperparameter search. For reducing variance Collect more data Regularization (L2, dropout, data augmentation) Change/tweak NN architecture/hyperparameter search.","title":"The two fundamental assumptions of supervised learning"},{"location":"structuring_machine_learning_projects/week_2/","text":"Week 2: ML Strategy (2) Error Analysis Manually examining mistakes that your algorithm is making can give you insights into what to do next ( especially if your learning algorithm is not yet at the performance of a human ). This process is called error analysis . Let's start with an example. Carrying out error analysis Take for example our cat image classifier , and say we obtain 10% error on our test set , much worse than we were hoping to do. Assume further that a colleague notices some of the misclassified examples are actually pictures of dogs. The question becomes, should you try to make your cat classifier do better on dogs? This is where error analysis is particularly useful. In this example, we might: collect ~100 mislabeled dev set examples count up how any many dogs Lets say we find that 5/100 (5%) mislabeled dev set example are dogs. Thus, the best we could hope to do (if we were to completely solve the dog problem) is decrease our error from 10% to 9.5% (a 5% relative drop in error.) We conclude that this is likely not the best use of our time . Sometimes, this is called the ceiling , i.e., the maximum amount of improvement we can expect from some change to our algorithm/dataset. Suppose instead we find 50/100 (50%) mislabeled dev set examples are dogs. Thus, if we solve the dog problem, we could decrease our error from 10% to 5% (a 50% relative drop in error.) We conclude that solving the dog problem is likely a good use of our time . Note Notice the disproportionate 'payoff' here. It may take < 10 min to manually examine 100 examples from our dev set, but the exercise offers major clues as to where to focus our efforts. Evaluate multiple ideas in parallel Lets, continue with our cat detection example. Sometimes we might want to evaluate multiple ideas in parallel . For example, say we have the following ideas: fix pictures of dogs being recognized as cats fix great cats (lions, panthers, etc..) being misrecognized improve performance on blurry images What can do is create a table, where the rows represent the images we plan on evaluating manually, and the columns represent the categorizes we think the algorithm may be misrecognizing. It is also helpful to add comments describing the the misclassified example. As you are part-way through this process, you may also notice another common category of mistake, which you can add to this manual evaluation and repeat. The conclusion of this process is estimates for: which errors we should direct our attention to solving how much we should expect performance to improve if reduce the number of errors in each category Summary To summarize: when carrying out error analysis, you should find a set of mislabeled examples and look at these examples for false positives and false negatives . Counting up the number of errors that fall into various different categories will often this will help you prioritize, or give you inspiration for new directions to go in for improving your algorithm. Three numbers to keep your eye on Overall dev set error Errors due to cause of interest / Overall dev set error Error due to other causes / Overall dev set error If the errors due to other causes >> errors due to cause of interest, it will likely be more productive to ignore our cause of interest for the time being and seek another source of error we can try to minimize. Note In this case, cause of interest is just our idea for improving our leaning algorithm, e.g., fix pictures of dogs being recognized as cats Cleaning up incorrectly labeled data In supervised learning, we (typically) have hand-labeled training data. What if we realize that some examples are incorrectly labeled? First, lets consider our training set. Note In an effort to be less ambiguous, we use mislabeled when we are referring to examples the ML algo labeled incorrectly and incorrectly labeled when we are referring to examples in the training data set with the wrong label. Training set Deep learning algorithms are quite robust to random errors in the training set. If the errors are reasonably random and the dataset is big enough (i.e., the errors make up only a tiny proportion of all examples) performance of our algorithm is unlikely to be affected. Systematic errors are much more of a problem. Taking as example our cat classifier again, if labelers mistakingly label all white dogs as cats, this will dramatically impact performance of our classifier, which is likely to labels white dogs as cats with high degree of confidence . Dev/test set If you suspect that there are many incorrectly labeled examples in your dev or test set, you can add another column to your error analysis table where you track these incorrectly labeled examples. Depending on the total percentage of these examples, you can decide if it is worth the time to go through and correct all incorrectly labeled examples in your dev or test set. There are some special considerations when correcting incorrect dev/test set examples, namely: apply the same process to your dev and test sets to make sure they continue to come from the same distribution considering examining examples your algorithm got right as well as ones it got wrong train and dev/test data may now come from different distributions --- this is not necessarily a problem Build quickly, then iterate If you are working on a brand new ML system, it is recommended to build quickly , then iterate . For many problems, there are often tens or hundreds of directions we could reasonably choose to go in. Building a system quickly breaks down to the following tasks: set up a dev/test set and metric build the initial system quickly and deploy use bias/variance analysis & error analysis to prioritize next steps A lot of value in this approach lies in the fact that we can quickly build insight to our problem. Note Note that this advice applies less when we have significant expertise in a given area and/or there is a significant body of academic work for the same or a very similar task (i.e., face recognition). Mismatched training and dev/test set Deep learning algorithms are extremely data hungry . Because of this, some teams are tempted into shoving as much information into their training sets as possible. However, this poses a problem when the data sources do not come from the same distributions. Lets illustrate this again with an example. Take our cat classifier. Say we have ~10,000 images from a mobile app , and these are the images (or type of images) we hope to do well on. Assume as well that we have ~200,000 images from webpages , which have a slightly different underlying distribution than the mobile app images (say, for example, that they are generally higher quality.) How do we combine these data sets? Option 1 We could take the all datasets, combine them, and shuffle them randomly into train/dev/test sets. However, this poses the obvious problem that many of the examples in our dev set (~95% of them) will be from the webpage dataset . We are effectively tuning our algorithm to a distribution that is slightly different than our target distribution --- data from the mobile app. Option 2 The second, recommended option, is to comprise the dev/test sets of images entirely from the target (i.e., mobile data) distribution . The advantage, is that we are now \"aiming the target\" in the right place, i.e., the distribution we hope to perform well on. The disadvantage of course, is that the training set comes from a different distribution than our target (dev/test) sets. However, this method is still superior to option 1 , and we will discuss laters further ways of dealing with this difference in distributions. Note Note, we can still include examples from the distribution we care about in our training set, assuming we have enough data from this distribution. Bias and Variance with mismatched data distributions Estimating the bias and variance of your learning algorithm can really help you prioritize what to work on next. The way you analyze bias and variance changes when your training set comes from a different distribution than your dev and test sets. Let's see how. Let's keep using our cat classification example and let's say humans get near perfect performance on this. So, Bayes error, or Bayes optimal error, we know is nearly 0% on this problem. Assume further: training error: 1% dev error: 10% If your dev data came from the same distribution as your training set, you would say that you have a large variance problem, i.e., your algorithm is not generalizing well from the training set to the dev set. But in the setting where your training data and your dev data comes from a different distribution , you can no longer safely draw this conclusion. If the training and dev data come from different underlying distributions , then by comparing the training set to the dev set we are actually observing two different changes at the same time: The algorithm saw the training data. It did not see the dev data The data do not come from the same underlying distribution In order to tease out these which of these is conributing to the drop in perfromsnce from our train to dev set, it will be useful to define a new piece of data which we'll call the training-dev set: a new subset of data with the same distribution as the training set, but not used for training. Heres what we mean, previously we had train/dev/test sets. What we are going to do instead is randomly shuffle the training set and carve out a part of this shuffled set to be the training-dev . Note Just as the dev/test sets have the same distribution, the train-dev set and train set have the same distribution. Now, say we have the following errors: training error: 1% train-dev error: 9% dev error: 10% We see that training error \\(\\lt \\lt\\) train-dev error \\(\\approx\\) dev error. Because the train and train-dev sets come from the same underlying distribution, we can safely conclude that the large increase in error from the train set to the dev set is due to variance (i.e., our network is not generalizing well) Lets look at a counter example. Say we have the following errors: training error: 1% train-dev error: 1.5% dev error: 10% This is much more likely to be a data mismatch problem . Specifically, the algorithm is performing extremely well on the train and train-dev sets, but poorly on the dev set, hinting that the train/train-dev sets likely come from different underlying distributions than the dev set. Finally, one last example. Say we have the following errors: Bayes error: \\(\\approx\\) 0% training error: 10% train-dev error: 11% dev error: 20% Here, we likely have two problems. First, we notice an avoidable bias problem, suggested by the fact that our training error \\(\\gt \\gt\\) Bayes error. We also have a data mismatch problem , suggested by the fact that our training error \\(\\approx\\) train-dev error by both are \\(\\lt \\lt\\) our dev error. So let's take what we've done and write out the general principles. The key quantities your want to look at are: human-level error (or Bayes error), training set error, training-dev set error and the dev set error. The differences between these errors give us a sense about the avoidable bias , the variance , and the data mismatch problem . Generally, training error \\(\\gt \\gt\\) Bayes error: avoidable bias problem training error \\(\\lt \\lt\\) train-dev error: variance problem training error \\(\\approx\\) train-dev error \\(\\lt \\lt\\) dev error: data mismatch problem. More general formulation We can organize these metrics into a table; where the columns are different datasets (if you have more than one) and the rows are the error for examples the algorithm was trained on and examples the algorithm was not trained on. Addressing data mismatch If your training set comes from a different distribution, than your dev and test set, and if error analysis shows you that you have a data mismatch problem, what can you do? Unfortunately, there are not (completely) systematic solutions to this, but let's look at some things you could try. Some recommendations: carry out manual error analysis to try to understand different between training and dev/test sets. for example, you may find that many of the examples in your dev set are noisy when compared to those in your training set. make training data more similar; or collect more data similar to dev/test sets. for example, you may simulate noise in the training set The second point leads us into the idea of artificial data synthesis Artificial data synthesis In some cases, we may be able to artificially synthesis data to make up for a lack of real data. For example, we can imagine synthesizing images of cars to supplement a dataset of car images for the task of car recognition in photos. While artificial data synthesis can be a powerful technique for increasing the size of our dataset (and thus the performance of our learning algorithm), we must be wary of overfitting to the synthesized data. Say for example, the set of \"all cars\" and \"synthesized cars\" looked as follows: In this case, we run a real risk of our algorithm overfitting to the synthesized images. Learning from multiple tasks Transfer learning One of the most powerful ideas in deep learning is that you can take knowledge the neural network has learned from one task and apply that knowledge to a separate task . So for example, maybe you could have the neural network learn to recognize objects like cats and then use parts of that knowledge to help you do a better job reading X-ray scans. This is called transfer learning . Let's take a look. Lets say you have trained a neural network for image recognition . If you want to take this neural network and transfer it to a different task, say radiology diagnosis, one method would be to delete the last layer, and re-randomly initialize the weights feeding into the output layer. To be concrete: during the first phase of training when you're training on an image recognition task, you train all of the usual parameters for the neural network, all the weights, all the layers having trained that neural network, what you now do to implement transfer learning is swap in a new data set \\(X,Y\\), where now these are radiology images and diagnoses pairs. finally, initialize the last layers' weights randomly and retrain the neural network on this new data set. We have a couple options on how we retrain the dataset. If the radiology dataset is small : we should likely \"freeze\" the transferred layers and only train the output layer. If the radiology dataset is large : we should likely train all layers. Note Sometimes, we call the process of training on the first dataset pre-training , and the process of training on the second dataset fine-tuning . The idea is that learning from a very large image data set allows us to transfer some fundamental knowledge for the task of computer vision (i.e., extracting features such as lines/edges, small objects, etc.) Note Note that transfer learning is not confined to computer vision examples, recent research has shown much success deploying transfer learning for NLP tasks. When does transfer learning make sense? Transfer learning makes sense when you have a lot of data for the problem you're transferring from and usually relatively less data for the problem you're transferring to . So for our example, let's say you have a million examples for image recognition task. Thats a lot of data to learn low level features or to learn a lot of useful features in the earlier layers in neural network. But for the radiology task, assume we only a hundred examples. So a lot of knowledge you learn from image recognition can be transferred and can really help you get going with radiology recognition even if you don't have enough data to perform well for the radiology diagnosis task. If you're trying to learn from some Task A and transfer some of the knowledge to some Task B , then transfer learning makes sense when: Task A and B have the same input X. you have a lot more data for Task A than for Task B --- all this is under the assumption that what you really want to do well on is Task B. transfer learning will tend to make more sense if you suspect that low level features from Task A could be helpful for learning Task B. Multi-task learning Whereas in transfer learning, you have a sequential process where you learn from task A and then transfer that to task B --- in multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. The idea is that shared information from each of these tasks improves performance on all tasks. Let's look at an example. Simplified autonomous driving example Let's say you're building an autonomous vehicle. Then your self driving car would need to detect several different things such as pedestrians , other cars , stop signs , traffic lights etc. Our input to the learning algorithm could be a single image, our our label for that example, \\(y^{(i)}\\) might be a four-dimensional column vector, where \\(0\\) at position \\(j\\) represents absence of that object from the image and \\(1\\) represents presence. Note E.g., a \\(0\\) at the first index of \\(y^{(i)}\\) might specify absence of a pedestrian in the image. Our neural network architecture would then involve a single input and output layer. The twist is that the output layer would have \\(j\\) number of nodes, one per object we want to recognize. To account for this, our cost function will need to sum over the individual loss functions for each of the objects we wish to recongize: \\[Cost = \\frac{1}{m}\\sum^m_{i=1}\\sum^m_{j=1}\\ell(\\hat y_j^{(i)}, y_j^{(i)})\\] Note Were \\(\\ell\\) is our logisitc loss. Unlike traditional softmax regression, one image can have multiple labels. This, in essense, is multi-task learning, as we are preforming multiple tasks with the same neural network (sets of weights/biases). When does multi-task learning make sense? Typically (but with some exceptions) when the following hold: Training on a set of tasks that could benefit from having shared lower-level features. Amount of data you have for each task is quite similar. Can train a big enough neural network to do well on all the tasks Note The last point is important. We typically need to \"scale-up\" the neural network in multi-task learning, as we will need a high variance model to be able to perform well on multiple tasks and typically more data --- as opposed to single tasks. End-to-end deep learning One of the most exciting recent developments in deep learning has been the rise of end-to-end deep learning. So what is the end-to-end learning? Briefly, there have been some data processing systems, or learning systems that require multiple stages of processing . In contrast, end-to-end deep learning attempts to replace those multiple stages with a single neural network. Let's look at some examples. What is end-to-end deep learning? Speech recognition example At a high level, the task of speech recognition requires receiving as input some audio singles containing spoken words, and mapping that to a transcript containing those words. Traditionally, speech recognition involved many stages of processing: First, you would extract \"hand-designed\" features from the audio clip Feed these features into a ML algorithm which would extract phonemes Concatenate these phonemes to form words and then transcripts In contrast to this step-by-step pipeline, end-to-end deep learning seeks to model all these tasks with a single network given a set of inputs. The more traditional, hand-crafted approach tends to outperform the end-to-end approach when our dataset is small , but this relationship flips as the dataset grows larger. Indeed, one of the biggest barriers to using end-to-end deep learning approaches is that large datasets which map our input to our final downstream task are rare . Note Think about this for a second and it makes perfect sense, its only recently in the era of deep learning that datasets have begun to map inputs to downstream outputs, skipping many of the intermediate levels of representation (images \\(\\Rightarrow\\) labels, audio clips \\(\\Rightarrow\\) transcripts.) One example where end-to-end deep learning currently works very well is machine translation (massive, parallel corpuses have made end-to-end solutions feasible.) Summary When end-to-end deep learning works, it can work really well and can simplify the system, removing the need to build many hand-designed individual components. But it's also not panacea, it doesn't always work . Whether or not to use end-to-end learning Let's say in building a machine learning system you're trying to decide whether or not to use an end-to-end approach. Let's take a look at some of the pros and cons of end-to-end deep learning so that you can come away with some guidelines on whether or not an end-to-end approach seems promising for your application. Pros and cons of end-to-end deep learning Pros : let the data speak : if you have enough labeled data, your network (given that it is large enough) should be able to a mapping from \\(x \\rightarrow y\\), with out having to rely on a humans preconceived notions or forcing the model to use some representation of the relationship between inputs an outputs. less hand-designing of components needed : end-to-end deep learning seeks to model the entire task with a single learning algorithm, which typically involves little in the way of hand-designing components. Cons : likely need a large amount of data for end-to-end learning to work well excludes potentially useful hand-designed components : if we have only a small training set, our learning algorithm likely does not have enough examples to learn representations that perform well. Although deep learning practitioners often speak despairingly about hand-crafted features or components, they allow us to inject priors into our model, which is particularly useful when we do not have a lot of labeled data. Note Note: hand-designed components and features are a double-edged sword. Poorly designed components may actually harm performance of the model by forcing the model to obey incorrect assumptions about the data. Should I use end-to-end deep learning? The key question we need to ask ourselves when considering on using end-to-end deep learning is: Do you have sufficient data to learn a function of the complexity needed to map \\(x\\) to \\(y\\)? Unfortunately, we do not have a formal definition of complexity --- we have to rely on our intuition.","title":"Week 2"},{"location":"structuring_machine_learning_projects/week_2/#week-2-ml-strategy-2","text":"","title":"Week 2: ML Strategy (2)"},{"location":"structuring_machine_learning_projects/week_2/#error-analysis","text":"Manually examining mistakes that your algorithm is making can give you insights into what to do next ( especially if your learning algorithm is not yet at the performance of a human ). This process is called error analysis . Let's start with an example.","title":"Error Analysis"},{"location":"structuring_machine_learning_projects/week_2/#carrying-out-error-analysis","text":"Take for example our cat image classifier , and say we obtain 10% error on our test set , much worse than we were hoping to do. Assume further that a colleague notices some of the misclassified examples are actually pictures of dogs. The question becomes, should you try to make your cat classifier do better on dogs? This is where error analysis is particularly useful. In this example, we might: collect ~100 mislabeled dev set examples count up how any many dogs Lets say we find that 5/100 (5%) mislabeled dev set example are dogs. Thus, the best we could hope to do (if we were to completely solve the dog problem) is decrease our error from 10% to 9.5% (a 5% relative drop in error.) We conclude that this is likely not the best use of our time . Sometimes, this is called the ceiling , i.e., the maximum amount of improvement we can expect from some change to our algorithm/dataset. Suppose instead we find 50/100 (50%) mislabeled dev set examples are dogs. Thus, if we solve the dog problem, we could decrease our error from 10% to 5% (a 50% relative drop in error.) We conclude that solving the dog problem is likely a good use of our time . Note Notice the disproportionate 'payoff' here. It may take < 10 min to manually examine 100 examples from our dev set, but the exercise offers major clues as to where to focus our efforts.","title":"Carrying out error analysis"},{"location":"structuring_machine_learning_projects/week_2/#evaluate-multiple-ideas-in-parallel","text":"Lets, continue with our cat detection example. Sometimes we might want to evaluate multiple ideas in parallel . For example, say we have the following ideas: fix pictures of dogs being recognized as cats fix great cats (lions, panthers, etc..) being misrecognized improve performance on blurry images What can do is create a table, where the rows represent the images we plan on evaluating manually, and the columns represent the categorizes we think the algorithm may be misrecognizing. It is also helpful to add comments describing the the misclassified example. As you are part-way through this process, you may also notice another common category of mistake, which you can add to this manual evaluation and repeat. The conclusion of this process is estimates for: which errors we should direct our attention to solving how much we should expect performance to improve if reduce the number of errors in each category","title":"Evaluate multiple ideas in parallel"},{"location":"structuring_machine_learning_projects/week_2/#summary","text":"To summarize: when carrying out error analysis, you should find a set of mislabeled examples and look at these examples for false positives and false negatives . Counting up the number of errors that fall into various different categories will often this will help you prioritize, or give you inspiration for new directions to go in for improving your algorithm. Three numbers to keep your eye on Overall dev set error Errors due to cause of interest / Overall dev set error Error due to other causes / Overall dev set error If the errors due to other causes >> errors due to cause of interest, it will likely be more productive to ignore our cause of interest for the time being and seek another source of error we can try to minimize. Note In this case, cause of interest is just our idea for improving our leaning algorithm, e.g., fix pictures of dogs being recognized as cats","title":"Summary"},{"location":"structuring_machine_learning_projects/week_2/#cleaning-up-incorrectly-labeled-data","text":"In supervised learning, we (typically) have hand-labeled training data. What if we realize that some examples are incorrectly labeled? First, lets consider our training set. Note In an effort to be less ambiguous, we use mislabeled when we are referring to examples the ML algo labeled incorrectly and incorrectly labeled when we are referring to examples in the training data set with the wrong label.","title":"Cleaning up incorrectly labeled data"},{"location":"structuring_machine_learning_projects/week_2/#training-set","text":"Deep learning algorithms are quite robust to random errors in the training set. If the errors are reasonably random and the dataset is big enough (i.e., the errors make up only a tiny proportion of all examples) performance of our algorithm is unlikely to be affected. Systematic errors are much more of a problem. Taking as example our cat classifier again, if labelers mistakingly label all white dogs as cats, this will dramatically impact performance of our classifier, which is likely to labels white dogs as cats with high degree of confidence .","title":"Training set"},{"location":"structuring_machine_learning_projects/week_2/#devtest-set","text":"If you suspect that there are many incorrectly labeled examples in your dev or test set, you can add another column to your error analysis table where you track these incorrectly labeled examples. Depending on the total percentage of these examples, you can decide if it is worth the time to go through and correct all incorrectly labeled examples in your dev or test set. There are some special considerations when correcting incorrect dev/test set examples, namely: apply the same process to your dev and test sets to make sure they continue to come from the same distribution considering examining examples your algorithm got right as well as ones it got wrong train and dev/test data may now come from different distributions --- this is not necessarily a problem","title":"Dev/test set"},{"location":"structuring_machine_learning_projects/week_2/#build-quickly-then-iterate","text":"If you are working on a brand new ML system, it is recommended to build quickly , then iterate . For many problems, there are often tens or hundreds of directions we could reasonably choose to go in. Building a system quickly breaks down to the following tasks: set up a dev/test set and metric build the initial system quickly and deploy use bias/variance analysis & error analysis to prioritize next steps A lot of value in this approach lies in the fact that we can quickly build insight to our problem. Note Note that this advice applies less when we have significant expertise in a given area and/or there is a significant body of academic work for the same or a very similar task (i.e., face recognition).","title":"Build quickly, then iterate"},{"location":"structuring_machine_learning_projects/week_2/#mismatched-training-and-devtest-set","text":"Deep learning algorithms are extremely data hungry . Because of this, some teams are tempted into shoving as much information into their training sets as possible. However, this poses a problem when the data sources do not come from the same distributions. Lets illustrate this again with an example. Take our cat classifier. Say we have ~10,000 images from a mobile app , and these are the images (or type of images) we hope to do well on. Assume as well that we have ~200,000 images from webpages , which have a slightly different underlying distribution than the mobile app images (say, for example, that they are generally higher quality.) How do we combine these data sets?","title":"Mismatched training and dev/test set"},{"location":"structuring_machine_learning_projects/week_2/#option-1","text":"We could take the all datasets, combine them, and shuffle them randomly into train/dev/test sets. However, this poses the obvious problem that many of the examples in our dev set (~95% of them) will be from the webpage dataset . We are effectively tuning our algorithm to a distribution that is slightly different than our target distribution --- data from the mobile app.","title":"Option 1"},{"location":"structuring_machine_learning_projects/week_2/#option-2","text":"The second, recommended option, is to comprise the dev/test sets of images entirely from the target (i.e., mobile data) distribution . The advantage, is that we are now \"aiming the target\" in the right place, i.e., the distribution we hope to perform well on. The disadvantage of course, is that the training set comes from a different distribution than our target (dev/test) sets. However, this method is still superior to option 1 , and we will discuss laters further ways of dealing with this difference in distributions. Note Note, we can still include examples from the distribution we care about in our training set, assuming we have enough data from this distribution.","title":"Option 2"},{"location":"structuring_machine_learning_projects/week_2/#bias-and-variance-with-mismatched-data-distributions","text":"Estimating the bias and variance of your learning algorithm can really help you prioritize what to work on next. The way you analyze bias and variance changes when your training set comes from a different distribution than your dev and test sets. Let's see how. Let's keep using our cat classification example and let's say humans get near perfect performance on this. So, Bayes error, or Bayes optimal error, we know is nearly 0% on this problem. Assume further: training error: 1% dev error: 10% If your dev data came from the same distribution as your training set, you would say that you have a large variance problem, i.e., your algorithm is not generalizing well from the training set to the dev set. But in the setting where your training data and your dev data comes from a different distribution , you can no longer safely draw this conclusion. If the training and dev data come from different underlying distributions , then by comparing the training set to the dev set we are actually observing two different changes at the same time: The algorithm saw the training data. It did not see the dev data The data do not come from the same underlying distribution In order to tease out these which of these is conributing to the drop in perfromsnce from our train to dev set, it will be useful to define a new piece of data which we'll call the training-dev set: a new subset of data with the same distribution as the training set, but not used for training. Heres what we mean, previously we had train/dev/test sets. What we are going to do instead is randomly shuffle the training set and carve out a part of this shuffled set to be the training-dev . Note Just as the dev/test sets have the same distribution, the train-dev set and train set have the same distribution. Now, say we have the following errors: training error: 1% train-dev error: 9% dev error: 10% We see that training error \\(\\lt \\lt\\) train-dev error \\(\\approx\\) dev error. Because the train and train-dev sets come from the same underlying distribution, we can safely conclude that the large increase in error from the train set to the dev set is due to variance (i.e., our network is not generalizing well) Lets look at a counter example. Say we have the following errors: training error: 1% train-dev error: 1.5% dev error: 10% This is much more likely to be a data mismatch problem . Specifically, the algorithm is performing extremely well on the train and train-dev sets, but poorly on the dev set, hinting that the train/train-dev sets likely come from different underlying distributions than the dev set. Finally, one last example. Say we have the following errors: Bayes error: \\(\\approx\\) 0% training error: 10% train-dev error: 11% dev error: 20% Here, we likely have two problems. First, we notice an avoidable bias problem, suggested by the fact that our training error \\(\\gt \\gt\\) Bayes error. We also have a data mismatch problem , suggested by the fact that our training error \\(\\approx\\) train-dev error by both are \\(\\lt \\lt\\) our dev error. So let's take what we've done and write out the general principles. The key quantities your want to look at are: human-level error (or Bayes error), training set error, training-dev set error and the dev set error. The differences between these errors give us a sense about the avoidable bias , the variance , and the data mismatch problem . Generally, training error \\(\\gt \\gt\\) Bayes error: avoidable bias problem training error \\(\\lt \\lt\\) train-dev error: variance problem training error \\(\\approx\\) train-dev error \\(\\lt \\lt\\) dev error: data mismatch problem.","title":"Bias and Variance with mismatched data distributions"},{"location":"structuring_machine_learning_projects/week_2/#more-general-formulation","text":"We can organize these metrics into a table; where the columns are different datasets (if you have more than one) and the rows are the error for examples the algorithm was trained on and examples the algorithm was not trained on.","title":"More general formulation"},{"location":"structuring_machine_learning_projects/week_2/#addressing-data-mismatch","text":"If your training set comes from a different distribution, than your dev and test set, and if error analysis shows you that you have a data mismatch problem, what can you do? Unfortunately, there are not (completely) systematic solutions to this, but let's look at some things you could try. Some recommendations: carry out manual error analysis to try to understand different between training and dev/test sets. for example, you may find that many of the examples in your dev set are noisy when compared to those in your training set. make training data more similar; or collect more data similar to dev/test sets. for example, you may simulate noise in the training set The second point leads us into the idea of artificial data synthesis","title":"Addressing data mismatch"},{"location":"structuring_machine_learning_projects/week_2/#artificial-data-synthesis","text":"In some cases, we may be able to artificially synthesis data to make up for a lack of real data. For example, we can imagine synthesizing images of cars to supplement a dataset of car images for the task of car recognition in photos. While artificial data synthesis can be a powerful technique for increasing the size of our dataset (and thus the performance of our learning algorithm), we must be wary of overfitting to the synthesized data. Say for example, the set of \"all cars\" and \"synthesized cars\" looked as follows: In this case, we run a real risk of our algorithm overfitting to the synthesized images.","title":"Artificial data synthesis"},{"location":"structuring_machine_learning_projects/week_2/#learning-from-multiple-tasks","text":"","title":"Learning from multiple tasks"},{"location":"structuring_machine_learning_projects/week_2/#transfer-learning","text":"One of the most powerful ideas in deep learning is that you can take knowledge the neural network has learned from one task and apply that knowledge to a separate task . So for example, maybe you could have the neural network learn to recognize objects like cats and then use parts of that knowledge to help you do a better job reading X-ray scans. This is called transfer learning . Let's take a look. Lets say you have trained a neural network for image recognition . If you want to take this neural network and transfer it to a different task, say radiology diagnosis, one method would be to delete the last layer, and re-randomly initialize the weights feeding into the output layer. To be concrete: during the first phase of training when you're training on an image recognition task, you train all of the usual parameters for the neural network, all the weights, all the layers having trained that neural network, what you now do to implement transfer learning is swap in a new data set \\(X,Y\\), where now these are radiology images and diagnoses pairs. finally, initialize the last layers' weights randomly and retrain the neural network on this new data set. We have a couple options on how we retrain the dataset. If the radiology dataset is small : we should likely \"freeze\" the transferred layers and only train the output layer. If the radiology dataset is large : we should likely train all layers. Note Sometimes, we call the process of training on the first dataset pre-training , and the process of training on the second dataset fine-tuning . The idea is that learning from a very large image data set allows us to transfer some fundamental knowledge for the task of computer vision (i.e., extracting features such as lines/edges, small objects, etc.) Note Note that transfer learning is not confined to computer vision examples, recent research has shown much success deploying transfer learning for NLP tasks.","title":"Transfer learning"},{"location":"structuring_machine_learning_projects/week_2/#when-does-transfer-learning-make-sense","text":"Transfer learning makes sense when you have a lot of data for the problem you're transferring from and usually relatively less data for the problem you're transferring to . So for our example, let's say you have a million examples for image recognition task. Thats a lot of data to learn low level features or to learn a lot of useful features in the earlier layers in neural network. But for the radiology task, assume we only a hundred examples. So a lot of knowledge you learn from image recognition can be transferred and can really help you get going with radiology recognition even if you don't have enough data to perform well for the radiology diagnosis task. If you're trying to learn from some Task A and transfer some of the knowledge to some Task B , then transfer learning makes sense when: Task A and B have the same input X. you have a lot more data for Task A than for Task B --- all this is under the assumption that what you really want to do well on is Task B. transfer learning will tend to make more sense if you suspect that low level features from Task A could be helpful for learning Task B.","title":"When does transfer learning make sense?"},{"location":"structuring_machine_learning_projects/week_2/#multi-task-learning","text":"Whereas in transfer learning, you have a sequential process where you learn from task A and then transfer that to task B --- in multi-task learning, you start off simultaneously, trying to have one neural network do several things at the same time. The idea is that shared information from each of these tasks improves performance on all tasks. Let's look at an example.","title":"Multi-task learning"},{"location":"structuring_machine_learning_projects/week_2/#simplified-autonomous-driving-example","text":"Let's say you're building an autonomous vehicle. Then your self driving car would need to detect several different things such as pedestrians , other cars , stop signs , traffic lights etc. Our input to the learning algorithm could be a single image, our our label for that example, \\(y^{(i)}\\) might be a four-dimensional column vector, where \\(0\\) at position \\(j\\) represents absence of that object from the image and \\(1\\) represents presence. Note E.g., a \\(0\\) at the first index of \\(y^{(i)}\\) might specify absence of a pedestrian in the image. Our neural network architecture would then involve a single input and output layer. The twist is that the output layer would have \\(j\\) number of nodes, one per object we want to recognize. To account for this, our cost function will need to sum over the individual loss functions for each of the objects we wish to recongize: \\[Cost = \\frac{1}{m}\\sum^m_{i=1}\\sum^m_{j=1}\\ell(\\hat y_j^{(i)}, y_j^{(i)})\\] Note Were \\(\\ell\\) is our logisitc loss. Unlike traditional softmax regression, one image can have multiple labels. This, in essense, is multi-task learning, as we are preforming multiple tasks with the same neural network (sets of weights/biases).","title":"Simplified autonomous driving example"},{"location":"structuring_machine_learning_projects/week_2/#when-does-multi-task-learning-make-sense","text":"Typically (but with some exceptions) when the following hold: Training on a set of tasks that could benefit from having shared lower-level features. Amount of data you have for each task is quite similar. Can train a big enough neural network to do well on all the tasks Note The last point is important. We typically need to \"scale-up\" the neural network in multi-task learning, as we will need a high variance model to be able to perform well on multiple tasks and typically more data --- as opposed to single tasks.","title":"When does multi-task learning make sense?"},{"location":"structuring_machine_learning_projects/week_2/#end-to-end-deep-learning","text":"One of the most exciting recent developments in deep learning has been the rise of end-to-end deep learning. So what is the end-to-end learning? Briefly, there have been some data processing systems, or learning systems that require multiple stages of processing . In contrast, end-to-end deep learning attempts to replace those multiple stages with a single neural network. Let's look at some examples.","title":"End-to-end deep learning"},{"location":"structuring_machine_learning_projects/week_2/#what-is-end-to-end-deep-learning","text":"","title":"What is end-to-end deep learning?"},{"location":"structuring_machine_learning_projects/week_2/#speech-recognition-example","text":"At a high level, the task of speech recognition requires receiving as input some audio singles containing spoken words, and mapping that to a transcript containing those words. Traditionally, speech recognition involved many stages of processing: First, you would extract \"hand-designed\" features from the audio clip Feed these features into a ML algorithm which would extract phonemes Concatenate these phonemes to form words and then transcripts In contrast to this step-by-step pipeline, end-to-end deep learning seeks to model all these tasks with a single network given a set of inputs. The more traditional, hand-crafted approach tends to outperform the end-to-end approach when our dataset is small , but this relationship flips as the dataset grows larger. Indeed, one of the biggest barriers to using end-to-end deep learning approaches is that large datasets which map our input to our final downstream task are rare . Note Think about this for a second and it makes perfect sense, its only recently in the era of deep learning that datasets have begun to map inputs to downstream outputs, skipping many of the intermediate levels of representation (images \\(\\Rightarrow\\) labels, audio clips \\(\\Rightarrow\\) transcripts.) One example where end-to-end deep learning currently works very well is machine translation (massive, parallel corpuses have made end-to-end solutions feasible.)","title":"Speech recognition example"},{"location":"structuring_machine_learning_projects/week_2/#summary_1","text":"When end-to-end deep learning works, it can work really well and can simplify the system, removing the need to build many hand-designed individual components. But it's also not panacea, it doesn't always work .","title":"Summary"},{"location":"structuring_machine_learning_projects/week_2/#whether-or-not-to-use-end-to-end-learning","text":"Let's say in building a machine learning system you're trying to decide whether or not to use an end-to-end approach. Let's take a look at some of the pros and cons of end-to-end deep learning so that you can come away with some guidelines on whether or not an end-to-end approach seems promising for your application.","title":"Whether or not to use end-to-end learning"},{"location":"structuring_machine_learning_projects/week_2/#pros-and-cons-of-end-to-end-deep-learning","text":"Pros : let the data speak : if you have enough labeled data, your network (given that it is large enough) should be able to a mapping from \\(x \\rightarrow y\\), with out having to rely on a humans preconceived notions or forcing the model to use some representation of the relationship between inputs an outputs. less hand-designing of components needed : end-to-end deep learning seeks to model the entire task with a single learning algorithm, which typically involves little in the way of hand-designing components. Cons : likely need a large amount of data for end-to-end learning to work well excludes potentially useful hand-designed components : if we have only a small training set, our learning algorithm likely does not have enough examples to learn representations that perform well. Although deep learning practitioners often speak despairingly about hand-crafted features or components, they allow us to inject priors into our model, which is particularly useful when we do not have a lot of labeled data. Note Note: hand-designed components and features are a double-edged sword. Poorly designed components may actually harm performance of the model by forcing the model to obey incorrect assumptions about the data.","title":"Pros and cons of end-to-end deep learning"},{"location":"structuring_machine_learning_projects/week_2/#should-i-use-end-to-end-deep-learning","text":"The key question we need to ask ourselves when considering on using end-to-end deep learning is: Do you have sufficient data to learn a function of the complexity needed to map \\(x\\) to \\(y\\)? Unfortunately, we do not have a formal definition of complexity --- we have to rely on our intuition.","title":"Should I use end-to-end deep learning?"}]}